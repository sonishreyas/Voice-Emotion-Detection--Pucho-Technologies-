{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-o2JI49WBAe",
        "colab_type": "code",
        "outputId": "31490bd1-9534-41bf-8541-041cb87643e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgFwaDhMbJVm",
        "colab_type": "code",
        "outputId": "355a5533-5625-41fa-e003-1f5f25206286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install librosa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.14.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.22.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.1)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.17.5)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.47.0)\n",
            "Requirement already satisfied: llvmlite>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (45.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxI4xzngdS-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "data, sampling_rate = librosa.load('/content/drive/My Drive/Ravdess/Actor1/Actor_01/03-02-01-01-01-01-01.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgaSHtCIdtX2",
        "colab_type": "code",
        "outputId": "e54bcbec-1fe6-435f-ed71-5be277c46c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f5a4af42588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAEICAYAAACZEKh9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd5wU9fkH8M9znV5Pyh1w0kQERDhR\nUBGkiBIFe4kJMRpLYqxRsWIPRn+W/DQx/pRIjLEbRUGQIiqKytGbFPGUKkcvB1ef3x87d+zdze7O\n7s7szOx93q8XL3ZnZ2ce5pbbZ7/7fJ+vqCqIiIiIiMi6FLcDICIiIiLyGybRRERERERRYhJNRERE\nRBQlJtFERERERFFiEk1EREREFCUm0UREREREUbIliRaRUSKyRkTWi8h4k8czReRN4/FvRCTP2J4u\nIpNFZLmIrBaRu+yIh4iIiIjISWnxHkBEUgE8D2AEgE0AFojIFFVdFbTbVQB2q2pXEbkUwOMALgFw\nEYBMVe0tIg0BrBKR11W1MNw5W7durXl5efGGTkREREQU0sKFC3eoarbZY3En0QAGAFivqhsAQETe\nADAGQHASPQbAA8btdwA8JyICQAE0EpE0AA0AlALYF+mEeXl5KCgosCF0IiIiIiJzIvJjqMfsKOfI\nAbAx6P4mY5vpPqpaDmAvgFYIJNQHAWwF8BOAJ1V1lw0xERERERE5xu2JhQMAVABoD+BoALeJSGez\nHUXkGhEpEJGCoqKiRMZIRERERFSDHUn0ZgAdgu7nGttM9zFKN5oB2AngcgDTVbVMVbcD+BJAvtlJ\nVPVFVc1X1fzsbNPSFCIiIiKihLAjiV4AoJuIHC0iGQAuBTCl1j5TAIwzbl8IYI6qKgIlHGcAgIg0\nAnAygO9siImIiIiIyDFxJ9FGjfMNAGYAWA3gLVVdKSIPici5xm4vA2glIusB3Aqgqg3e8wAai8hK\nBJLxf6rqsnhjIiIiIiJykgQGhP0lPz9f2Z2DiIiIiJwkIgtV1bTU2O2JhUREREREvsMkmoiIiIgo\nSkyiiYjIcZWVihFPfeZ2GEREtmESTUREjiuvVKzbfsDtMIiIbMMkmoiIHKcITGIvq6jER8u2uBwN\nEVH8mEQTEZHjjrl3OgBgwQ+7cMN/FrscDRFR/JhEExERERFFiUk0EREljP9WJiAiMsckmoiIHJU3\nfmr17V++9I2LkRAR2YdJNBERERFRlJhEExFR3MorKvH0zLVuh0FElDBMoomIKG7b95fg2dnr3A6D\niChhmEQTEVHcqiYMfvX9Dtz13vKI++87XOZsQEREDmMSTUREcamsVJwycQ4A4M0FG/H6tz9FfE6f\nBz5xOiwiIkcxiSaiem311n3IGz8V2/YedjsU32LbOiKqj9LcDoCIyA0vfbEBa7btx9sLNwEAdhwo\nQdtmWS5H5X9qZNS7DpaiZaMMd4MhInIQR6KJqF56Y8HG6gQaAH7xv/NcjCZ5TFm6BQDQ7+GZUT3v\nv4s34YXPvnciJCIiRzCJJqJ6p7S8Euu3H6iz/bpXF7oQjf+VVVTGfYyJH3+HiR9/Z0M0RESJYUsS\nLSKjRGSNiKwXkfEmj2eKyJvG49+ISF7QY31EZL6IrBSR5SLC71OJyFFvFWw03T595bYER5Icetw3\n3XT72wUba6xWWNvqrfuqbwsEAHC4rMLe4IiIHBJ3Ei0iqQCeB3AWgJ4ALhORnrV2uwrAblXtCuBp\nAI8bz00D8G8A16nqcQCGAGDfIyJyVGl5/COnBHS9exq+2bAz5OO3v7Ms7PPPevaLOtse/HBV3HER\nESWCHSPRAwCsV9UNqloK4A0AY2rtMwbAZOP2OwCGiYgAGAlgmaouBQBV3amqHIYgctFX63fgj68v\ndjsMR4m4HUFyKK9UXPLi17Ycq+pnUrS/xJbjERE5zY4kOgdA8Hejm4xtpvuoajmAvQBaAegOQEVk\nhogsEpE7Qp1ERK4RkQIRKSgqKrIhbCIy8/6SzfjQmBxmxYGScqgmT5MzjlK7K5leS0SU3NyeWJgG\n4FQAvzT+Pk9EhpntqKovqmq+quZnZ2cnMkYiCqPXhBmY4eFa4rzxU3Hj64urk7PKSg1bMnDlK98m\nKjRf+8X/1i3FiEfVlwNMoYnIL+xIojcD6BB0P9fYZrqPUQfdDMBOBEatP1fVHapaDGAagH42xERE\nCbRp9yG3QwhrytItOPnPswEAp/3l07D7frl+J0rKWVUWyYrN+yLvFAOORBORX9iRRC8A0E1EjhaR\nDACXAphSa58pAMYZty8EMEcDvylnAOgtIg2N5Pp0AJxVQuQzpTa0OHPaz/tK8Pyn67F5T+SEf9K8\nQucD8qldB0uxt9je+d+l5ZU4UFIOgCPRROQfcSfRRo3zDQgkxKsBvKWqK0XkIRE519jtZQCtRGQ9\ngFsBjDeeuxvAUwgk4ksALFLV0P2QiCihrp68wFLLsb9MX4MVm/cmIKLo3FmrO8QTM9ZYet7MVds4\nIhrC0Cfn4vy/f2nrMe98dxn2HQ4k0ZW87ETkE7Ys+62q0xAoxQjedn/Q7cMALgrx3H8j0OaOiDxm\n1urt2HGgBLktGkbcd/OeQ+iV0ywBUVn3Zoh+0JEs+mkPdh4sRevGmTZH5G/FpeXYe6jMlsVVgm3Y\ncbD69udri/DjzoPo1KqRrecgIrKb2xMLicjj/Dogu3VvfHXaC3/cbVMkyeMyo51dcal9NeOfra3b\nbWnSvB9sOz4RkVOYRBN5zF3vLcPHy7e6cu6Plm3BWwWbLO//087i6ttea7184d/nx/X8a7kEeB0b\nig5G3ilK4yaxGwoR+ROTaCKPef3bjXjtm59cOffanw9U396293DE/Qc/caTTxd5D3lps1O6SAyIi\nomBMook8yK0V9YJPW9USzqrb31mGDUUHIu+YAOu378d2G1a+q+AsNyIiCoFJNJGH7Dvs7mhuqOTd\nSocOADhY4o3+yu8vtr7iYjhd7p4Wead64s/TVmO/0YaOiIiYRBN5Sv+HZwIAxK2haBN//+x79Lhv\nOj5YUnsNpbrUI11+U1K8c/2SxdJNexJ2roIfd1v+4EZE5BYm0UQeUlYRSELdSgHF5Mz/Meqzb3pj\nScTne6WTR6qHPoQki+YNMhJ2rpVb9mHyV4UJO5/f3PPf5dhTXOp2GET1HpNoIg9yrSY6wnlfnV9Y\nffuTldscjSUeqfzNZrv0NOcu6tKNdUe5y2vVo5eWV+Igy0mQN34qXvvmJ7ZgJPIAvtUQkWX3fbAS\nJeWBr9mv8XALOC+VwySL9FR3r+mtby3B8Q9+4moMRETBmEQTeZB75RyRHXPvdBSXmo8IeqGao7JS\n8YqNpQAvc+EPAECGy8P7G4oO1hmdtsO+w2Vc4p2IYsIkmsiD3BpJtXrasc9/GXK7223hdhwoQZEN\n7e2qPPzRKtuO5VeqmpQT/T5ZuQ19HvgEj05d7frrloj8h0k0kcuWb9qLV7/+EXnjp1ZvcyOFXrF5\nL578ZK2lfYMXZalt50H7EtiYOHDx/jW/0P6D+sjMVT/j/SX2tA2MVdUHvKL9JdhxIP7XWN74qdUl\nSS/N+wFffb8j7mMmEgfPidzHJJrIZec8Nw/3vb+ixjY3BqJXbtmb+JM6IMWBi3f/ByttP6afFNmQ\ntMar6sd64qOzMPqvX9h+/FvfWhqyTKnKroOlePBD914Lm3YXV99mDk3kvjS3AyAiM4nPopNlMl5y\n/Cu8xYkPJvHYecD+9m5F+0uwY38pOraq+7ZYUak48dFZ2HUwcN4J5xxn+/mt2Lb3sCvnJSJzHIkm\n8iA3chbbEqUkHSL788er3Q7BNcnUd7u8ohJPzPjO9LFfvvw1ikvL63wzdLisojqBdlMS/RiIkgKT\naCIPcuO98k9vL7XlOG7n0E6Nmr63KPKKjcnK7eRNVW2rAe56z8d4/tPvTR/buOsQPln5M179+kfM\nCOqD/u0Pu+w5uY3YUYTIfUyiiRJg0+5ijHluHs75X2u1nHx7jJ3bCV8ycqOcoySoG8jTs9Zh5ZZ9\nCTnvzW8GVua89tWFKC2vxDX/KsCVryyosU/e+KkoLa9MSDzB7n2/ftfmE3mNLUm0iIwSkTUisl5E\nxps8nikibxqPfyMiebUe7ygiB0TkT3bEQ+Q1d/93BZZu2ovlm60lAnZ0H3BLsg6Q1efcPDUl8f/6\nv85ZX91Wb4nJioaJ0P3ej/HJqp9NH7vm1QJs3nMo4mREO63eeuT3h5cXOyKqL+JOokUkFcDzAM4C\n0BPAZSLSs9ZuVwHYrapdATwN4PFajz8F4ON4YyHyoi17DuGHHUdawuWNn4rKCD1p01P4JVF9cex9\n05E3fir2HS5zOxRTG3cVV4/OJlrV4ioVlYkf9Y1k7poinDJxDh6cErqPuJ29tbfvrzup0I3RcCI6\nwo536gEA1qvqBlUtBfAGgDG19hkDYLJx+x0Aw8RoBSAiYwH8AIDfU1FSOv9vX2HjrkM1tr00b0P1\n8tmm6vOwZ5ycGgl3qqLhkJFo9XnAm0tar9m237VzV13y8oqaP1Qvlexs3nMo5PYexgckO5RV1H1h\nr98eul87ETnPjiQ6B8DGoPubjG2m+6hqOYC9AFqJSGMAdwJ40IY4iDznk5XbsG1f3RGkx6Z9h1Vh\najz9PGno5D/PdjsER0gCPtnc8J9Fjp8jWm4mrNuNlSe9vJrgvPU78NCHq3Cw5EhZx/7DZfjDa0d+\nlnaMGJtV1Nzx7lJbV+ckoui4/Z3xAwCeVtWIH6dF5BoRKRCRgqKiIucjI7LBujAjRbNW/4wBj84y\nfczDOYPn+enS/XX2uhr3P1q21aVIzP160rd4YsYa184/9Mm5qKhUFPy4u8Z2r33GnPTlDzhuwgxs\n33cYSzbuQe8HPqlRx9393virFc0+xK3YvA8nhvgdYpdkXfKdyA52LLayGUCHoPu5xjazfTaJSBqA\nZgB2AjgJwIUi8hcAzQFUishhVX2u9klU9UUALwJAfn6+x36FEplLCzMhK1SbLQCoTHCW8Pe5oWOJ\nhaq6tniLn0bx/+/zDW6HENK+w2X4fK37AxZfrrdvOe7vtjnb4WPAY859C+PWNwJvF2zCHe8uw4C8\nlrhuSGec0aONO4EQeZAdI9ELAHQTkaNFJAPApQCm1NpnCoBxxu0LAczRgNNUNU9V8wA8A+AxswSa\nyK/+74sfYnpeovPAx6ebLz4RKzfz2KdnrXXkuIlKYrwy6jfq6c/dDgEAUG4yqTDWn8WoZ+xfLjwR\nVBVvLdgY8vGhT8517NwbdhwEAHxbuAt/eG2xY+ch8qO4k2ijxvkGADMArAbwlqquFJGHRORcY7eX\nEaiBXg/gVgB12uARJaNYW9X5aTTVjJvR//vrnxw57ta9hxOS4F7x0jeOn8OKLR5ZYvqFz7w7Wp8o\nR981Df8zM/SHwx+MRNdupeWVKCg8stDMobIKvPQFfx5EVWypiVbVaaraXVW7qOqjxrb7VXWKcfuw\nql6kql1VdYCq1vlfqKoPqOqTdsRD5Hd+r4n2+4eAUP41v9Dxc6z52b1uGF5ktlpgWYVixea9LkTj\nP/Gstvj+4s116tG/+n5nvCERJQ23JxYSkYlE1kRPWbrF9mMmZwpt3mbMbm4sbOJHV7zsjRF7q2av\nNl+0xSljn/8Sk78qxMX/mI/9MfQg/3j5VtM2nHO+24453yX230LkVUyiiTwokSPRN75uf51joidG\nJkoi6qL3FJd5uqWbV+wp9ubiNKFcNbkgoedbsnEPJkwJLL8QyyTf619bhI9XbDN97LevFDg+SZPI\nD5hEE3lQ8PK+fpSkOTRSbMyiyyoqsb/EfMnoRJSNUHL6v8831FngpdeEGeEXd6rlhIcCC/+EK90Y\n9cwXmM/SDqrnmEQTeVSy1hX7mZ0D0a98WRjysQc/XIVlm/Zgm0cm95F7dh0stbzv3kNleHTaatPH\njrl3esTnF+0vwdqf92O3xVH+y/7va/zp7aWYNC+2LkREfmdHn2gicsDXG3ZhYJdWbocRk2TN/+0c\niY5Up3ruc18CAAonjrbtnOQ/4yZ9a2m/VVv24YkZ8bWqjGXhlncWbgIA9OvUAn07NI/r/ER+wySa\nyKMOhviq3w9YE22f4tJyNMzgr+r6anextZHos//6BY53MYkd+/yX1bf7dWyO5y7vh/bNG7gWD1Ei\nsJyDyCGnP/FpXM9P8fH/TrdS6GQsgbn+34vcDiEp7PXZRMQqm3Yfsrzv0qClxs08O2td2Mftsuin\nPRg0cU5CzkXkJh+/TRN52487i+N6vthagZtYbiWz/wxTZ2wHN5Yy/2xtEbbutZ5I1SeHSq1Pljve\nmCxXn4VbzfPiF+bbfr57/rvc9mMSeQmTaCKv8m8O7dpI9GMhJlXZxdYWzlEk5LNXb7fxxMnjmlcT\n2zYuXk/OWON2CCF9Wxj7oiyhvPaNM6uHEnkFk2gij7JzElso3e6Z5shxtdKRw0ZU7nB/ZTt/JtEc\nyY1abD/4Yt2OhCzFbpfnPl3vdgj4bG1RnW13vefciPECB5JzIq9gEk3kUYlYuM6pFfjUhbHoRLxZ\n25XMHiwpx7Oz469PXbNtP25+YzF+irN0iOqPcZO+rTEiXlpeide/dW7E+CIHykSIvIJJNJFH+bsm\nOvHnTMSbtV0/kUNRjp7e898VptvPfOZzvL9kC95dtKl6m6om5QTLUJL5nxpNj+hoPPfpeiz6aTdW\nbN6L7vd+7Mg5iOoDJtFEHjV3zXZU+nT5Z39GbYFNQ9GxlIWES4yfnb2uemGWY+6djmcS1IWBnHXT\nG4sdO/b5f/sKv/jfeY4dP9hx90de6IXIj9h8lMijXpr3A8aekINeOc3cDiVqyToSaleJTSyHUQ2f\nw5/859lo2SgDpRWVWLllb8yx+Y0bpUPxUFXLXV72HvJnW77aDkbRRYXITzgSTUS2WxKhX61f2VVi\nE8tIdO0FbP42t+4kNbu+/vdTSz2/fVnzxbodlvctKXNphq4DKvz2gyKygEk0kQN++dLXthzHya4M\ni3/a7dixr5ocX+ux77btw1sFG22Kxj5pqe4NRU+YshIAcLisAnnjp+Iv00O3S5u1ejvyxk+tse3d\nhZssr4J50xtLog/QJVZWx5y9+ucERGJNNCuRHi5PnhHcvg+yTzclHybRRA74cv1OW47j5ORCq8sJ\nu+Ev09fgjneWuR1GHWk21XPMXRN93+fXvvkJ+w+X4by/fWX5OXnjp6LESMRue3spZqzcZul5peX+\nGQG1UjkU74c6O6VE8RqKZjEZr9sfxYcHIr9gTTSRhzk1Er23uAy/fcU7iUWwHQdK8NOu6Fq2JaoG\nO9WmJDrWkd7eD0Q/mnfMvdFP6rIyuusVpz0+B1/fPQwNM/zxdhbNB7Ht+0scjISI4mXLSLSIjBKR\nNSKyXkTGmzyeKSJvGo9/IyJ5xvYRIrJQRJYbf59hRzxEycKpJNrLo9C/fvlbrN9+IKrnzPkuMSv6\n+anMwcym3dZqnf1Uv7rvcLljreCc8MhUZ1fVJKLEiTuJFpFUAM8DOAtATwCXiUjPWrtdBWC3qnYF\n8DSAx43tOwCco6q9AYwD8Gq88RAlk8IdziyikZWe6shx7RBND+WqEeg9xYnrYrD/sH87Jjw1c62l\nUXunV360W2WY6hOvrWj4w46Dbofgmokff+d2CES2smMkegCA9aq6QVVLAbwBYEytfcYAmGzcfgfA\nMBERVV2sqluM7SsBNBCRTBtiIkoK1/17oSPH9fIy0plp1n8t3fTGEvzuXwW47e2lDkZUU7SlJl6z\n73Dk2lS/9ScPV37S4z72KPYKsyXHifzMjiQ6B0DwNPpNxjbTfVS1HMBeAK1q7XMBgEWqaloEJiLX\niEiBiBQUFfE/IpFV5RWVKKvwz0SxjFTrv5Y+XrEVM1cltvPC9f9elNDz2e14C10S/FTOAQAVPqrh\nrs/s6rNO5BWe6M4hIschUOJxbah9VPVFVc1X1fzs7OzEBUfkc79/bRFGPPVZjW1eHhGKJh1yY2l0\nv49EW+GniYVA8i7uk2xi6Y9O5GV2JNGbAXQIup9rbDPdR0TSADQDsNO4nwvgvwB+rarf2xAPEQVZ\nvHEPCnceSfxKyys92T6uSjQr0HFkyxl+G4neHaIm/tpXvdmBxorvi6KbXOsHyzfvxYrN9Wc1TUp+\ndiTRCwB0E5GjRSQDwKUAptTaZwoCEwcB4EIAc1RVRaQ5gKkAxqvqlzbEQkS11B6l83KC9MOOg1ix\neZ/1JzCJjslHy7aEfdxv5REXvTAf2/cfrrN9xkrvLLISrYtfmO92CI54fDonF1LyiDuJNmqcbwAw\nA8BqAG+p6koReUhEzjV2exlAKxFZD+BWAFVt8G4A0BXA/SKyxPhzVLwxEbmp9kpxbqudM0cz0huP\nWL5i/3pDzUVq5kVYIplfD8fmhv8sRu8JM0I+Hq7bhVcdLvVh0GGU+GjBm2hYmdhK5Be21ESr6jRV\n7a6qXVT1UWPb/ao6xbh9WFUvUtWuqjpAVTcY2x9R1Uaq2jfoT2IavhLVE1WdFv42dz0Aayu82XLe\nGM6T26JBjftXvPxN2P39mELv80iLvP0l5diyx7xvtN9GooHEfThMlANJusLf0o173A6ByDaemFhI\nRM6pmiT2zMx1AIAPl4b/Kt8usZSNmD0n7Ii2D7PoS//xtdshVBs0cY7pdr+1uAPqfjisz/2YvW7N\ntv1uh0BkCybRRB63fV/dWs9oVOVDGWkp2FNcivHvLbchqsimr9wW9XPMkujSMO35/FjOsXWvtVUD\nE8XsQ4ofR6KDlZRXYOiTc90Og0K49/3luOqVBVi+iZMMyd+YRBN53IDHZmNDHDP1q0aiD5SUo+9D\nM+0KK6IbX19sab/S8krc/d5yvLtwE177+sc6jxeXhF5xzn8pNJDqsZYi5z5Xd063L0eijb9/2lmM\n4+4PXe/tBclaqmHVgsLdmP3ddpzz3Dy3QyGKC5NoIhv9/jVnVhg8729fxfxctwcVl27cE7YkY+fB\nEvzn259w29tLMWdNUZ3EePnmvXirYKPpc8WHI9Fei3m5ScsxH+bQAIDr/70Qw5/6zPPLlq/ZFkUH\nmiQ3ad4PbodAFDMm0UQ2mrY8+hIGK2IZGcwbPxWHSiuqR6LP6tXW7rAi+vWkbzHm+S+xemvoGsiD\ntUaaa4/UPjBlJe54ZxmOvqtu1xNvpaPWeGwguvoa/nfxJkz+qhCAPxcvGfrkXHy8YlvY8h+v8GMZ\nklP+Omed2yEQxSzN7QCIKLJoU5qqJOjqyQuqW2XtPZT4rhCfGysjHiwN/fV1ca3HMtJSUF56JLHe\nVVwKwHxE3Y+5iNcSKEWgb/Qtby4FAOS1buT7mmiv89prwE17isvw9My1uGVEd7dDIYoaR6KJfKBS\nFY9NW42fLU4yrPo6+8vvj/Rd/ur7naF2d1xJWejRwUOloWueAWBfmOTfa6URVnhtJBoI9I0+cnuR\nb8s5/MIsic4bP7W6o0h9617xySr/LopD9RuTaCIfKC6twIufb7D0ZpM3fio2FHmrvdfhstCJcu2v\n32uXroRK6DbvOYRdB0vjji3RvJ6gNslK8+XEQj9JqfXOe9z90wEA320N1Eqf+czniQ7JVau37sPR\nd03F3hDLtxN5FZNoIh/5aedB/LjzIMoj1H1+57GJS5VhygMO1FrB7LDFldo+Xr41rpjicdmLX8ec\naHq9UiIjNSXsz4viFzwSXbjjIA4a38b4oZ7bKarA4o273Q6DKCpMool85P+++AGnPzEX932wMux+\nOw/4Z4T2+tcWWd53294j5SzhyjycNn/DTnS+e1rYEfZQvJ6gqno/0fe74Os7JKiftR/Lk+xUULgb\nX63f4XYYRJYxiSbyoaL9JQCA8opKvF2wsbqrQpWHPlrlQlTOW7X1SDs2L1QcxLIqo9eT6B93FSfZ\nAtreU/UamLtme43tL8z9Hrt9WKJkl+c+XY/LX/rG7TCILGMSTeRDs1b/jM/WFuG7bftx+zvLMGHK\nSuSNr9sCLh5enAC3YvORMhUvxGclIZ78VSE27S6uvu/xHJoSoHBnYM7Cb/65oMb2VVv3mfbtrm9u\nf3up2yEQWcIWd0QWzVu3AxWqOL17ttuhAADGTfoWo/u0q7HNzkTazpHea15diPd+Pwj9Oraosf3V\n+YXITEupbsMXyVMz12LHgRIcOFyOo5pm2RdgjCothD1hykocKCnHH4Z2dT4g8oUb/rM4ZJu7WL7d\nSDZvL9yEe0Yfi+YNM9wOhSgsjkQTWfTbyQswbtK3AICzn/0C67fvr7EohRsLVExd5t7kumh9tqYI\n3e6Zhr/OPrK4wn0frLScQFf51/wf8d7izdi295DdIUatPEwWfePrizHGWNY4u3Fm9fad9fjrejri\n9yHmAsxjTTAAoO9DM+v98ujkfUyiiSwqNZK9XQdLsWrrPgx/6nP8uPPI1/QcQArv2dnrUFaheGrm\nWizZuCfu472/ZIsNUcUn3KIkX32/A0s3Bb6av+PdZcgbP7X6NUQUystcBrta3wc/cTsEorCYRBNF\nqf/DM6tv7y4+MqrIr2GtG/v8lzj5sdluhxG3AY+G/jfsMOmQMns1F5Ugsqq8UrGgcJfbYRCFxCSa\nKErBqfLyzXurezb/5p/fuhOQT22zuPpibalemFEY5IqXvsH67dZWmIumnR8RARe9MN/tEIhCYhJN\nFIf7P1iJSV8Gvn51c1nt+sRrI/7z1u/A8Kc+x5Y97tdoEyWjPcWcR0DeZEsSLSKjRGSNiKwXkfEm\nj2eKyJvG49+ISF7QY3cZ29eIyJl2xEOUSI9N+w6X/IOjJfXdoIlzqvt3T1/hnwmfRF538T/m48XP\nv8ff5q7Htr2HcZATDskjJN6OAiKSCmAtgBEANgFYAOAyVV0VtM/vAfRR1etE5FIA56nqJSLSE8Dr\nAAYAaA9gFoDuqhp2GbD8/HwtKCiIK26q3x7/+Dv0bN8U5xzf3vJz7O7DTEREsbn0xA54cMxxyExL\ndTsUSnIislBV880es6NP9AAA61V1g3GyNwCMARC8ZNoYAA8Yt98B8JwE1jcdA+ANVS0B8IOIrDeO\nx2G9emD7vsPIbpLpylK3f//se/Tv1CKqJJqIiLzhjQUb8caCjXW2N2+YjisHHY0ze7VBg/RU7D1U\nhqz0VKzcshflFYqRPdsiJa3rbGwAACAASURBVCUwt6VheirSUlNQUakoLi1Hk6x0lFVUIi1FUKmA\nACgpr0RWeorp+5Sqmm6vrFSkxDF3I9RxyXvsSKJzAAS/kjcBOCnUPqpaLiJ7AbQytn9d67k5ZicR\nkWsAXAMAqU2zOSpIcVv4426+joiIksie4jI8PWstnp611vTx27EswRGR32W07do/1GO+WbFQVV8E\n8CIA9OnbT9+79XQEPkt6+dNaVamMXTFW/XuDjxvNOcyeH825g4V6rtmxg58bOH9puWLHgRJkN8lE\nighEgBSRGguWVH0Qr9pk/sG86t9T9aBWjyCY7V+pgeOd+cznyG6SideuPgmqgSWkFYEYKlWrn1+1\nPxB4DhEReVvbZlm4dnBn7C4uQ2l5Jdo0zcTin/Zg/+Ey/GpgJ5SUVaK4tAIdWjZEo8xU7DtUjrKK\nSmQ3ycTBkvLq94LUlMD7QYP01OoJzSkiSEsVlFcoKlWRmiKB95AUIFUECqC4tAIZqSlISxWUGiPZ\nqkBZhSIlBUhLEVRUIvBeI0feY4BA96GyikqkpghSjffGisrA+5Eg0Js+zRjlNntvLKtQpKce2aBa\n9d4WvH/gvbZqu9n7qGrVces+VjOPqBK4VnUH4Gs/vzqyED89qXX+UDEA4XOYSPmh1fxR0f3JH5eH\netSOJHozgA5B93ONbWb7bBKRNADNAOy0+Nw6MtJS0PWoxvHETPXcad1aY2CXVujeponboRARUZQ6\ntWqI1393MhplpKFxViCVCdf+8spTEhUZJRutKAvZHsaOJHoBgG4icjQCCfClAC6vtc8UAOMQqHW+\nEMAcVVURmQLgPyLyFAITC7sBYLNdctyrV9WuOCKK37vXD0Kf3GZ4ZtZaPP/p926HQ5Q0BhzdEpWq\nuGV4d7RpmsWBNPKEuJNoo8b5BgAzAKQCmKSqK0XkIQAFqjoFwMsAXjUmDu5CINGGsd9bCExCLAfw\nh0idOYi85vdDuuCOUT1YX13PffTHU9ErpxkA4PYzezCJJrLJ57cPRcdWDd0Og6gOW2qiVXUagGm1\ntt0fdPswgItCPPdRAI/aEQdRol0zuDP+MLQrgECdW7nHFgJJRl68zq9eNaA6gSYiezGBJq/iioVE\ncTjzuDZolBn4LLryIa4VlAheS6ALJ47Gad2yLe1756geDkdDlFz+fH5vt0MgColJNFGUZt06uPp2\n06z06ttpKfzvZNUTF/bBhsfOdjuMuH1xx9CQj7VunFln228G5TkYDVHyuWxAR7dDIAqJ7/pEFmWl\nB/67dD0q0NHjxV/1R+fsI5Nb4uitXy+M6RtY2Oai/rm4KL9DXIsRAMDYvu4vlNMwI/RqaZ1bN0Lr\nxhkAgEfG9sK6R89CgzD7EwHAqV1bux2CZ4T7kErkBUyiiSy666xjce/oYwEEvsIfeVzbGi2V3Fhh\nKqd5g4SfM1ZjT8jB94+djccv6FO97cL+uVEfp3/HFmjWIB2tTEZ6Ey1cS63Jvx2AT/80BABQXFqO\n9NTAr9tWjTISERp5XNXvktp+e2peYgPxqOk3n4YOLVkLTd7GJJrIonGD8nD1aZ3dDqPa3Wf3wD9+\nVXMhpeUPjHQpmvD+etkJGHrMUUhNkRoj0E9edHxUxzm/Xw7e/f0gLJ0wEo08MKobLolukJGKJlnp\n+EWfdji5c6sERkVed+3gzvjtKUebPpbKsjC0a5aFHm2buh0GUUS+WbGQiI4Yekw2rhncBQdKyjH8\n2DYAFH/7ZX9kpHnzDTjLprjOOf5ICYcXpheGS6KrPHd5vxr3XfjCgjzmnOPbIyVFcNOwbnh29roa\nj7H/MTD/rmFuh0BkiTffcYkorMy0wChs48w0vDQuHy+NO7FGAn31qeajXH6X67HyFbG0bGxNKR7P\noo9q4n6ZTLKreg3cMqJ7je3PXtrXVyVadhtyTDbGn8UONuQfTKKJfOQXfdrh5XH5+MtFfcLud1yO\nf74KjaaFVbegZdqrWgu6oVmDdHx917CYJgp6PIdGo8w0TpJNoP/8jqunVhk3KA/Xnd7F7TCILGMS\nTeQjJ3VuhWHHtqnRWs9Mp1aNEhSRNeEmXbavNfJmtSRlsMXezE5YdN8ItG2WFdNzvT4SXVZR6cok\n2fpEg4qRBnU50o3D668Npx0T9CGZyA+YRBP5QMOMVPTOaYaTjm4Zcd8Nj52N3h5bPS8zTGKckVrz\nsVSLiUTP9k3RomH4DxNOsVILHYrX86T9h8s5Eu0wrVXQXzhxNADg6NaBD78vXNG/9lOSWnaTTHz/\n2Nl1PlATeR2TaCIfSBHBh388Fd0tjNSkpAjSTLKgTi62i8pKD132ULskonaS2SQrdNmGFyYXRqt2\nAuUF4wZ2qr598/Bulj/IUGwqTV4ES+4fUb10/KhebRMdkquGdM+O64MpkVuYRBP5QLRvL1Vfxy9/\nYCTSUwO3e7ZPfJ10E6NuuWqhGjO1W9WVVVTWuN8yXF9lDyakkZglUG5KFcGDY3rhxjO64vIBHXDl\nKUfHvRAOhVdhsnR984b1s394g/RULu1NvsUWd0Q2atYgHXsPldl/4BhymiX3j0CTrHSkpgjKKhTz\n1u+wP64Ilj94Jt5fvBk924VO4BvWmiBYO8e8/vQuWLlln+msfa8lpFZ4LeQKI6BbRx5Tvc2Ptbkf\n/fFUXPyP+SgurXA7lIhMcuh668L+uUhL5Xge+RNfuUQ2WjrBmcVO/nrZCVE/p2pkqyoh2n+43NaY\nrBp7Qk7YN8mWDTPQqVVDXHt6Z+Q0b4DyWhnGyOPa4uGxvVztxmEnryX+VUuTB/PjQHSjzDSsemgU\nXrva+90uOrXiSnxVHh7by+0QiGLGJJrI46bffBqGHnNUzM+vSqKbZqXhs9uH2BRVZL8OqrMNp0FG\nKj67fSjuOutYPHjucXUebximjZy30lFrzL7Kd9Ps24bU2ebHco6qiE/p2hoF9w53NZZIWntgyXo3\ntTJKtP4nyhVLibyGSTSRx8W7/G1VPnS4vBKdWjXCsB6xJ+TRuGf0sVE/JzW1bvIWrrOHxwZ1LfHa\nqpLNGtTtcOLHiYXBIbdunInXf3eye8FQWE9f0heFE0fjgv65bodCFBdv/TYnIttVTTL8RZ92AGIr\nDYlFLImYWVeRcD2L1YdZ9LvXD3I7hGof3nCq6XYfDkTXMbBLK7dDoBAGd3evxzuRneJKokWkpYjM\nFJF1xt8tQuw3zthnnYiMM7Y1FJGpIvKdiKwUkYnxxEJE5qoSoqcu7gsgcX2KY2lZVbS/pMb9m4Z1\nC7u//1JoILeFd+phe+ea9xP3YzkH+cPxHZq7HQKRbeIdiR4PYLaqdgMw27hfg4i0BDABwEkABgCY\nEJRsP6mqPQCcAOAUETkrzniIXFe1cIJX1E5mJZZWHzGIZdW7qsUmqtwyonvY/b02Sc8vrju9c9hJ\nsH7s2ZtsHR7cWkjIaY0zQ89xIPKbeH/rjAEw2bg9GcBYk33OBDBTVXep6m4AMwGMUtViVf0UAFS1\nFMAiACyQIrJdrSTaw/nRCR1b4Lho+lkzh47J+LOONa2FruK3FnfPXtoXOSar3TUP82/0uqcu6et2\nCI4YNzDP7RCIbBNvEt1GVbcat7cBaGOyTw6AjUH3NxnbqolIcwDnIDCaTUQ2ymnRoMZktqz0VPxm\nUJ57AUUQTfrGHNoZfptYGKoP+RKHWk4mQjwdebyqT24zjDyufq3GSMktYuNVEZkFwOxVf0/wHVVV\nEYn6PU1E0gC8DuCvqrohzH7XALgGADp27BjtaYjqrVevGoCy8pqrAF53ehe88lWhOwFFEM0oKMs5\nnOG3cg7WcPsD/79SsomYRKtqyIabIvKziLRT1a0i0g7AdpPdNgMYEnQ/F8DcoPsvAlinqs9EiONF\nY1/k5+fzfyKRRU2z6n6l7eWBxrIo+iif0KE59hwqw9qfDzgYUU0f/dG8o4VfzLh5cMR9fJdEe/kF\nTdUqKyPvQ+Qn8ZZzTAEwzrg9DsAHJvvMADBSRFoYEwpHGtsgIo8AaAbg5jjjIEpKZouP2MHLA0Il\nZdaXbf7nlQPw0R9PS+iiDe1Nam/9pHubxhH38VsSHa785MvxZyQwEgqnm4XXHpGfxJtETwQwQkTW\nARhu3IeI5IvISwCgqrsAPAxggfHnIVXdJSK5CJSE9ASwSESWiMjVccZDlFRO7uxMr9uScuuJaqKV\nlFsfrmqUmYaMtBSkJ3ABk5aN6i6T7RcX9s+11DXFb0l0uH+S2YRDcseTXKGQkkxc7zyqulNVh6lq\nN1UdbiTMUNUCVb06aL9JqtrV+PNPY9smVRVVPVZV+xp/Xorvn0OUXNShqXMNMyJWcrnmX1cNQPMo\n23uN7t3OoWhqStRCNU4ZaPFDmd/KI5r6qAvHS7/OdzsE16QnWRtCIr6iiTzMqRrC7CaZCS2BiEaX\n7MY4IcoFGRI1clph0w/k8pNimxw985bI9cy1zb/rDKx/NLoW/H6aqLf0/pFh2/V5TUUUtVTZTTId\njISI4sUkmsjDnBqJBrxd2/vrQXm44mTvdeEpq7Dn53HnmT2ifk7n7Ebo1qYJVj80yvJzCieORrtm\nDZCWmoLz++XglK6tLT3PbPl1rxIL72LXnd7F+UAsqoxi4mzDDC5MQuRlTKKJHGDXqoVOTgA86eiW\njh37tggrDUYy9Jij8MjY3jZFY5+KKBIgu1V11WiQkYrCiaNxzvHtQ+47/Nij6rwGn7q4L9o2y7J0\nrntHHxt7oAlmpfRk/FnRf2hxSjQreWYmsNbfaQvvDdnoi8i3kud/KFEScjKJdvIr+8tiLFfwOrv6\n3MbyDUPtZPGpi0OX48Qb5gkdW8R3gATyz5h5wIieZmuSmctMS56R6FaNWZpCyYdJNJFH9c5p5tuW\nUH6bmGaVXR9qYhnQrv2Zp/YkrXeuG1g9+mx1xDkZ+O2lFk39fjKNRBMlI/4PJfKoO0f1QFa6P0ei\nfJbXWKZ2jUTHcJxwZQA3DeuG/LxAec6i+0bg/nN6xhyb30jSvtqAiRc4V9L05/N741+/HeDY8YMt\nf8C/y68ThcMkmsijnJxU6DQ3Rgcnnu98DbVdP5Fo+1o/dp75v+1vv+wHABja46jqbS0bZSRVGUAk\nfhuJjkbXo5o4ctyhx2TjsgEdMbh7NpbcP8KRcwRrYrJqKlEyYBJN5FEuzmGLmxujg5cOcL4O265y\njqZZ6fjdaUdbP2+I9P3s3u1QOHE0+kbZEpDqr1uGd8c/rzwyAt28YQYGODjJ2KutNInswCSayKPs\nKh0IZ+kEZ75mtdJ2zI/smlgIAA2iWPDGzx+onOanuuFBXZxZgTQaNw3vVmfbG7872bHzXdA/17Fj\nE7nNP799iOqZBOTQji1S4dY37DcM7ero8RPxMzFzYp5/umUk0rIHRkbVMs5t/3EwWY1HSoo4srBL\n26b1Z4Ir1U9Mook8yt810e4kNmajbHaycyTaakbeJ7cZerRtat95k0jTKGpt59x2uoOR+MPg7tkh\nH/vijqG2n+/ru4fZfkwiL2ESTeSQNY9YX1nOjFNLfieCW2ODtdu+JYOXxuW7HUJS6Jztz3aROVGs\nLHp8hNr4cN047O4E9NEfT7X1eEReZL0oj4iiEm+HhBQf54PsE22fo5rwK/H6zGpf6Wcv7YtXvip0\nNpgwHh5zHM7s1RYtG2YgLQk/zBKZ4SudyKNO735U5J08KklzaFtLbCKVvDx+QW+O5hFeufJES/uN\n6ZuD5y7vF9e5pt98Gp67/ISon3feCTn41cA8HNUkiwk01St8tRN5VDQrm1Fi2Nkl43eDO4d87NYR\n3XHJiR3RK6eZfSckX4qmDCWneQP88iTzVo/z7oxc89yjbVP8ok97y+e7c1QPFE4cjacv6Wv5OUTJ\nhEk0kQf1aOvMIguJkqwj0XZOLGycmYYmmeYVddeESbCJwnn0vN5Y+8hZNbZ9e/cw5LZoaPkYVcvH\nt2kaumPHv686CdcP6RJbkERJgkk0kQclsqb4Qgf6uCbrUsyJqIlu0TDdt8u9J5Kf+kMDwLWnJ+6D\nUYZxbQYYS8E3yIj+9XRh/1xcO9g8Sf7z+b1xarfWsQdIlCT89VuIqJ5I5KTCJx1YUSxZK1ESsQBO\nBVdWseR/LvbXSnh3nXVsQs9XOHE03rpuIO45+1g0imJhnypPXnQ8MtPr/iLqndMMl+R3sCNEIt+L\n661aRFqKyEwRWWf8bboigIiMM/ZZJyLjTB6fIiIr4omFKJn4vbuFnxbAiMblJ3Vy/BwtGmU4fg6/\nS0+VqGp367PfDe6MlBg/1f6iT3s0qjWKfXLnljEfjyjZxDveNR7AbFXtBmC2cb8GEWkJYAKAkwAM\nADAhONkWkfMBHIgzDqKk4vck1M3oj27dyJHjtmuWhZYJSHDfvX6Q4+ewIs0jidLLJn2y3Vo50i0/\n/Pls9MpJ/II7zRqk41cD86rvp6UIxid4RJ3Iy+JNoscAmGzcngxgrMk+ZwKYqaq7VHU3gJkARgGA\niDQGcCuAR+KMg8iT7j67R0zPS3T+Yvey0m5+BnjnuoGOHDdRiVvrxvYvvxyLT/80xO0QAMDWUc8X\nroivBZxbRASTxoVudbfywTMdO3eroA+O48/qwa5BREHiTaLbqOpW4/Y2AG1M9skBsDHo/iZjGwA8\nDOB/ABRHOpGIXCMiBSJSUFRUFEfIRIlTVhE68+obZnWxRJdzvH2dvaOfbo6k+2kUf9ix3u0F3qFl\nQ7RvloXGITqIJMqJxuS4YLH+iEf1ahdnNOG9e/0gPDK2lyPHDlcq38jBn9GVp+ThizuGonDiaFx9\nGrvGEAWLmESLyCwRWWHyZ0zwfhqYcWN5rEZE+gLooqr/tbK/qr6oqvmqmp+dnW31NESuyg4zqjjh\nnJ7VraRq42BP7Px06Z65tObCFrktrC/xnAif3TEUT7k4ge/VqwagcWYa+nUMv5y129JTBfPuHIr+\nnVrgipM7YcbNg2s8/sUdkXs0RxJqoZ9pN54W97HDSUtNQYeW1tvjEdUnEZNoVR2uqr1M/nwA4GcR\naQcAxt/bTQ6xGUDwVN5cY9tAAPkiUghgHoDuIjI3vn8Okbdc2D/XdNRsYOdWYRdR8NNoam1zbjvd\n7RAcYedqhaHMu/MMx88RjfTUFFcnufbrGCgzqr0Knpdqoo9t1xTrHj27Rh/mY9o2qVEOY0cSajYS\nPe3G09CzfeJrpYkoIN5yjikAqrptjAPwgck+MwCMFJEWxoTCkQBmqOrfVbW9quYBOBXAWlUdEmc8\nRJ6SkiJ4x6RU4tWrBqBZg/TQT/RQkhCthjG007KTUzmf04nbrFu9+eEjxwOj416Z5GimU4gE+ejW\njTD3T0Ow4J7htpzH7Ap0aOn+z4aoPov33W4igLdE5CoAPwK4GABEJB/Adap6taruEpGHASwwnvOQ\nqu6K87xEvtG/UwsM7p6Nz9cGavnXPDKqzshabeWVlYkIjTxg2QMj0TA9NeJrwi3HtmuKZy7pi5vf\nXJLwc1d9IPLitTmufVM8el7vsCU4eTZ2imnXLKvOtiZZYT6IE5Hj4vrNpKo7VXWYqnYzyj52GdsL\nVPXqoP0mqWpX488/TY5TqKrOzMYg8oA/ntEVANAoIxWZaZFXD2ve0L+9gn1cieKKplnpnkwSg9m5\n3LlVF+fnVn+rUTtRTdRr7Is7hiIjxM/mtatPQt8OzRPWTaV2idfzl/uz0whRMvH2b26iJHFiXkus\ne/QsLLxvhKX9vfz1tdc5le/5uMImbm4sohg8ijvhnJ7IaX4kkXYyp79leHcAwBUnd0SHlg2x9tGz\ncOOwbjX22fDY2a580H33+iPtG9NT+TuCyG1MookSJD01BVnpkUehAXcStr9c0MeW47j91u7Utcvv\nZG8vbT9xYyQ6+JSZaalo0cie0oWv7xqG48O0l/zVwE7Iad4Aj4ztXb3t2sE1W7u5tWJf8DXx8+Rj\nomTBJJrIg1zpPmDXe7LL7+3q0MWrz1+fV7oxFO2Qts2y8MEfTjF9bN6dQ9GyUQa+HF+zS0pGGt8q\niagu/mYg8iQ3Rv6SI1FyKt9za/TRC5Iohw4r1MIy6akpKJw4Gu/9fhA6ZzuzrLwVfp4rQZSM3O1F\nRUR47Lze+HDpFszfsLN6mxv5LBdUCO38fjmRd0piWenuj7dU/Z+YectgR0oZ/vXbARGT1H4dW2DO\nbUNsP7dVXY860lu+/n6kI/IO938zEtVzl5/UEa9fczIW3nukn6wbA3+DurTGn0Z2j/s4zRu4O1rm\nxKIoT13c1/Zj+smYvjkY2LmV22EAALq1aVIjmYzVmkdGYdzATgAC7ery8/xV886SaCL3MYkm8ohW\nCWqVFY7VEfBv7h5muv2DP5ziev2o3Un8Rf1zbT2eH6WmCPJau/tNhd3fzmSmpeLBMb0w85bBmHLD\nqa4vEkRE/sMkmsiDvFyf/MUdQ9Gmad2FH7wiIy0Fd4w6xrbjPXHR8bYdy89Ky919TYZd4TMO3do0\nQWo9rncnotjxozeRB7mVrkQ67wX9cqprp8cN7ITJ8390PqgYJFM3Ca8oq3B3Fc0XruiP/SVlrsZA\nRBSMI9FEHuTWQHSk8/5PUG3wg2O8u8ioy/leUnIyiQ7Xt7lKs4bpyG3Bya/LHhiJji0boke7pm6H\nQlTvMYkm8iAvjaP2NRKcX53cKeK+Xpns5MbiIMlu3+HEjgKzFt1c06x0fH7H0BorOBKRO1jOQeQh\nn98+FIOf+NS1mmizzhbPXtoXKSK+aoHHJNp+2Qmc+DpuYCcc5eG6eyIigCPRRJ7SsZX3OiBEk0Cn\np3rjV8qpXVvbcpxQXUjqo8cv7MMJeEREQbzxjkdENbhWEx10++Vx+VE999YR3dGjbRN7A4rRSZ1b\noa0NI5le7kKSaJlpqWiYkep2GEREnsEkmshjeuU0xZBjsl05d3aTI1/ZDzu2TcT9X7v6pOrbPdo2\ncWQluVgdLq9wO4TkwyoZIqJqTKKJPOajP56Gq0/r7Mq5fzmgI4b1OKrGtnB58SlBZRNey6/+9dsB\ncT3/9jPt6zWdLJpk2T+N5q6zeth+TCKiRGASTUTVUlIELRu5u2y3XfrkRm6bFs7F+R1siiR5/PcP\npwAAGqTbV9Zx7eld6mw7t297245PROSUuJJoEWkpIjNFZJ3xd4sQ+40z9lknIuOCtmeIyIsislZE\nvhORC+KJh4jiV3vkuXlDa0m1UyvKxaNfx9gT6WT5MGGnqhpxu5d2bxo0wj24ezb6d2pp6/GJiJwQ\n72/C8QBmq2o3ALON+zWISEsAEwCcBGAAgAlByfY9ALarancAPQF8Fmc8RGSjwomj0Tgz8lf4V56S\nh5M7t0pARNF59/pBNe7nWex+ctOwbuxEEcKc207Hhzecausxn7usX/VtXnUi8ot4k+gxACYbtycD\nGGuyz5kAZqrqLlXdDWAmgFHGY78F8GcAUNVKVd0RZzxE5IL2zby58EPtiY5zbx9q6XlXnpLnQDTJ\noXN2Y9tbMTZrmF69eIiH5qYSEYUVbxLdRlW3Gre3ATCbzp8DYGPQ/U0AckSk6nvWh0VkkYi8LSKR\n2wEQkec0a+i9Uo7a1j16FgBgw2Nnh92v21GNLZewkH2qFhhKYRZNRD4RMYkWkVkissLkz5jg/TTw\nGzCaCfppAHIBfKWq/QDMB/BkmDiuEZECESkoKiqK4jRE5KTZt52OC/p5d4nmhfcOx/ePnV29EExK\nioTtvPHWtQMTFZqvFU4cbevxqt48mEITkV9ELHZU1eGhHhORn0WknapuFZF2ALab7LYZwJCg+7kA\n5gLYCaAYwHvG9rcBXBUmjhcBvAgA+fn5XuumRZQ0BnVpjS/X77S8f5fsxg5GE79WJstVNwqzaEgL\nTih0lZd6jRMRhRNvOccUAFXdNsYB+MBknxkARopIC2NC4UgAM4yR6w9xJMEeBmBVnPEQUZzGnpCD\nL8ef4XYYjuKncPs8POY4W4+XaXPnDyIip8T722oigBEisg7AcOM+RCRfRF4CAFXdBeBhAAuMPw8Z\n2wDgTgAPiMgyAL8CcFuc8RARReTWsurJpnDiaPxqYF7Ix8/u3Tbs8/962QnVt6t+Jg+P7WVHaERE\njosriVbVnao6TFW7qerwquRYVQtU9eqg/Sapalfjzz+Dtv+oqoNVtY9xnJ/iiYeIyIoL+pvXcLc1\n+iBTdOb+aYjp9r/9sn/Y2ulzjz+yqIoa3w+wPzcR+YX9a7gSEXlcswbp6HZUY6zbfqDG9q/vHuZS\nRP6W2yL+FoeXnNgRG3cV2xANEVFiMIkmonppUJdWyEhLwcot+wDA9gVE6pPgyYDnHt8eU5Zuwaxb\nB0d1jFtHdLc7LCIiRzGJJqJ66cExgdrb6Su24rp/L0LTBvx1GKvgfhpV+XTXo5q4EgsRUaLwXYOI\n6rVRvdphxYNnWlrenIiIqAp7CRFRvccEOj4pKYJpN54GABhyTDZ6tI08Cv3JLdGVexAReQ3fOYiI\nKG7NjaXfzzshF+edEHkFy+5tWO5BRP7GkWgiIopby0YZOK59U7fDICJKGCbRREQUt6z0VEw1SjqI\niOoDJtFERERERFFiEk1ERI5aOmFk9e3Xrj7JxUiIiOzDJJqIiBzVrEF69W0Jsx8RkZ8wiSYiIiIi\nihKTaCIicty8O4cCAHrlNsPF+ZFb4BEReR2TaCIiclybplkAgKZZ6fjLhce7HA0RUfyYRBMRkeNS\nhNXQRJRcmEQTEZHjUlMEhRNHux0GEZFtmEQTEREREUWJSTQRERERUZTiSqJFpKWIzBSRdcbfLULs\nN87YZ52IjAvafpmILBeRZSIyXURaxxMPEREREVEixDsSPR7AbFXtBmC2cb8GEWkJYAKAkwAMADBB\nRFqISBqAZwEMVdU+AJYBuCHOeIiIiIiIHBdvEj0GwGTj9mQAY032ORPATFXdpaq7AcwEMAqBhasE\nQCMREQBNAWyJMx4iIiIiIsfFm0S3UdWtxu1tANqY7JMDYGPQ/U0AclS1DMD1AJYjkDz3BPByqBOJ\nyDUiUiAiBUVFRXGGUdnNswAABmRJREFUTUREREQUu4hJtIjMEpEVJn/GBO+nqgpArZ5YRNIRSKJP\nANAegXKOu0Ltr6ovqmq+quZnZ2dbPQ0RERERke3SIu2gqsNDPSYiP4tIO1XdKiLtAGw32W0zgCFB\n93MBzAXQ1zj+98ax3oJJTTURERERkddIYAA5xieLPAFgp6pOFJHxAFqq6h219mkJYCGAfsamRQD6\nA8gytvdR1SIReRhAQ1W9zcJ59wNYE3Pg9VtrADvcDsKneO1ix2sXO1672PHaxY7XLna8drHz4rXr\npKqmJRARR6IjmAjgLRG5CsCPAC4GABHJB3Cdql6tqruMBHmB8ZyHVHWXsd+DAD4XkTLj+b+xeN41\nqpofZ+z1kogU8NrFhtcudrx2seO1ix2vXex47WLHaxc7v127uJJoVd0JYJjJ9gIAVwfdnwRgksl+\nLwB4IZ4YiIiIiIgSjSsWEhERERFFya9J9ItuB+BjvHax47WLHa9d7HjtYsdrFzteu9jx2sXOV9cu\nromFRERERET1kV9HoomIiIiIXOOrJFpERonIGhFZb7TUIxORrpOI/EZEikRkifHnarPjUICITBKR\n7SKywu1YvCzSdRKRISKyN+h1d3+iY/QLEekgIp+KyCoRWSkiN7kdk1dZuVZ87VkjIlki8q2ILDWu\n5YNux+RVVq4V32ujIyKpIrJYRD5yOxar4m1xlzAikgrgeQAjEFg6fIGITFHVVe5G5i1RXKc3VfWG\nhAfoT68AeA7Av1yOw+teQeTr9IWq/iIx4fhaOYDbVHWRiDQBsFBEZvL3nSmr14qvvchKAJyhqgeM\nVYXnicjHqvq124F5kNVrxfda624CsBpAU7cDscpPI9EDAKxX1Q2qWgrgDQBjIjynPuJ1spmqfg5g\nl9txeB2vk31UdauqLjJu70fgjSXH3ai8idfKPhpwwLibbvzhxCkTvFb2EpFcAKMBvOR2LNHwUxKd\nA2Bj0P1N4C9KM1av0wUiskxE3hGRDokJjQgDja8/PxaR49wOxg9EJA/ACQC+cTcS74twrfjas8D4\nSn0JgO0AZqoqX3chWLxWfK+15hkAdwCodDuQaPgpiSb7fAggT1X7AJgJYLLL8VD9sAiB5VOPB/C/\nAN53OR7PE5HGAN4FcLOq7nM7Hi+LcK342rNIVStUtS+AXAADRKSX2zF5lYVrxfdaC0TkFwC2q+pC\nt2OJlp+S6M0Agj/F5RrbqKaI10lVd6pqiXH3JQD9ExQb1WOquq/q609VnQYgXURauxyWZxl1lu8C\neE1V33M7Hi+LdK342ouequ4B8CmAUW7H4nWhrhXfay07BcC5IlKIQAnqGSLyb3dDssZPSfQCAN1E\n5GgRyQBwKYApLsfkRRGvk4i0C7p7LgI1hESOEpG2IiLG7QEI/P7Z6W5U3mRcp5cBrFbVp9yOx8us\nXCu+9qwRkWwRaW7cboDABPXv3I3Km6xcK77XWqOqd6lqrqrmIZCzzFHVK1wOyxLfdOdQ1XIRuQHA\nDACpACap6kqXw/KcUNdJRB4CUKCqUwDcKCLnIjCrfReA37gWsA+IyOsAhgBoLSKbAExQ1Zfdjcp7\nzK4TApNtoKovALgQwPUiUg7gEIBLlas9hXIKgF8BWG7UXALA3cYoKtVkeq0AdAT42otSOwCTjS5P\nKQDeUlXftBtLMNNrxffa+oUrFhIRERERRclP5RxERERERJ7AJJqIiIiIKEpMoomIiIiIosQkmoiI\niIgoSkyiiYiIiIii5JsWd0REdISItAIw27jbFkAFgCLjfrGqDnIlMCKieoIt7oiIfE5EHgBwQFWf\ndDsWIqL6guUcRERJRkQOGH8PEZHPROQDEdkgIhNF5Jci8q2ILBeRLsZ+2SLyrogsMP6c4u6/gIjI\n+5hEExElt+MBXAfgWARW9uuuqgMAvATgj8Y+zwJ4WlVPBHCB8RgREYXBmmgiouS2QFW3AoCIfA/g\nE2P7cgBDjdvDAfQUkarnNBWRxqp6IKGREhH5CJNoIqLkVhJ0uzLofiWOvAekADhZVQ8nMjAiIj9j\nOQcREX2CI6UdEJG+LsZCROQLTKKJiOhGAPkiskxEViFQQ01ERGGwxR0RERERUZQ4Ek1EREREFCUm\n0UREREREUWISTUREREQUJSbRRERERERRYhJNRERERBQlJtFERERERFFiEk1EREREFCUm0URERERE\nUfp/nW8IrUZKCGsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCtNuVWlr5jL",
        "colab_type": "text"
      },
      "source": [
        "# Load all files\n",
        "\n",
        "Extracting Mel-frequency cepstral coefficients (MFCCs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKvuF--gd6F-",
        "colab_type": "code",
        "outputId": "4589f600-49f7-48fc-ad3c-2508b5bcf204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import time\n",
        "\n",
        "path = '/content/drive/My Drive/Ravdess/Actor1'\n",
        "lst = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        file = int(file[7:8]) - 1 \n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Data loaded. Loading time: 473.3380661010742 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLSggnF7kKY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzvBRTJIlIE9",
        "colab_type": "code",
        "outputId": "7d541bd9-b15f-49be-cf47-ac33bac46116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1012, 40), (1012,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agw-3KN1sDhh",
        "colab_type": "text"
      },
      "source": [
        "# Decision Tree Classifier\n",
        "\n",
        "Creating a basic model to test on dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-Xgb5NslTBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UshLOC1ClWL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BnCR52nlXw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtree = DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWyTownblZM0",
        "colab_type": "code",
        "outputId": "e5e06b17-9990-4477-c093-5186adcaf02b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "dtree.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEuw6TUQlr7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = dtree.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4kNSYkAleIv",
        "colab_type": "code",
        "outputId": "c65f97b0-0ca5-4c19-ef84-e36ec988a6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.64      0.63        25\n",
            "           1       0.87      0.66      0.75        73\n",
            "           2       0.54      0.60      0.57        55\n",
            "           3       0.59      0.60      0.59        67\n",
            "           4       0.50      0.66      0.57        47\n",
            "           5       0.63      0.58      0.60        67\n",
            "\n",
            "    accuracy                           0.62       334\n",
            "   macro avg       0.62      0.62      0.62       334\n",
            "weighted avg       0.64      0.62      0.62       334\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9eqMHV3S8i6",
        "colab_type": "text"
      },
      "source": [
        "# Neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4i187-Pe-w5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnvoCRX1gQCh",
        "colab_type": "code",
        "outputId": "649511fa-3bff-4764-a475-86cc89c7ab91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((678, 40, 1), (334, 40, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZOGIpuefCd3",
        "colab_type": "code",
        "outputId": "0966397d-40cd-422f-a05c-f680c598e027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(8))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LphftMIZzUvz",
        "colab_type": "text"
      },
      "source": [
        "With *model.summary* we can see a recap of what we have build:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIWPB4Zgfic7",
        "colab_type": "code",
        "outputId": "4f88ece7-a325-4f54-e85a-63ba7d8b1f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 5128      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qQSBeBhzcLu",
        "colab_type": "text"
      },
      "source": [
        "Now we can compile and fit our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNI1znbsfpTx",
        "colab_type": "code",
        "outputId": "9f0f226c-25db-4eea-8cc7-f437d65d3df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktdF-nJKfq6F",
        "colab_type": "code",
        "outputId": "139cf8f4-393e-437f-ceb5-81f77ea8f8d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 678 samples, validate on 334 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "678/678 [==============================] - 1s 2ms/step - loss: 9.5822 - acc: 0.1519 - val_loss: 7.9700 - val_acc: 0.1557\n",
            "Epoch 2/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 8.6577 - acc: 0.2035 - val_loss: 8.5252 - val_acc: 0.0749\n",
            "Epoch 3/1000\n",
            "678/678 [==============================] - 0s 494us/step - loss: 8.8713 - acc: 0.1770 - val_loss: 8.1240 - val_acc: 0.1407\n",
            "Epoch 4/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 7.8103 - acc: 0.1681 - val_loss: 5.4060 - val_acc: 0.1407\n",
            "Epoch 5/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 6.9173 - acc: 0.1991 - val_loss: 4.9154 - val_acc: 0.1916\n",
            "Epoch 6/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 6.8034 - acc: 0.1814 - val_loss: 5.3172 - val_acc: 0.2186\n",
            "Epoch 7/1000\n",
            "678/678 [==============================] - 0s 561us/step - loss: 6.5322 - acc: 0.1785 - val_loss: 5.6651 - val_acc: 0.1527\n",
            "Epoch 8/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 6.4729 - acc: 0.1888 - val_loss: 5.7220 - val_acc: 0.1407\n",
            "Epoch 9/1000\n",
            "678/678 [==============================] - 0s 556us/step - loss: 5.9873 - acc: 0.2124 - val_loss: 5.4098 - val_acc: 0.1497\n",
            "Epoch 10/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 5.9730 - acc: 0.2271 - val_loss: 5.5420 - val_acc: 0.1497\n",
            "Epoch 11/1000\n",
            "678/678 [==============================] - 0s 475us/step - loss: 6.0035 - acc: 0.2006 - val_loss: 4.8728 - val_acc: 0.1587\n",
            "Epoch 12/1000\n",
            "678/678 [==============================] - 0s 476us/step - loss: 5.0054 - acc: 0.2153 - val_loss: 1.7816 - val_acc: 0.2066\n",
            "Epoch 13/1000\n",
            "678/678 [==============================] - 0s 492us/step - loss: 3.9973 - acc: 0.2212 - val_loss: 1.6631 - val_acc: 0.3204\n",
            "Epoch 14/1000\n",
            "678/678 [==============================] - 0s 499us/step - loss: 3.9925 - acc: 0.1947 - val_loss: 1.6862 - val_acc: 0.2814\n",
            "Epoch 15/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 3.4919 - acc: 0.2198 - val_loss: 1.7124 - val_acc: 0.2934\n",
            "Epoch 16/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 3.4867 - acc: 0.2257 - val_loss: 1.6965 - val_acc: 0.3503\n",
            "Epoch 17/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 3.4052 - acc: 0.2537 - val_loss: 1.7662 - val_acc: 0.3323\n",
            "Epoch 18/1000\n",
            "678/678 [==============================] - 0s 509us/step - loss: 3.0881 - acc: 0.2271 - val_loss: 2.0198 - val_acc: 0.2455\n",
            "Epoch 19/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 2.8722 - acc: 0.2478 - val_loss: 1.7667 - val_acc: 0.3802\n",
            "Epoch 20/1000\n",
            "678/678 [==============================] - 0s 498us/step - loss: 2.7691 - acc: 0.2389 - val_loss: 1.5716 - val_acc: 0.3563\n",
            "Epoch 21/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 2.5291 - acc: 0.2817 - val_loss: 1.4927 - val_acc: 0.3862\n",
            "Epoch 22/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 2.5590 - acc: 0.2684 - val_loss: 1.6414 - val_acc: 0.3263\n",
            "Epoch 23/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 2.6074 - acc: 0.2404 - val_loss: 1.6986 - val_acc: 0.3653\n",
            "Epoch 24/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 2.3174 - acc: 0.2876 - val_loss: 1.5132 - val_acc: 0.3802\n",
            "Epoch 25/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 2.1660 - acc: 0.3083 - val_loss: 1.5490 - val_acc: 0.3533\n",
            "Epoch 26/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 2.1620 - acc: 0.2876 - val_loss: 1.4780 - val_acc: 0.3563\n",
            "Epoch 27/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 2.0216 - acc: 0.2802 - val_loss: 1.4096 - val_acc: 0.3683\n",
            "Epoch 28/1000\n",
            "678/678 [==============================] - 0s 500us/step - loss: 2.0062 - acc: 0.3068 - val_loss: 1.4855 - val_acc: 0.4072\n",
            "Epoch 29/1000\n",
            "678/678 [==============================] - 0s 473us/step - loss: 2.0120 - acc: 0.2920 - val_loss: 1.5321 - val_acc: 0.3743\n",
            "Epoch 30/1000\n",
            "678/678 [==============================] - 0s 481us/step - loss: 1.9222 - acc: 0.3038 - val_loss: 1.3309 - val_acc: 0.4701\n",
            "Epoch 31/1000\n",
            "678/678 [==============================] - 0s 486us/step - loss: 1.8293 - acc: 0.3481 - val_loss: 1.4547 - val_acc: 0.3922\n",
            "Epoch 32/1000\n",
            "678/678 [==============================] - 0s 509us/step - loss: 1.8031 - acc: 0.3496 - val_loss: 1.4256 - val_acc: 0.3802\n",
            "Epoch 33/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 1.7053 - acc: 0.3687 - val_loss: 1.2929 - val_acc: 0.4760\n",
            "Epoch 34/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 1.6523 - acc: 0.3687 - val_loss: 1.3353 - val_acc: 0.4850\n",
            "Epoch 35/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 1.6524 - acc: 0.3894 - val_loss: 1.4462 - val_acc: 0.3772\n",
            "Epoch 36/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 1.6062 - acc: 0.3968 - val_loss: 1.2988 - val_acc: 0.4581\n",
            "Epoch 37/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 1.5868 - acc: 0.3953 - val_loss: 1.3108 - val_acc: 0.4820\n",
            "Epoch 38/1000\n",
            "678/678 [==============================] - 0s 573us/step - loss: 1.5602 - acc: 0.3673 - val_loss: 1.2804 - val_acc: 0.4641\n",
            "Epoch 39/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 1.5525 - acc: 0.3982 - val_loss: 1.2986 - val_acc: 0.4371\n",
            "Epoch 40/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 1.5134 - acc: 0.4100 - val_loss: 1.3096 - val_acc: 0.4102\n",
            "Epoch 41/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 1.4978 - acc: 0.4115 - val_loss: 1.2551 - val_acc: 0.5120\n",
            "Epoch 42/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 1.4426 - acc: 0.4543 - val_loss: 1.2000 - val_acc: 0.5210\n",
            "Epoch 43/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 1.4324 - acc: 0.4513 - val_loss: 1.2411 - val_acc: 0.5449\n",
            "Epoch 44/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 1.4418 - acc: 0.4263 - val_loss: 1.1975 - val_acc: 0.5778\n",
            "Epoch 45/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 1.4162 - acc: 0.4454 - val_loss: 1.2244 - val_acc: 0.5479\n",
            "Epoch 46/1000\n",
            "678/678 [==============================] - 0s 539us/step - loss: 1.3846 - acc: 0.4454 - val_loss: 1.1993 - val_acc: 0.5240\n",
            "Epoch 47/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 1.3978 - acc: 0.4617 - val_loss: 1.1900 - val_acc: 0.5449\n",
            "Epoch 48/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 1.3504 - acc: 0.4499 - val_loss: 1.1430 - val_acc: 0.5719\n",
            "Epoch 49/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 1.3486 - acc: 0.4926 - val_loss: 1.1879 - val_acc: 0.5090\n",
            "Epoch 50/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 1.3814 - acc: 0.4543 - val_loss: 1.1405 - val_acc: 0.6317\n",
            "Epoch 51/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 1.2853 - acc: 0.4853 - val_loss: 1.1521 - val_acc: 0.5659\n",
            "Epoch 52/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 1.3133 - acc: 0.4926 - val_loss: 1.1035 - val_acc: 0.5868\n",
            "Epoch 53/1000\n",
            "678/678 [==============================] - 0s 559us/step - loss: 1.2889 - acc: 0.4912 - val_loss: 1.1203 - val_acc: 0.6287\n",
            "Epoch 54/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 1.2814 - acc: 0.5044 - val_loss: 1.0814 - val_acc: 0.6257\n",
            "Epoch 55/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 1.2884 - acc: 0.5029 - val_loss: 1.1237 - val_acc: 0.5719\n",
            "Epoch 56/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 1.2418 - acc: 0.5118 - val_loss: 1.0517 - val_acc: 0.6377\n",
            "Epoch 57/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 1.2537 - acc: 0.4956 - val_loss: 1.0718 - val_acc: 0.5928\n",
            "Epoch 58/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 1.2358 - acc: 0.5354 - val_loss: 1.0381 - val_acc: 0.6228\n",
            "Epoch 59/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 1.2737 - acc: 0.5162 - val_loss: 1.0920 - val_acc: 0.5808\n",
            "Epoch 60/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 1.1875 - acc: 0.5413 - val_loss: 1.0728 - val_acc: 0.5838\n",
            "Epoch 61/1000\n",
            "678/678 [==============================] - 0s 551us/step - loss: 1.2497 - acc: 0.5044 - val_loss: 1.0901 - val_acc: 0.6108\n",
            "Epoch 62/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 1.1953 - acc: 0.5251 - val_loss: 1.0570 - val_acc: 0.6647\n",
            "Epoch 63/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 1.1880 - acc: 0.5236 - val_loss: 1.0369 - val_acc: 0.6257\n",
            "Epoch 64/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 1.1681 - acc: 0.5531 - val_loss: 1.0229 - val_acc: 0.6467\n",
            "Epoch 65/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 1.1312 - acc: 0.5737 - val_loss: 1.0065 - val_acc: 0.6527\n",
            "Epoch 66/1000\n",
            "678/678 [==============================] - 0s 560us/step - loss: 1.1508 - acc: 0.5457 - val_loss: 1.0452 - val_acc: 0.6018\n",
            "Epoch 67/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 1.1561 - acc: 0.5560 - val_loss: 1.0494 - val_acc: 0.5928\n",
            "Epoch 68/1000\n",
            "678/678 [==============================] - 0s 601us/step - loss: 1.1304 - acc: 0.5590 - val_loss: 1.0135 - val_acc: 0.6647\n",
            "Epoch 69/1000\n",
            "678/678 [==============================] - 0s 542us/step - loss: 1.1392 - acc: 0.5546 - val_loss: 0.9804 - val_acc: 0.6766\n",
            "Epoch 70/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 1.1262 - acc: 0.5649 - val_loss: 1.0020 - val_acc: 0.6826\n",
            "Epoch 71/1000\n",
            "678/678 [==============================] - 0s 578us/step - loss: 1.0967 - acc: 0.5560 - val_loss: 1.0006 - val_acc: 0.6527\n",
            "Epoch 72/1000\n",
            "678/678 [==============================] - 0s 561us/step - loss: 1.1031 - acc: 0.5826 - val_loss: 0.9484 - val_acc: 0.7006\n",
            "Epoch 73/1000\n",
            "678/678 [==============================] - 0s 555us/step - loss: 1.0992 - acc: 0.5737 - val_loss: 0.9831 - val_acc: 0.6467\n",
            "Epoch 74/1000\n",
            "678/678 [==============================] - 0s 565us/step - loss: 1.0713 - acc: 0.5811 - val_loss: 0.9773 - val_acc: 0.6916\n",
            "Epoch 75/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 1.1020 - acc: 0.5428 - val_loss: 0.9491 - val_acc: 0.6527\n",
            "Epoch 76/1000\n",
            "678/678 [==============================] - 0s 574us/step - loss: 1.0594 - acc: 0.5914 - val_loss: 0.9207 - val_acc: 0.7096\n",
            "Epoch 77/1000\n",
            "678/678 [==============================] - 0s 585us/step - loss: 1.0441 - acc: 0.6047 - val_loss: 1.0321 - val_acc: 0.5928\n",
            "Epoch 78/1000\n",
            "678/678 [==============================] - 0s 570us/step - loss: 1.0291 - acc: 0.6106 - val_loss: 0.9672 - val_acc: 0.6407\n",
            "Epoch 79/1000\n",
            "678/678 [==============================] - 0s 595us/step - loss: 1.0281 - acc: 0.6062 - val_loss: 0.9296 - val_acc: 0.6886\n",
            "Epoch 80/1000\n",
            "678/678 [==============================] - 0s 555us/step - loss: 1.0470 - acc: 0.5900 - val_loss: 0.9084 - val_acc: 0.7186\n",
            "Epoch 81/1000\n",
            "678/678 [==============================] - 0s 582us/step - loss: 1.0090 - acc: 0.6091 - val_loss: 0.9654 - val_acc: 0.6078\n",
            "Epoch 82/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.9966 - acc: 0.6032 - val_loss: 0.9467 - val_acc: 0.6108\n",
            "Epoch 83/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 1.0097 - acc: 0.6091 - val_loss: 0.8808 - val_acc: 0.7096\n",
            "Epoch 84/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.9726 - acc: 0.6268 - val_loss: 0.8626 - val_acc: 0.7186\n",
            "Epoch 85/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.9930 - acc: 0.6180 - val_loss: 0.8886 - val_acc: 0.6886\n",
            "Epoch 86/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.9930 - acc: 0.6180 - val_loss: 0.8644 - val_acc: 0.6946\n",
            "Epoch 87/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.9804 - acc: 0.6180 - val_loss: 0.8916 - val_acc: 0.6946\n",
            "Epoch 88/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.9485 - acc: 0.6416 - val_loss: 0.8919 - val_acc: 0.6647\n",
            "Epoch 89/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.9230 - acc: 0.6593 - val_loss: 0.8423 - val_acc: 0.7305\n",
            "Epoch 90/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.9846 - acc: 0.6239 - val_loss: 0.8595 - val_acc: 0.6916\n",
            "Epoch 91/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.9593 - acc: 0.6136 - val_loss: 0.8338 - val_acc: 0.7335\n",
            "Epoch 92/1000\n",
            "678/678 [==============================] - 0s 551us/step - loss: 0.9203 - acc: 0.6372 - val_loss: 0.8335 - val_acc: 0.7216\n",
            "Epoch 93/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.9276 - acc: 0.6268 - val_loss: 0.8590 - val_acc: 0.6617\n",
            "Epoch 94/1000\n",
            "678/678 [==============================] - 0s 502us/step - loss: 0.8928 - acc: 0.6622 - val_loss: 0.8193 - val_acc: 0.7275\n",
            "Epoch 95/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.9505 - acc: 0.6165 - val_loss: 0.8362 - val_acc: 0.7365\n",
            "Epoch 96/1000\n",
            "678/678 [==============================] - 0s 552us/step - loss: 0.8788 - acc: 0.6519 - val_loss: 0.8157 - val_acc: 0.6856\n",
            "Epoch 97/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.9381 - acc: 0.6298 - val_loss: 0.8477 - val_acc: 0.6976\n",
            "Epoch 98/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.8805 - acc: 0.6770 - val_loss: 0.8093 - val_acc: 0.7335\n",
            "Epoch 99/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.8886 - acc: 0.6637 - val_loss: 0.8202 - val_acc: 0.7036\n",
            "Epoch 100/1000\n",
            "678/678 [==============================] - 0s 509us/step - loss: 0.9096 - acc: 0.6401 - val_loss: 0.8241 - val_acc: 0.7006\n",
            "Epoch 101/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.8707 - acc: 0.6681 - val_loss: 0.8052 - val_acc: 0.7305\n",
            "Epoch 102/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.8426 - acc: 0.7006 - val_loss: 0.8015 - val_acc: 0.7036\n",
            "Epoch 103/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.8852 - acc: 0.6667 - val_loss: 0.8132 - val_acc: 0.7216\n",
            "Epoch 104/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.8662 - acc: 0.6667 - val_loss: 0.7957 - val_acc: 0.7186\n",
            "Epoch 105/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.8464 - acc: 0.6726 - val_loss: 0.7998 - val_acc: 0.7066\n",
            "Epoch 106/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.8939 - acc: 0.6829 - val_loss: 0.8016 - val_acc: 0.7156\n",
            "Epoch 107/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.8678 - acc: 0.6578 - val_loss: 0.7679 - val_acc: 0.7455\n",
            "Epoch 108/1000\n",
            "678/678 [==============================] - 0s 565us/step - loss: 0.8389 - acc: 0.6740 - val_loss: 0.8324 - val_acc: 0.6976\n",
            "Epoch 109/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.8390 - acc: 0.6785 - val_loss: 0.7946 - val_acc: 0.7365\n",
            "Epoch 110/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.8241 - acc: 0.6696 - val_loss: 0.7659 - val_acc: 0.7335\n",
            "Epoch 111/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.8525 - acc: 0.6460 - val_loss: 0.7561 - val_acc: 0.7545\n",
            "Epoch 112/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.8361 - acc: 0.6814 - val_loss: 0.7724 - val_acc: 0.7335\n",
            "Epoch 113/1000\n",
            "678/678 [==============================] - 0s 547us/step - loss: 0.8195 - acc: 0.6696 - val_loss: 0.7427 - val_acc: 0.7635\n",
            "Epoch 114/1000\n",
            "678/678 [==============================] - 0s 496us/step - loss: 0.8237 - acc: 0.6770 - val_loss: 0.7617 - val_acc: 0.7455\n",
            "Epoch 115/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.7978 - acc: 0.6829 - val_loss: 0.7682 - val_acc: 0.7275\n",
            "Epoch 116/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.8066 - acc: 0.7065 - val_loss: 0.7857 - val_acc: 0.7066\n",
            "Epoch 117/1000\n",
            "678/678 [==============================] - 0s 554us/step - loss: 0.7969 - acc: 0.6888 - val_loss: 0.8667 - val_acc: 0.6377\n",
            "Epoch 118/1000\n",
            "678/678 [==============================] - 0s 551us/step - loss: 0.7994 - acc: 0.6770 - val_loss: 0.7346 - val_acc: 0.7395\n",
            "Epoch 119/1000\n",
            "678/678 [==============================] - 0s 545us/step - loss: 0.7819 - acc: 0.7035 - val_loss: 0.7574 - val_acc: 0.7096\n",
            "Epoch 120/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.8241 - acc: 0.6785 - val_loss: 0.7427 - val_acc: 0.7365\n",
            "Epoch 121/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.8102 - acc: 0.6814 - val_loss: 0.7311 - val_acc: 0.7425\n",
            "Epoch 122/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.7868 - acc: 0.6976 - val_loss: 0.7666 - val_acc: 0.7455\n",
            "Epoch 123/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.7803 - acc: 0.6947 - val_loss: 0.8555 - val_acc: 0.6317\n",
            "Epoch 124/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.7760 - acc: 0.6903 - val_loss: 0.7225 - val_acc: 0.7485\n",
            "Epoch 125/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.7750 - acc: 0.6932 - val_loss: 0.7341 - val_acc: 0.7485\n",
            "Epoch 126/1000\n",
            "678/678 [==============================] - 0s 550us/step - loss: 0.7398 - acc: 0.7301 - val_loss: 0.7341 - val_acc: 0.7365\n",
            "Epoch 127/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.7770 - acc: 0.7094 - val_loss: 0.7200 - val_acc: 0.7515\n",
            "Epoch 128/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.7346 - acc: 0.7168 - val_loss: 0.7049 - val_acc: 0.7605\n",
            "Epoch 129/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.7539 - acc: 0.6947 - val_loss: 0.7917 - val_acc: 0.6916\n",
            "Epoch 130/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.7529 - acc: 0.7065 - val_loss: 0.7956 - val_acc: 0.6886\n",
            "Epoch 131/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.7635 - acc: 0.7227 - val_loss: 0.7022 - val_acc: 0.7605\n",
            "Epoch 132/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.7375 - acc: 0.7198 - val_loss: 0.6770 - val_acc: 0.7784\n",
            "Epoch 133/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.7338 - acc: 0.7109 - val_loss: 0.7009 - val_acc: 0.7605\n",
            "Epoch 134/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.7358 - acc: 0.6962 - val_loss: 0.6876 - val_acc: 0.7575\n",
            "Epoch 135/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.7316 - acc: 0.7109 - val_loss: 0.7583 - val_acc: 0.7036\n",
            "Epoch 136/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.6999 - acc: 0.7153 - val_loss: 0.7724 - val_acc: 0.6826\n",
            "Epoch 137/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.7148 - acc: 0.7198 - val_loss: 0.7111 - val_acc: 0.7305\n",
            "Epoch 138/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.7097 - acc: 0.7257 - val_loss: 0.7638 - val_acc: 0.7126\n",
            "Epoch 139/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.7232 - acc: 0.7124 - val_loss: 0.7214 - val_acc: 0.7395\n",
            "Epoch 140/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.7115 - acc: 0.7242 - val_loss: 0.7344 - val_acc: 0.7305\n",
            "Epoch 141/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.6764 - acc: 0.7478 - val_loss: 0.7093 - val_acc: 0.7335\n",
            "Epoch 142/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.7144 - acc: 0.7360 - val_loss: 0.6749 - val_acc: 0.7605\n",
            "Epoch 143/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 0.7013 - acc: 0.7286 - val_loss: 0.7294 - val_acc: 0.7246\n",
            "Epoch 144/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 0.6961 - acc: 0.7286 - val_loss: 0.6972 - val_acc: 0.7365\n",
            "Epoch 145/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.6815 - acc: 0.7360 - val_loss: 0.6765 - val_acc: 0.7725\n",
            "Epoch 146/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.7056 - acc: 0.7316 - val_loss: 0.6734 - val_acc: 0.7725\n",
            "Epoch 147/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.7122 - acc: 0.7080 - val_loss: 0.6640 - val_acc: 0.7695\n",
            "Epoch 148/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.6553 - acc: 0.7552 - val_loss: 0.6744 - val_acc: 0.7515\n",
            "Epoch 149/1000\n",
            "678/678 [==============================] - 0s 567us/step - loss: 0.7284 - acc: 0.7212 - val_loss: 0.6611 - val_acc: 0.7725\n",
            "Epoch 150/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.6783 - acc: 0.7478 - val_loss: 0.6566 - val_acc: 0.7695\n",
            "Epoch 151/1000\n",
            "678/678 [==============================] - 0s 509us/step - loss: 0.6618 - acc: 0.7478 - val_loss: 0.6731 - val_acc: 0.7665\n",
            "Epoch 152/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.6868 - acc: 0.7227 - val_loss: 0.6596 - val_acc: 0.7545\n",
            "Epoch 153/1000\n",
            "678/678 [==============================] - 0s 505us/step - loss: 0.6623 - acc: 0.7330 - val_loss: 0.7416 - val_acc: 0.7365\n",
            "Epoch 154/1000\n",
            "678/678 [==============================] - 0s 549us/step - loss: 0.6654 - acc: 0.7286 - val_loss: 0.6757 - val_acc: 0.7485\n",
            "Epoch 155/1000\n",
            "678/678 [==============================] - 0s 595us/step - loss: 0.6464 - acc: 0.7566 - val_loss: 0.6982 - val_acc: 0.7575\n",
            "Epoch 156/1000\n",
            "678/678 [==============================] - 0s 543us/step - loss: 0.6553 - acc: 0.7419 - val_loss: 0.6956 - val_acc: 0.7335\n",
            "Epoch 157/1000\n",
            "678/678 [==============================] - 0s 587us/step - loss: 0.6424 - acc: 0.7330 - val_loss: 0.6926 - val_acc: 0.7275\n",
            "Epoch 158/1000\n",
            "678/678 [==============================] - 0s 573us/step - loss: 0.6449 - acc: 0.7360 - val_loss: 0.6789 - val_acc: 0.7515\n",
            "Epoch 159/1000\n",
            "678/678 [==============================] - 0s 595us/step - loss: 0.6602 - acc: 0.7434 - val_loss: 0.7069 - val_acc: 0.7335\n",
            "Epoch 160/1000\n",
            "678/678 [==============================] - 0s 598us/step - loss: 0.6359 - acc: 0.7552 - val_loss: 0.6375 - val_acc: 0.7665\n",
            "Epoch 161/1000\n",
            "678/678 [==============================] - 0s 592us/step - loss: 0.6458 - acc: 0.7507 - val_loss: 0.6338 - val_acc: 0.7754\n",
            "Epoch 162/1000\n",
            "678/678 [==============================] - 0s 585us/step - loss: 0.6298 - acc: 0.7714 - val_loss: 0.6679 - val_acc: 0.7395\n",
            "Epoch 163/1000\n",
            "678/678 [==============================] - 0s 574us/step - loss: 0.6252 - acc: 0.7640 - val_loss: 0.6819 - val_acc: 0.7335\n",
            "Epoch 164/1000\n",
            "678/678 [==============================] - 0s 564us/step - loss: 0.6638 - acc: 0.7537 - val_loss: 0.6347 - val_acc: 0.7605\n",
            "Epoch 165/1000\n",
            "678/678 [==============================] - 0s 598us/step - loss: 0.6530 - acc: 0.7434 - val_loss: 0.6608 - val_acc: 0.7485\n",
            "Epoch 166/1000\n",
            "678/678 [==============================] - 0s 561us/step - loss: 0.6617 - acc: 0.7448 - val_loss: 0.6470 - val_acc: 0.7455\n",
            "Epoch 167/1000\n",
            "678/678 [==============================] - 0s 589us/step - loss: 0.6273 - acc: 0.7596 - val_loss: 0.6521 - val_acc: 0.7635\n",
            "Epoch 168/1000\n",
            "678/678 [==============================] - 0s 571us/step - loss: 0.6331 - acc: 0.7389 - val_loss: 0.6186 - val_acc: 0.7665\n",
            "Epoch 169/1000\n",
            "678/678 [==============================] - 0s 564us/step - loss: 0.6127 - acc: 0.7581 - val_loss: 0.6133 - val_acc: 0.7784\n",
            "Epoch 170/1000\n",
            "678/678 [==============================] - 0s 580us/step - loss: 0.6365 - acc: 0.7478 - val_loss: 0.6996 - val_acc: 0.7246\n",
            "Epoch 171/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.5950 - acc: 0.7817 - val_loss: 0.6622 - val_acc: 0.7575\n",
            "Epoch 172/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.5907 - acc: 0.7684 - val_loss: 0.6702 - val_acc: 0.7635\n",
            "Epoch 173/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.6024 - acc: 0.7655 - val_loss: 0.6290 - val_acc: 0.7635\n",
            "Epoch 174/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.6166 - acc: 0.7655 - val_loss: 0.6081 - val_acc: 0.7844\n",
            "Epoch 175/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.5895 - acc: 0.7714 - val_loss: 0.6192 - val_acc: 0.7485\n",
            "Epoch 176/1000\n",
            "678/678 [==============================] - 0s 495us/step - loss: 0.6177 - acc: 0.7699 - val_loss: 0.6143 - val_acc: 0.7814\n",
            "Epoch 177/1000\n",
            "678/678 [==============================] - 0s 492us/step - loss: 0.6249 - acc: 0.7581 - val_loss: 0.6061 - val_acc: 0.7725\n",
            "Epoch 178/1000\n",
            "678/678 [==============================] - 0s 484us/step - loss: 0.6051 - acc: 0.7788 - val_loss: 0.5950 - val_acc: 0.7994\n",
            "Epoch 179/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.5853 - acc: 0.7714 - val_loss: 0.6092 - val_acc: 0.7635\n",
            "Epoch 180/1000\n",
            "678/678 [==============================] - 0s 501us/step - loss: 0.6012 - acc: 0.7788 - val_loss: 0.6193 - val_acc: 0.7754\n",
            "Epoch 181/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.5868 - acc: 0.7729 - val_loss: 0.6223 - val_acc: 0.7725\n",
            "Epoch 182/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.5948 - acc: 0.7773 - val_loss: 0.6072 - val_acc: 0.7725\n",
            "Epoch 183/1000\n",
            "678/678 [==============================] - 0s 575us/step - loss: 0.5553 - acc: 0.7950 - val_loss: 0.6043 - val_acc: 0.7635\n",
            "Epoch 184/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.5821 - acc: 0.7802 - val_loss: 0.6032 - val_acc: 0.7874\n",
            "Epoch 185/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.6015 - acc: 0.7670 - val_loss: 0.6630 - val_acc: 0.7545\n",
            "Epoch 186/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.5947 - acc: 0.7773 - val_loss: 0.6116 - val_acc: 0.7814\n",
            "Epoch 187/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.5989 - acc: 0.7596 - val_loss: 0.6094 - val_acc: 0.7784\n",
            "Epoch 188/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.5756 - acc: 0.7817 - val_loss: 0.5855 - val_acc: 0.7754\n",
            "Epoch 189/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.5708 - acc: 0.7802 - val_loss: 0.6066 - val_acc: 0.7754\n",
            "Epoch 190/1000\n",
            "678/678 [==============================] - 0s 548us/step - loss: 0.5705 - acc: 0.7920 - val_loss: 0.6662 - val_acc: 0.7335\n",
            "Epoch 191/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.5588 - acc: 0.7847 - val_loss: 0.6335 - val_acc: 0.7455\n",
            "Epoch 192/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.5382 - acc: 0.7876 - val_loss: 0.6302 - val_acc: 0.7814\n",
            "Epoch 193/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.5465 - acc: 0.8024 - val_loss: 0.5848 - val_acc: 0.7814\n",
            "Epoch 194/1000\n",
            "678/678 [==============================] - 0s 499us/step - loss: 0.5685 - acc: 0.7684 - val_loss: 0.5964 - val_acc: 0.7665\n",
            "Epoch 195/1000\n",
            "678/678 [==============================] - 0s 485us/step - loss: 0.5899 - acc: 0.7773 - val_loss: 0.5929 - val_acc: 0.7934\n",
            "Epoch 196/1000\n",
            "678/678 [==============================] - 0s 494us/step - loss: 0.5385 - acc: 0.7950 - val_loss: 0.5939 - val_acc: 0.7814\n",
            "Epoch 197/1000\n",
            "678/678 [==============================] - 0s 500us/step - loss: 0.5468 - acc: 0.7773 - val_loss: 0.5695 - val_acc: 0.7994\n",
            "Epoch 198/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.5653 - acc: 0.7611 - val_loss: 0.6118 - val_acc: 0.7754\n",
            "Epoch 199/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.5698 - acc: 0.7773 - val_loss: 0.5664 - val_acc: 0.7994\n",
            "Epoch 200/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.5517 - acc: 0.8009 - val_loss: 0.5751 - val_acc: 0.7844\n",
            "Epoch 201/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.5665 - acc: 0.7802 - val_loss: 0.5715 - val_acc: 0.7934\n",
            "Epoch 202/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.5370 - acc: 0.8053 - val_loss: 0.5690 - val_acc: 0.7515\n",
            "Epoch 203/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.5416 - acc: 0.7891 - val_loss: 0.5809 - val_acc: 0.7605\n",
            "Epoch 204/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.5359 - acc: 0.7876 - val_loss: 0.5683 - val_acc: 0.7814\n",
            "Epoch 205/1000\n",
            "678/678 [==============================] - 0s 539us/step - loss: 0.5283 - acc: 0.7950 - val_loss: 0.5764 - val_acc: 0.7844\n",
            "Epoch 206/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 0.5337 - acc: 0.8009 - val_loss: 0.5785 - val_acc: 0.7814\n",
            "Epoch 207/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.5520 - acc: 0.7788 - val_loss: 0.5872 - val_acc: 0.7784\n",
            "Epoch 208/1000\n",
            "678/678 [==============================] - 0s 505us/step - loss: 0.5537 - acc: 0.7965 - val_loss: 0.5995 - val_acc: 0.7635\n",
            "Epoch 209/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.5175 - acc: 0.8053 - val_loss: 0.5605 - val_acc: 0.7994\n",
            "Epoch 210/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.5410 - acc: 0.8024 - val_loss: 0.5734 - val_acc: 0.7844\n",
            "Epoch 211/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.5280 - acc: 0.7979 - val_loss: 0.5694 - val_acc: 0.7635\n",
            "Epoch 212/1000\n",
            "678/678 [==============================] - 0s 575us/step - loss: 0.5040 - acc: 0.8083 - val_loss: 0.5789 - val_acc: 0.7874\n",
            "Epoch 213/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.5294 - acc: 0.7847 - val_loss: 0.5511 - val_acc: 0.8024\n",
            "Epoch 214/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.5323 - acc: 0.7935 - val_loss: 0.5670 - val_acc: 0.7934\n",
            "Epoch 215/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.5235 - acc: 0.8024 - val_loss: 0.5657 - val_acc: 0.7994\n",
            "Epoch 216/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.5278 - acc: 0.7847 - val_loss: 0.5589 - val_acc: 0.7874\n",
            "Epoch 217/1000\n",
            "678/678 [==============================] - 0s 503us/step - loss: 0.5271 - acc: 0.7891 - val_loss: 0.6343 - val_acc: 0.7575\n",
            "Epoch 218/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.5053 - acc: 0.8068 - val_loss: 0.6125 - val_acc: 0.7784\n",
            "Epoch 219/1000\n",
            "678/678 [==============================] - 0s 539us/step - loss: 0.4970 - acc: 0.7861 - val_loss: 0.5876 - val_acc: 0.7695\n",
            "Epoch 220/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.5199 - acc: 0.8068 - val_loss: 0.5614 - val_acc: 0.7784\n",
            "Epoch 221/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.5082 - acc: 0.8024 - val_loss: 0.5807 - val_acc: 0.7784\n",
            "Epoch 222/1000\n",
            "678/678 [==============================] - 0s 554us/step - loss: 0.5079 - acc: 0.7861 - val_loss: 0.5582 - val_acc: 0.7784\n",
            "Epoch 223/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.5014 - acc: 0.8201 - val_loss: 0.5841 - val_acc: 0.7605\n",
            "Epoch 224/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.5060 - acc: 0.8171 - val_loss: 0.5664 - val_acc: 0.7725\n",
            "Epoch 225/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.4980 - acc: 0.7979 - val_loss: 0.5377 - val_acc: 0.8114\n",
            "Epoch 226/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.5067 - acc: 0.8009 - val_loss: 0.5487 - val_acc: 0.7874\n",
            "Epoch 227/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.4904 - acc: 0.8127 - val_loss: 0.5674 - val_acc: 0.7994\n",
            "Epoch 228/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.5044 - acc: 0.8009 - val_loss: 0.5535 - val_acc: 0.7844\n",
            "Epoch 229/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.5018 - acc: 0.7965 - val_loss: 0.5435 - val_acc: 0.7784\n",
            "Epoch 230/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.4962 - acc: 0.8112 - val_loss: 0.5373 - val_acc: 0.7964\n",
            "Epoch 231/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.4820 - acc: 0.8024 - val_loss: 0.5623 - val_acc: 0.7635\n",
            "Epoch 232/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.5009 - acc: 0.8215 - val_loss: 0.5673 - val_acc: 0.7754\n",
            "Epoch 233/1000\n",
            "678/678 [==============================] - 0s 501us/step - loss: 0.4974 - acc: 0.8024 - val_loss: 0.5394 - val_acc: 0.7874\n",
            "Epoch 234/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.4747 - acc: 0.8068 - val_loss: 0.5474 - val_acc: 0.8114\n",
            "Epoch 235/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.4935 - acc: 0.7935 - val_loss: 0.5295 - val_acc: 0.8084\n",
            "Epoch 236/1000\n",
            "678/678 [==============================] - 0s 502us/step - loss: 0.4810 - acc: 0.8171 - val_loss: 0.5371 - val_acc: 0.7844\n",
            "Epoch 237/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.4791 - acc: 0.8171 - val_loss: 0.5461 - val_acc: 0.7964\n",
            "Epoch 238/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.4713 - acc: 0.8112 - val_loss: 0.5739 - val_acc: 0.7754\n",
            "Epoch 239/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.4938 - acc: 0.8024 - val_loss: 0.5430 - val_acc: 0.7964\n",
            "Epoch 240/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.4797 - acc: 0.8215 - val_loss: 0.5300 - val_acc: 0.7904\n",
            "Epoch 241/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.4889 - acc: 0.8053 - val_loss: 0.5341 - val_acc: 0.7964\n",
            "Epoch 242/1000\n",
            "678/678 [==============================] - 0s 572us/step - loss: 0.4686 - acc: 0.8156 - val_loss: 0.5651 - val_acc: 0.7754\n",
            "Epoch 243/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.4655 - acc: 0.8171 - val_loss: 0.5354 - val_acc: 0.8174\n",
            "Epoch 244/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.4455 - acc: 0.8333 - val_loss: 0.5612 - val_acc: 0.7844\n",
            "Epoch 245/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.4743 - acc: 0.8127 - val_loss: 0.5333 - val_acc: 0.7934\n",
            "Epoch 246/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.4437 - acc: 0.8186 - val_loss: 0.5654 - val_acc: 0.7874\n",
            "Epoch 247/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.4910 - acc: 0.8053 - val_loss: 0.5381 - val_acc: 0.8144\n",
            "Epoch 248/1000\n",
            "678/678 [==============================] - 0s 509us/step - loss: 0.4579 - acc: 0.8363 - val_loss: 0.5908 - val_acc: 0.7665\n",
            "Epoch 249/1000\n",
            "678/678 [==============================] - 0s 502us/step - loss: 0.4589 - acc: 0.8171 - val_loss: 0.5399 - val_acc: 0.7904\n",
            "Epoch 250/1000\n",
            "678/678 [==============================] - 0s 504us/step - loss: 0.4735 - acc: 0.8171 - val_loss: 0.5134 - val_acc: 0.8054\n",
            "Epoch 251/1000\n",
            "678/678 [==============================] - 0s 476us/step - loss: 0.4490 - acc: 0.8304 - val_loss: 0.6094 - val_acc: 0.7725\n",
            "Epoch 252/1000\n",
            "678/678 [==============================] - 0s 491us/step - loss: 0.4531 - acc: 0.8230 - val_loss: 0.5532 - val_acc: 0.7844\n",
            "Epoch 253/1000\n",
            "678/678 [==============================] - 0s 495us/step - loss: 0.4622 - acc: 0.8127 - val_loss: 0.6008 - val_acc: 0.7695\n",
            "Epoch 254/1000\n",
            "678/678 [==============================] - 0s 502us/step - loss: 0.4888 - acc: 0.8127 - val_loss: 0.5954 - val_acc: 0.7605\n",
            "Epoch 255/1000\n",
            "678/678 [==============================] - 0s 493us/step - loss: 0.4625 - acc: 0.8156 - val_loss: 0.5796 - val_acc: 0.7545\n",
            "Epoch 256/1000\n",
            "678/678 [==============================] - 0s 472us/step - loss: 0.4599 - acc: 0.8260 - val_loss: 0.5106 - val_acc: 0.8054\n",
            "Epoch 257/1000\n",
            "678/678 [==============================] - 0s 491us/step - loss: 0.4606 - acc: 0.8053 - val_loss: 0.5279 - val_acc: 0.8084\n",
            "Epoch 258/1000\n",
            "678/678 [==============================] - 0s 479us/step - loss: 0.4637 - acc: 0.8112 - val_loss: 0.5364 - val_acc: 0.7874\n",
            "Epoch 259/1000\n",
            "678/678 [==============================] - 0s 486us/step - loss: 0.4457 - acc: 0.8407 - val_loss: 0.5185 - val_acc: 0.8054\n",
            "Epoch 260/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.4572 - acc: 0.8230 - val_loss: 0.5051 - val_acc: 0.8174\n",
            "Epoch 261/1000\n",
            "678/678 [==============================] - 0s 474us/step - loss: 0.4518 - acc: 0.8289 - val_loss: 0.5072 - val_acc: 0.8024\n",
            "Epoch 262/1000\n",
            "678/678 [==============================] - 0s 497us/step - loss: 0.4273 - acc: 0.8289 - val_loss: 0.5143 - val_acc: 0.7994\n",
            "Epoch 263/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.4220 - acc: 0.8363 - val_loss: 0.5606 - val_acc: 0.7844\n",
            "Epoch 264/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.4407 - acc: 0.8392 - val_loss: 0.5071 - val_acc: 0.7934\n",
            "Epoch 265/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.4347 - acc: 0.8304 - val_loss: 0.5329 - val_acc: 0.7874\n",
            "Epoch 266/1000\n",
            "678/678 [==============================] - 0s 549us/step - loss: 0.4606 - acc: 0.8156 - val_loss: 0.5310 - val_acc: 0.7934\n",
            "Epoch 267/1000\n",
            "678/678 [==============================] - 0s 555us/step - loss: 0.4262 - acc: 0.8378 - val_loss: 0.5393 - val_acc: 0.7874\n",
            "Epoch 268/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.4040 - acc: 0.8422 - val_loss: 0.5183 - val_acc: 0.7934\n",
            "Epoch 269/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.4329 - acc: 0.8186 - val_loss: 0.5238 - val_acc: 0.8054\n",
            "Epoch 270/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.4454 - acc: 0.8215 - val_loss: 0.4963 - val_acc: 0.8114\n",
            "Epoch 271/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.4609 - acc: 0.8171 - val_loss: 0.5123 - val_acc: 0.7874\n",
            "Epoch 272/1000\n",
            "678/678 [==============================] - 0s 574us/step - loss: 0.4363 - acc: 0.8260 - val_loss: 0.5012 - val_acc: 0.8084\n",
            "Epoch 273/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.4145 - acc: 0.8496 - val_loss: 0.5574 - val_acc: 0.7725\n",
            "Epoch 274/1000\n",
            "678/678 [==============================] - 0s 557us/step - loss: 0.4484 - acc: 0.8171 - val_loss: 0.5820 - val_acc: 0.7754\n",
            "Epoch 275/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.4089 - acc: 0.8496 - val_loss: 0.5010 - val_acc: 0.8293\n",
            "Epoch 276/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.4110 - acc: 0.8304 - val_loss: 0.5058 - val_acc: 0.8144\n",
            "Epoch 277/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.4237 - acc: 0.8245 - val_loss: 0.5035 - val_acc: 0.8024\n",
            "Epoch 278/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.4302 - acc: 0.8348 - val_loss: 0.5144 - val_acc: 0.7874\n",
            "Epoch 279/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.3935 - acc: 0.8555 - val_loss: 0.5052 - val_acc: 0.7994\n",
            "Epoch 280/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.4251 - acc: 0.8260 - val_loss: 0.5446 - val_acc: 0.7784\n",
            "Epoch 281/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.4180 - acc: 0.8363 - val_loss: 0.5253 - val_acc: 0.8054\n",
            "Epoch 282/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.4105 - acc: 0.8260 - val_loss: 0.5412 - val_acc: 0.7814\n",
            "Epoch 283/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.4099 - acc: 0.8510 - val_loss: 0.4885 - val_acc: 0.8114\n",
            "Epoch 284/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.4157 - acc: 0.8437 - val_loss: 0.4832 - val_acc: 0.8234\n",
            "Epoch 285/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.4165 - acc: 0.8392 - val_loss: 0.4869 - val_acc: 0.8204\n",
            "Epoch 286/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 0.4170 - acc: 0.8333 - val_loss: 0.4968 - val_acc: 0.8114\n",
            "Epoch 287/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.3913 - acc: 0.8510 - val_loss: 0.5133 - val_acc: 0.8114\n",
            "Epoch 288/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.4207 - acc: 0.8363 - val_loss: 0.4923 - val_acc: 0.8204\n",
            "Epoch 289/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.4094 - acc: 0.8525 - val_loss: 0.4995 - val_acc: 0.8054\n",
            "Epoch 290/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.3898 - acc: 0.8569 - val_loss: 0.4783 - val_acc: 0.8263\n",
            "Epoch 291/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.4048 - acc: 0.8363 - val_loss: 0.5083 - val_acc: 0.8054\n",
            "Epoch 292/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.4099 - acc: 0.8392 - val_loss: 0.5261 - val_acc: 0.7904\n",
            "Epoch 293/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.4201 - acc: 0.8407 - val_loss: 0.4816 - val_acc: 0.8174\n",
            "Epoch 294/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.3981 - acc: 0.8496 - val_loss: 0.4843 - val_acc: 0.7994\n",
            "Epoch 295/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.4104 - acc: 0.8422 - val_loss: 0.4747 - val_acc: 0.8204\n",
            "Epoch 296/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.3947 - acc: 0.8481 - val_loss: 0.5073 - val_acc: 0.8144\n",
            "Epoch 297/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.3817 - acc: 0.8437 - val_loss: 0.4850 - val_acc: 0.8204\n",
            "Epoch 298/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.4001 - acc: 0.8466 - val_loss: 0.4934 - val_acc: 0.8084\n",
            "Epoch 299/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.4117 - acc: 0.8407 - val_loss: 0.5645 - val_acc: 0.7844\n",
            "Epoch 300/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.3920 - acc: 0.8496 - val_loss: 0.5293 - val_acc: 0.7874\n",
            "Epoch 301/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.3878 - acc: 0.8540 - val_loss: 0.5037 - val_acc: 0.8204\n",
            "Epoch 302/1000\n",
            "678/678 [==============================] - 0s 553us/step - loss: 0.3793 - acc: 0.8496 - val_loss: 0.4786 - val_acc: 0.8234\n",
            "Epoch 303/1000\n",
            "678/678 [==============================] - 0s 548us/step - loss: 0.4212 - acc: 0.8422 - val_loss: 0.4681 - val_acc: 0.8263\n",
            "Epoch 304/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.3802 - acc: 0.8584 - val_loss: 0.4613 - val_acc: 0.8204\n",
            "Epoch 305/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.4080 - acc: 0.8378 - val_loss: 0.4851 - val_acc: 0.8174\n",
            "Epoch 306/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.3811 - acc: 0.8422 - val_loss: 0.5099 - val_acc: 0.8114\n",
            "Epoch 307/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.3752 - acc: 0.8510 - val_loss: 0.5637 - val_acc: 0.7754\n",
            "Epoch 308/1000\n",
            "678/678 [==============================] - 0s 549us/step - loss: 0.3504 - acc: 0.8628 - val_loss: 0.4636 - val_acc: 0.8293\n",
            "Epoch 309/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.3685 - acc: 0.8614 - val_loss: 0.4591 - val_acc: 0.8263\n",
            "Epoch 310/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.3758 - acc: 0.8569 - val_loss: 0.4994 - val_acc: 0.8204\n",
            "Epoch 311/1000\n",
            "678/678 [==============================] - 0s 561us/step - loss: 0.3947 - acc: 0.8437 - val_loss: 0.4845 - val_acc: 0.8144\n",
            "Epoch 312/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.4006 - acc: 0.8466 - val_loss: 0.4875 - val_acc: 0.8054\n",
            "Epoch 313/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.3611 - acc: 0.8791 - val_loss: 0.5129 - val_acc: 0.7964\n",
            "Epoch 314/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.3947 - acc: 0.8451 - val_loss: 0.5245 - val_acc: 0.8054\n",
            "Epoch 315/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.3523 - acc: 0.8791 - val_loss: 0.4836 - val_acc: 0.8084\n",
            "Epoch 316/1000\n",
            "678/678 [==============================] - 0s 551us/step - loss: 0.3867 - acc: 0.8451 - val_loss: 0.5269 - val_acc: 0.7874\n",
            "Epoch 317/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.3744 - acc: 0.8481 - val_loss: 0.5203 - val_acc: 0.8114\n",
            "Epoch 318/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.3952 - acc: 0.8510 - val_loss: 0.4695 - val_acc: 0.8114\n",
            "Epoch 319/1000\n",
            "678/678 [==============================] - 0s 552us/step - loss: 0.3867 - acc: 0.8525 - val_loss: 0.4603 - val_acc: 0.8234\n",
            "Epoch 320/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.3753 - acc: 0.8584 - val_loss: 0.4723 - val_acc: 0.8204\n",
            "Epoch 321/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.3666 - acc: 0.8732 - val_loss: 0.4713 - val_acc: 0.8263\n",
            "Epoch 322/1000\n",
            "678/678 [==============================] - 0s 541us/step - loss: 0.3690 - acc: 0.8643 - val_loss: 0.4756 - val_acc: 0.8174\n",
            "Epoch 323/1000\n",
            "678/678 [==============================] - 0s 565us/step - loss: 0.3839 - acc: 0.8525 - val_loss: 0.4751 - val_acc: 0.8114\n",
            "Epoch 324/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.3890 - acc: 0.8540 - val_loss: 0.4896 - val_acc: 0.7964\n",
            "Epoch 325/1000\n",
            "678/678 [==============================] - 0s 541us/step - loss: 0.3593 - acc: 0.8525 - val_loss: 0.4686 - val_acc: 0.8054\n",
            "Epoch 326/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.3595 - acc: 0.8525 - val_loss: 0.4885 - val_acc: 0.8144\n",
            "Epoch 327/1000\n",
            "678/678 [==============================] - 0s 550us/step - loss: 0.3686 - acc: 0.8496 - val_loss: 0.5187 - val_acc: 0.7784\n",
            "Epoch 328/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.3738 - acc: 0.8437 - val_loss: 0.4987 - val_acc: 0.8114\n",
            "Epoch 329/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.3564 - acc: 0.8702 - val_loss: 0.4643 - val_acc: 0.8234\n",
            "Epoch 330/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.3824 - acc: 0.8451 - val_loss: 0.4701 - val_acc: 0.8174\n",
            "Epoch 331/1000\n",
            "678/678 [==============================] - 0s 576us/step - loss: 0.3494 - acc: 0.8717 - val_loss: 0.4996 - val_acc: 0.8024\n",
            "Epoch 332/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.3607 - acc: 0.8702 - val_loss: 0.4706 - val_acc: 0.8234\n",
            "Epoch 333/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.3386 - acc: 0.8776 - val_loss: 0.4903 - val_acc: 0.8084\n",
            "Epoch 334/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.3520 - acc: 0.8673 - val_loss: 0.4966 - val_acc: 0.8084\n",
            "Epoch 335/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.3586 - acc: 0.8761 - val_loss: 0.4762 - val_acc: 0.8204\n",
            "Epoch 336/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.3757 - acc: 0.8496 - val_loss: 0.4564 - val_acc: 0.8234\n",
            "Epoch 337/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.3438 - acc: 0.8746 - val_loss: 0.4800 - val_acc: 0.8114\n",
            "Epoch 338/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.3326 - acc: 0.8864 - val_loss: 0.4735 - val_acc: 0.8114\n",
            "Epoch 339/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.3619 - acc: 0.8510 - val_loss: 0.5126 - val_acc: 0.7904\n",
            "Epoch 340/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.3315 - acc: 0.8761 - val_loss: 0.5451 - val_acc: 0.7904\n",
            "Epoch 341/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.3596 - acc: 0.8584 - val_loss: 0.4427 - val_acc: 0.8204\n",
            "Epoch 342/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.3485 - acc: 0.8687 - val_loss: 0.4466 - val_acc: 0.8263\n",
            "Epoch 343/1000\n",
            "678/678 [==============================] - 0s 501us/step - loss: 0.3408 - acc: 0.8717 - val_loss: 0.4594 - val_acc: 0.8204\n",
            "Epoch 344/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.3580 - acc: 0.8687 - val_loss: 0.4547 - val_acc: 0.8174\n",
            "Epoch 345/1000\n",
            "678/678 [==============================] - 0s 501us/step - loss: 0.3417 - acc: 0.8673 - val_loss: 0.5597 - val_acc: 0.7605\n",
            "Epoch 346/1000\n",
            "678/678 [==============================] - 0s 481us/step - loss: 0.3544 - acc: 0.8673 - val_loss: 0.4358 - val_acc: 0.8323\n",
            "Epoch 347/1000\n",
            "678/678 [==============================] - 0s 509us/step - loss: 0.3384 - acc: 0.8673 - val_loss: 0.5704 - val_acc: 0.7754\n",
            "Epoch 348/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.3516 - acc: 0.8510 - val_loss: 0.4794 - val_acc: 0.8054\n",
            "Epoch 349/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.3375 - acc: 0.8658 - val_loss: 0.4472 - val_acc: 0.8174\n",
            "Epoch 350/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.3393 - acc: 0.8746 - val_loss: 0.4489 - val_acc: 0.8204\n",
            "Epoch 351/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.3527 - acc: 0.8673 - val_loss: 0.5165 - val_acc: 0.7844\n",
            "Epoch 352/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.3407 - acc: 0.8717 - val_loss: 0.4658 - val_acc: 0.8114\n",
            "Epoch 353/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.3549 - acc: 0.8628 - val_loss: 0.4563 - val_acc: 0.8174\n",
            "Epoch 354/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.3408 - acc: 0.8687 - val_loss: 0.4405 - val_acc: 0.8263\n",
            "Epoch 355/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.3588 - acc: 0.8496 - val_loss: 0.4649 - val_acc: 0.8144\n",
            "Epoch 356/1000\n",
            "678/678 [==============================] - 0s 504us/step - loss: 0.3247 - acc: 0.8791 - val_loss: 0.4667 - val_acc: 0.8054\n",
            "Epoch 357/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.3359 - acc: 0.8717 - val_loss: 0.4745 - val_acc: 0.8114\n",
            "Epoch 358/1000\n",
            "678/678 [==============================] - 0s 509us/step - loss: 0.3340 - acc: 0.8658 - val_loss: 0.4384 - val_acc: 0.8144\n",
            "Epoch 359/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.3239 - acc: 0.8702 - val_loss: 0.4988 - val_acc: 0.8054\n",
            "Epoch 360/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.3252 - acc: 0.8805 - val_loss: 0.5095 - val_acc: 0.8114\n",
            "Epoch 361/1000\n",
            "678/678 [==============================] - 0s 572us/step - loss: 0.3039 - acc: 0.8953 - val_loss: 0.4514 - val_acc: 0.8234\n",
            "Epoch 362/1000\n",
            "678/678 [==============================] - 0s 502us/step - loss: 0.3280 - acc: 0.8658 - val_loss: 0.4420 - val_acc: 0.8263\n",
            "Epoch 363/1000\n",
            "678/678 [==============================] - 0s 481us/step - loss: 0.3312 - acc: 0.8776 - val_loss: 0.4266 - val_acc: 0.8293\n",
            "Epoch 364/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.3050 - acc: 0.8776 - val_loss: 0.4693 - val_acc: 0.8234\n",
            "Epoch 365/1000\n",
            "678/678 [==============================] - 0s 479us/step - loss: 0.3136 - acc: 0.8717 - val_loss: 0.5075 - val_acc: 0.8114\n",
            "Epoch 366/1000\n",
            "678/678 [==============================] - 0s 494us/step - loss: 0.3200 - acc: 0.8702 - val_loss: 0.4928 - val_acc: 0.7964\n",
            "Epoch 367/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.3218 - acc: 0.8732 - val_loss: 0.5545 - val_acc: 0.7814\n",
            "Epoch 368/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.3104 - acc: 0.8938 - val_loss: 0.5546 - val_acc: 0.7814\n",
            "Epoch 369/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.3063 - acc: 0.8923 - val_loss: 0.4461 - val_acc: 0.8263\n",
            "Epoch 370/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.3168 - acc: 0.8805 - val_loss: 0.4319 - val_acc: 0.8263\n",
            "Epoch 371/1000\n",
            "678/678 [==============================] - 0s 541us/step - loss: 0.3076 - acc: 0.8717 - val_loss: 0.4933 - val_acc: 0.8084\n",
            "Epoch 372/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.3118 - acc: 0.8909 - val_loss: 0.4582 - val_acc: 0.8144\n",
            "Epoch 373/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.3335 - acc: 0.8643 - val_loss: 0.4401 - val_acc: 0.8234\n",
            "Epoch 374/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.3300 - acc: 0.8687 - val_loss: 0.4308 - val_acc: 0.8293\n",
            "Epoch 375/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.3301 - acc: 0.8658 - val_loss: 0.4666 - val_acc: 0.8084\n",
            "Epoch 376/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.3366 - acc: 0.8791 - val_loss: 0.4739 - val_acc: 0.8054\n",
            "Epoch 377/1000\n",
            "678/678 [==============================] - 0s 498us/step - loss: 0.3105 - acc: 0.8820 - val_loss: 0.4528 - val_acc: 0.8293\n",
            "Epoch 378/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.3241 - acc: 0.8923 - val_loss: 0.4315 - val_acc: 0.8323\n",
            "Epoch 379/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.3121 - acc: 0.8894 - val_loss: 0.4499 - val_acc: 0.8204\n",
            "Epoch 380/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.2975 - acc: 0.8850 - val_loss: 0.4513 - val_acc: 0.8263\n",
            "Epoch 381/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.2859 - acc: 0.9056 - val_loss: 0.4997 - val_acc: 0.7904\n",
            "Epoch 382/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.3209 - acc: 0.8717 - val_loss: 0.4416 - val_acc: 0.8263\n",
            "Epoch 383/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.3071 - acc: 0.8894 - val_loss: 0.4504 - val_acc: 0.8263\n",
            "Epoch 384/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.2993 - acc: 0.8879 - val_loss: 0.4584 - val_acc: 0.8204\n",
            "Epoch 385/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.3011 - acc: 0.8909 - val_loss: 0.4847 - val_acc: 0.8114\n",
            "Epoch 386/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.3119 - acc: 0.8732 - val_loss: 0.4516 - val_acc: 0.8204\n",
            "Epoch 387/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.3126 - acc: 0.8864 - val_loss: 0.4466 - val_acc: 0.8204\n",
            "Epoch 388/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.3170 - acc: 0.8805 - val_loss: 0.5308 - val_acc: 0.7814\n",
            "Epoch 389/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.3013 - acc: 0.8864 - val_loss: 0.4288 - val_acc: 0.8293\n",
            "Epoch 390/1000\n",
            "678/678 [==============================] - 0s 575us/step - loss: 0.2866 - acc: 0.9086 - val_loss: 0.4477 - val_acc: 0.8263\n",
            "Epoch 391/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.2891 - acc: 0.8894 - val_loss: 0.4630 - val_acc: 0.8144\n",
            "Epoch 392/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.3159 - acc: 0.8894 - val_loss: 0.4511 - val_acc: 0.8084\n",
            "Epoch 393/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.2943 - acc: 0.8864 - val_loss: 0.4592 - val_acc: 0.8144\n",
            "Epoch 394/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.3049 - acc: 0.8923 - val_loss: 0.4398 - val_acc: 0.8383\n",
            "Epoch 395/1000\n",
            "678/678 [==============================] - 0s 549us/step - loss: 0.3082 - acc: 0.8820 - val_loss: 0.4896 - val_acc: 0.8144\n",
            "Epoch 396/1000\n",
            "678/678 [==============================] - 0s 539us/step - loss: 0.3132 - acc: 0.8732 - val_loss: 0.5210 - val_acc: 0.7904\n",
            "Epoch 397/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.3286 - acc: 0.8791 - val_loss: 0.4232 - val_acc: 0.8353\n",
            "Epoch 398/1000\n",
            "678/678 [==============================] - 0s 498us/step - loss: 0.3030 - acc: 0.8732 - val_loss: 0.4546 - val_acc: 0.8383\n",
            "Epoch 399/1000\n",
            "678/678 [==============================] - 0s 501us/step - loss: 0.2981 - acc: 0.8909 - val_loss: 0.4253 - val_acc: 0.8473\n",
            "Epoch 400/1000\n",
            "678/678 [==============================] - 0s 504us/step - loss: 0.2986 - acc: 0.8761 - val_loss: 0.5029 - val_acc: 0.7994\n",
            "Epoch 401/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.3129 - acc: 0.8776 - val_loss: 0.4522 - val_acc: 0.8174\n",
            "Epoch 402/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.2890 - acc: 0.8894 - val_loss: 0.4241 - val_acc: 0.8204\n",
            "Epoch 403/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.2874 - acc: 0.8938 - val_loss: 0.4847 - val_acc: 0.8024\n",
            "Epoch 404/1000\n",
            "678/678 [==============================] - 0s 550us/step - loss: 0.2800 - acc: 0.8938 - val_loss: 0.4601 - val_acc: 0.8353\n",
            "Epoch 405/1000\n",
            "678/678 [==============================] - 0s 545us/step - loss: 0.3035 - acc: 0.8894 - val_loss: 0.4450 - val_acc: 0.8114\n",
            "Epoch 406/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.2739 - acc: 0.9012 - val_loss: 0.4217 - val_acc: 0.8413\n",
            "Epoch 407/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.2810 - acc: 0.8923 - val_loss: 0.5015 - val_acc: 0.8054\n",
            "Epoch 408/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.2536 - acc: 0.9130 - val_loss: 0.4280 - val_acc: 0.8323\n",
            "Epoch 409/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.2806 - acc: 0.8968 - val_loss: 0.4858 - val_acc: 0.8234\n",
            "Epoch 410/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.3158 - acc: 0.8820 - val_loss: 0.4535 - val_acc: 0.8293\n",
            "Epoch 411/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.2589 - acc: 0.9100 - val_loss: 0.5383 - val_acc: 0.7904\n",
            "Epoch 412/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.2959 - acc: 0.8820 - val_loss: 0.6173 - val_acc: 0.7575\n",
            "Epoch 413/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.2594 - acc: 0.9056 - val_loss: 0.4271 - val_acc: 0.8323\n",
            "Epoch 414/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.2758 - acc: 0.8968 - val_loss: 0.4290 - val_acc: 0.8353\n",
            "Epoch 415/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.2889 - acc: 0.8909 - val_loss: 0.4104 - val_acc: 0.8234\n",
            "Epoch 416/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.2801 - acc: 0.8968 - val_loss: 0.4241 - val_acc: 0.8353\n",
            "Epoch 417/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.2697 - acc: 0.8997 - val_loss: 0.4453 - val_acc: 0.8323\n",
            "Epoch 418/1000\n",
            "678/678 [==============================] - 0s 480us/step - loss: 0.2930 - acc: 0.8894 - val_loss: 0.4633 - val_acc: 0.8174\n",
            "Epoch 419/1000\n",
            "678/678 [==============================] - 0s 485us/step - loss: 0.2839 - acc: 0.8982 - val_loss: 0.4098 - val_acc: 0.8263\n",
            "Epoch 420/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.2789 - acc: 0.9100 - val_loss: 0.4792 - val_acc: 0.8144\n",
            "Epoch 421/1000\n",
            "678/678 [==============================] - 0s 484us/step - loss: 0.3014 - acc: 0.8805 - val_loss: 0.4184 - val_acc: 0.8263\n",
            "Epoch 422/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.2813 - acc: 0.8968 - val_loss: 0.4183 - val_acc: 0.8473\n",
            "Epoch 423/1000\n",
            "678/678 [==============================] - 0s 494us/step - loss: 0.2891 - acc: 0.8805 - val_loss: 0.4438 - val_acc: 0.8263\n",
            "Epoch 424/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.2855 - acc: 0.9115 - val_loss: 0.4143 - val_acc: 0.8293\n",
            "Epoch 425/1000\n",
            "678/678 [==============================] - 0s 485us/step - loss: 0.2777 - acc: 0.8820 - val_loss: 0.4128 - val_acc: 0.8293\n",
            "Epoch 426/1000\n",
            "678/678 [==============================] - 0s 485us/step - loss: 0.2746 - acc: 0.9100 - val_loss: 0.4242 - val_acc: 0.8413\n",
            "Epoch 427/1000\n",
            "678/678 [==============================] - 0s 493us/step - loss: 0.2835 - acc: 0.9012 - val_loss: 0.4520 - val_acc: 0.8263\n",
            "Epoch 428/1000\n",
            "678/678 [==============================] - 0s 488us/step - loss: 0.2775 - acc: 0.8909 - val_loss: 0.4301 - val_acc: 0.8293\n",
            "Epoch 429/1000\n",
            "678/678 [==============================] - 0s 500us/step - loss: 0.2928 - acc: 0.8909 - val_loss: 0.4169 - val_acc: 0.8263\n",
            "Epoch 430/1000\n",
            "678/678 [==============================] - 0s 494us/step - loss: 0.2849 - acc: 0.8938 - val_loss: 0.4355 - val_acc: 0.8323\n",
            "Epoch 431/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.2767 - acc: 0.8968 - val_loss: 0.4442 - val_acc: 0.8383\n",
            "Epoch 432/1000\n",
            "678/678 [==============================] - 0s 545us/step - loss: 0.2781 - acc: 0.8938 - val_loss: 0.4540 - val_acc: 0.8144\n",
            "Epoch 433/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.2564 - acc: 0.9012 - val_loss: 0.4656 - val_acc: 0.8114\n",
            "Epoch 434/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.2870 - acc: 0.8953 - val_loss: 0.4059 - val_acc: 0.8353\n",
            "Epoch 435/1000\n",
            "678/678 [==============================] - 0s 556us/step - loss: 0.3040 - acc: 0.8761 - val_loss: 0.4457 - val_acc: 0.7964\n",
            "Epoch 436/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.2704 - acc: 0.9012 - val_loss: 0.4241 - val_acc: 0.8353\n",
            "Epoch 437/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.2889 - acc: 0.8938 - val_loss: 0.4344 - val_acc: 0.8413\n",
            "Epoch 438/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.2812 - acc: 0.9041 - val_loss: 0.4435 - val_acc: 0.8413\n",
            "Epoch 439/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.2733 - acc: 0.9100 - val_loss: 0.4255 - val_acc: 0.8413\n",
            "Epoch 440/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.2708 - acc: 0.8953 - val_loss: 0.4388 - val_acc: 0.8263\n",
            "Epoch 441/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.2550 - acc: 0.9027 - val_loss: 0.4196 - val_acc: 0.8353\n",
            "Epoch 442/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.2628 - acc: 0.8894 - val_loss: 0.4535 - val_acc: 0.8114\n",
            "Epoch 443/1000\n",
            "678/678 [==============================] - 0s 548us/step - loss: 0.2708 - acc: 0.8835 - val_loss: 0.4592 - val_acc: 0.8204\n",
            "Epoch 444/1000\n",
            "678/678 [==============================] - 0s 563us/step - loss: 0.2687 - acc: 0.9071 - val_loss: 0.5430 - val_acc: 0.7665\n",
            "Epoch 445/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.2800 - acc: 0.8938 - val_loss: 0.4556 - val_acc: 0.8293\n",
            "Epoch 446/1000\n",
            "678/678 [==============================] - 0s 552us/step - loss: 0.2513 - acc: 0.9174 - val_loss: 0.4460 - val_acc: 0.8234\n",
            "Epoch 447/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.2906 - acc: 0.8923 - val_loss: 0.4260 - val_acc: 0.8383\n",
            "Epoch 448/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.2864 - acc: 0.8923 - val_loss: 0.4045 - val_acc: 0.8383\n",
            "Epoch 449/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.2779 - acc: 0.8997 - val_loss: 0.4316 - val_acc: 0.8443\n",
            "Epoch 450/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.2564 - acc: 0.9041 - val_loss: 0.4043 - val_acc: 0.8443\n",
            "Epoch 451/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.2574 - acc: 0.9130 - val_loss: 0.4273 - val_acc: 0.8413\n",
            "Epoch 452/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.2507 - acc: 0.9145 - val_loss: 0.4018 - val_acc: 0.8383\n",
            "Epoch 453/1000\n",
            "678/678 [==============================] - 0s 553us/step - loss: 0.2407 - acc: 0.9086 - val_loss: 0.4296 - val_acc: 0.8383\n",
            "Epoch 454/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.2675 - acc: 0.8968 - val_loss: 0.4194 - val_acc: 0.8323\n",
            "Epoch 455/1000\n",
            "678/678 [==============================] - 0s 558us/step - loss: 0.2600 - acc: 0.9100 - val_loss: 0.4471 - val_acc: 0.8204\n",
            "Epoch 456/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.2605 - acc: 0.9071 - val_loss: 0.4437 - val_acc: 0.8413\n",
            "Epoch 457/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.2527 - acc: 0.9027 - val_loss: 0.5097 - val_acc: 0.8144\n",
            "Epoch 458/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.2608 - acc: 0.9012 - val_loss: 0.4950 - val_acc: 0.8114\n",
            "Epoch 459/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.2442 - acc: 0.9130 - val_loss: 0.4223 - val_acc: 0.8353\n",
            "Epoch 460/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.2421 - acc: 0.9086 - val_loss: 0.4389 - val_acc: 0.8144\n",
            "Epoch 461/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.2670 - acc: 0.8953 - val_loss: 0.4459 - val_acc: 0.8263\n",
            "Epoch 462/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.2667 - acc: 0.8982 - val_loss: 0.4173 - val_acc: 0.8353\n",
            "Epoch 463/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.2251 - acc: 0.9204 - val_loss: 0.4704 - val_acc: 0.8084\n",
            "Epoch 464/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.2494 - acc: 0.9100 - val_loss: 0.4505 - val_acc: 0.8293\n",
            "Epoch 465/1000\n",
            "678/678 [==============================] - 0s 539us/step - loss: 0.2479 - acc: 0.9086 - val_loss: 0.4958 - val_acc: 0.8144\n",
            "Epoch 466/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.2604 - acc: 0.9100 - val_loss: 0.4058 - val_acc: 0.8323\n",
            "Epoch 467/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.2529 - acc: 0.9130 - val_loss: 0.4265 - val_acc: 0.8413\n",
            "Epoch 468/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.2581 - acc: 0.8923 - val_loss: 0.4676 - val_acc: 0.8054\n",
            "Epoch 469/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.2550 - acc: 0.8997 - val_loss: 0.4650 - val_acc: 0.8174\n",
            "Epoch 470/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.2537 - acc: 0.9027 - val_loss: 0.4234 - val_acc: 0.8204\n",
            "Epoch 471/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.2502 - acc: 0.9056 - val_loss: 0.4461 - val_acc: 0.8234\n",
            "Epoch 472/1000\n",
            "678/678 [==============================] - 0s 545us/step - loss: 0.2532 - acc: 0.8968 - val_loss: 0.5196 - val_acc: 0.8024\n",
            "Epoch 473/1000\n",
            "678/678 [==============================] - 0s 559us/step - loss: 0.2548 - acc: 0.8982 - val_loss: 0.4928 - val_acc: 0.7844\n",
            "Epoch 474/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.2505 - acc: 0.9145 - val_loss: 0.4216 - val_acc: 0.8234\n",
            "Epoch 475/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.2322 - acc: 0.9189 - val_loss: 0.4470 - val_acc: 0.8084\n",
            "Epoch 476/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.2687 - acc: 0.8864 - val_loss: 0.4100 - val_acc: 0.8383\n",
            "Epoch 477/1000\n",
            "678/678 [==============================] - 0s 560us/step - loss: 0.2594 - acc: 0.8968 - val_loss: 0.4479 - val_acc: 0.8114\n",
            "Epoch 478/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.2434 - acc: 0.9130 - val_loss: 0.4206 - val_acc: 0.8383\n",
            "Epoch 479/1000\n",
            "678/678 [==============================] - 0s 581us/step - loss: 0.2652 - acc: 0.9027 - val_loss: 0.4395 - val_acc: 0.8293\n",
            "Epoch 480/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.2456 - acc: 0.9218 - val_loss: 0.4109 - val_acc: 0.8383\n",
            "Epoch 481/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.2580 - acc: 0.9027 - val_loss: 0.4818 - val_acc: 0.8084\n",
            "Epoch 482/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.2408 - acc: 0.9189 - val_loss: 0.4051 - val_acc: 0.8473\n",
            "Epoch 483/1000\n",
            "678/678 [==============================] - 0s 539us/step - loss: 0.2326 - acc: 0.9204 - val_loss: 0.4778 - val_acc: 0.8054\n",
            "Epoch 484/1000\n",
            "678/678 [==============================] - 0s 577us/step - loss: 0.2333 - acc: 0.9130 - val_loss: 0.4218 - val_acc: 0.8383\n",
            "Epoch 485/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.2181 - acc: 0.9292 - val_loss: 0.5043 - val_acc: 0.7844\n",
            "Epoch 486/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.2421 - acc: 0.9027 - val_loss: 0.4000 - val_acc: 0.8263\n",
            "Epoch 487/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.2496 - acc: 0.8953 - val_loss: 0.4176 - val_acc: 0.8563\n",
            "Epoch 488/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.2370 - acc: 0.9115 - val_loss: 0.4005 - val_acc: 0.8353\n",
            "Epoch 489/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.2176 - acc: 0.9218 - val_loss: 0.5551 - val_acc: 0.7695\n",
            "Epoch 490/1000\n",
            "678/678 [==============================] - 0s 542us/step - loss: 0.2483 - acc: 0.8938 - val_loss: 0.4076 - val_acc: 0.8323\n",
            "Epoch 491/1000\n",
            "678/678 [==============================] - 0s 550us/step - loss: 0.2572 - acc: 0.8982 - val_loss: 0.4367 - val_acc: 0.8204\n",
            "Epoch 492/1000\n",
            "678/678 [==============================] - 0s 575us/step - loss: 0.2401 - acc: 0.9086 - val_loss: 0.4594 - val_acc: 0.8174\n",
            "Epoch 493/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.2483 - acc: 0.9086 - val_loss: 0.4119 - val_acc: 0.8293\n",
            "Epoch 494/1000\n",
            "678/678 [==============================] - 0s 559us/step - loss: 0.2232 - acc: 0.9145 - val_loss: 0.4220 - val_acc: 0.8413\n",
            "Epoch 495/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.2134 - acc: 0.9307 - val_loss: 0.4159 - val_acc: 0.8413\n",
            "Epoch 496/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.2375 - acc: 0.9100 - val_loss: 0.4066 - val_acc: 0.8473\n",
            "Epoch 497/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.2367 - acc: 0.9159 - val_loss: 0.3878 - val_acc: 0.8533\n",
            "Epoch 498/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.2310 - acc: 0.9174 - val_loss: 0.4414 - val_acc: 0.8323\n",
            "Epoch 499/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.2296 - acc: 0.9322 - val_loss: 0.4274 - val_acc: 0.8353\n",
            "Epoch 500/1000\n",
            "678/678 [==============================] - 0s 547us/step - loss: 0.2314 - acc: 0.9130 - val_loss: 0.4679 - val_acc: 0.8144\n",
            "Epoch 501/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.2358 - acc: 0.9071 - val_loss: 0.4324 - val_acc: 0.8323\n",
            "Epoch 502/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.2051 - acc: 0.9307 - val_loss: 0.4145 - val_acc: 0.8234\n",
            "Epoch 503/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.2238 - acc: 0.9145 - val_loss: 0.3996 - val_acc: 0.8593\n",
            "Epoch 504/1000\n",
            "678/678 [==============================] - 0s 553us/step - loss: 0.2440 - acc: 0.9100 - val_loss: 0.4007 - val_acc: 0.8563\n",
            "Epoch 505/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.2251 - acc: 0.9086 - val_loss: 0.4091 - val_acc: 0.8443\n",
            "Epoch 506/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.2102 - acc: 0.9174 - val_loss: 0.4136 - val_acc: 0.8353\n",
            "Epoch 507/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.2296 - acc: 0.9145 - val_loss: 0.3840 - val_acc: 0.8413\n",
            "Epoch 508/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.2384 - acc: 0.9071 - val_loss: 0.3926 - val_acc: 0.8383\n",
            "Epoch 509/1000\n",
            "678/678 [==============================] - 0s 542us/step - loss: 0.2254 - acc: 0.9233 - val_loss: 0.4130 - val_acc: 0.8293\n",
            "Epoch 510/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.2220 - acc: 0.9145 - val_loss: 0.5053 - val_acc: 0.8144\n",
            "Epoch 511/1000\n",
            "678/678 [==============================] - 0s 505us/step - loss: 0.2331 - acc: 0.9248 - val_loss: 0.3916 - val_acc: 0.8323\n",
            "Epoch 512/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.2139 - acc: 0.9174 - val_loss: 0.4236 - val_acc: 0.8323\n",
            "Epoch 513/1000\n",
            "678/678 [==============================] - 0s 497us/step - loss: 0.2346 - acc: 0.9056 - val_loss: 0.3959 - val_acc: 0.8353\n",
            "Epoch 514/1000\n",
            "678/678 [==============================] - 0s 486us/step - loss: 0.2311 - acc: 0.9233 - val_loss: 0.4015 - val_acc: 0.8413\n",
            "Epoch 515/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.2124 - acc: 0.9189 - val_loss: 0.4296 - val_acc: 0.8383\n",
            "Epoch 516/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.2079 - acc: 0.9336 - val_loss: 0.4379 - val_acc: 0.8353\n",
            "Epoch 517/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.2281 - acc: 0.9263 - val_loss: 0.4094 - val_acc: 0.8383\n",
            "Epoch 518/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.2237 - acc: 0.9174 - val_loss: 0.4383 - val_acc: 0.8323\n",
            "Epoch 519/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.2288 - acc: 0.9086 - val_loss: 0.3976 - val_acc: 0.8413\n",
            "Epoch 520/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.2283 - acc: 0.9056 - val_loss: 0.4049 - val_acc: 0.8413\n",
            "Epoch 521/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.2027 - acc: 0.9292 - val_loss: 0.4333 - val_acc: 0.8234\n",
            "Epoch 522/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.2435 - acc: 0.9100 - val_loss: 0.4533 - val_acc: 0.8024\n",
            "Epoch 523/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.2144 - acc: 0.9322 - val_loss: 0.3926 - val_acc: 0.8653\n",
            "Epoch 524/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.2246 - acc: 0.9130 - val_loss: 0.4006 - val_acc: 0.8413\n",
            "Epoch 525/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.2191 - acc: 0.9115 - val_loss: 0.4514 - val_acc: 0.8174\n",
            "Epoch 526/1000\n",
            "678/678 [==============================] - 0s 541us/step - loss: 0.2175 - acc: 0.9307 - val_loss: 0.5599 - val_acc: 0.7874\n",
            "Epoch 527/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.2201 - acc: 0.9115 - val_loss: 0.4045 - val_acc: 0.8503\n",
            "Epoch 528/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.2273 - acc: 0.9145 - val_loss: 0.3961 - val_acc: 0.8323\n",
            "Epoch 529/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.2412 - acc: 0.9115 - val_loss: 0.3965 - val_acc: 0.8413\n",
            "Epoch 530/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.2263 - acc: 0.9263 - val_loss: 0.3900 - val_acc: 0.8353\n",
            "Epoch 531/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.2093 - acc: 0.9263 - val_loss: 0.4492 - val_acc: 0.8263\n",
            "Epoch 532/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.2084 - acc: 0.9336 - val_loss: 0.3863 - val_acc: 0.8383\n",
            "Epoch 533/1000\n",
            "678/678 [==============================] - 0s 490us/step - loss: 0.2088 - acc: 0.9307 - val_loss: 0.4183 - val_acc: 0.8293\n",
            "Epoch 534/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.2050 - acc: 0.9292 - val_loss: 0.3894 - val_acc: 0.8443\n",
            "Epoch 535/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.2099 - acc: 0.9292 - val_loss: 0.3989 - val_acc: 0.8413\n",
            "Epoch 536/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.2096 - acc: 0.9218 - val_loss: 0.3886 - val_acc: 0.8293\n",
            "Epoch 537/1000\n",
            "678/678 [==============================] - 0s 539us/step - loss: 0.2079 - acc: 0.9174 - val_loss: 0.4093 - val_acc: 0.8473\n",
            "Epoch 538/1000\n",
            "678/678 [==============================] - 0s 579us/step - loss: 0.2011 - acc: 0.9248 - val_loss: 0.3916 - val_acc: 0.8413\n",
            "Epoch 539/1000\n",
            "678/678 [==============================] - 0s 550us/step - loss: 0.2230 - acc: 0.9159 - val_loss: 0.4020 - val_acc: 0.8473\n",
            "Epoch 540/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.2218 - acc: 0.9115 - val_loss: 0.4486 - val_acc: 0.8323\n",
            "Epoch 541/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.2172 - acc: 0.9233 - val_loss: 0.3916 - val_acc: 0.8563\n",
            "Epoch 542/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.2224 - acc: 0.9056 - val_loss: 0.4947 - val_acc: 0.8114\n",
            "Epoch 543/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.2171 - acc: 0.9233 - val_loss: 0.4610 - val_acc: 0.8174\n",
            "Epoch 544/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.1840 - acc: 0.9395 - val_loss: 0.3889 - val_acc: 0.8323\n",
            "Epoch 545/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.2032 - acc: 0.9263 - val_loss: 0.3923 - val_acc: 0.8323\n",
            "Epoch 546/1000\n",
            "678/678 [==============================] - 0s 545us/step - loss: 0.2113 - acc: 0.9204 - val_loss: 0.4065 - val_acc: 0.8533\n",
            "Epoch 547/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.2149 - acc: 0.9189 - val_loss: 0.4180 - val_acc: 0.8383\n",
            "Epoch 548/1000\n",
            "678/678 [==============================] - 0s 569us/step - loss: 0.2040 - acc: 0.9307 - val_loss: 0.4053 - val_acc: 0.8503\n",
            "Epoch 549/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.1891 - acc: 0.9425 - val_loss: 0.3876 - val_acc: 0.8473\n",
            "Epoch 550/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.2001 - acc: 0.9218 - val_loss: 0.3949 - val_acc: 0.8623\n",
            "Epoch 551/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.1905 - acc: 0.9395 - val_loss: 0.3881 - val_acc: 0.8503\n",
            "Epoch 552/1000\n",
            "678/678 [==============================] - 0s 542us/step - loss: 0.2125 - acc: 0.9233 - val_loss: 0.4822 - val_acc: 0.8084\n",
            "Epoch 553/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.2069 - acc: 0.9322 - val_loss: 0.3819 - val_acc: 0.8473\n",
            "Epoch 554/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.2002 - acc: 0.9263 - val_loss: 0.4023 - val_acc: 0.8413\n",
            "Epoch 555/1000\n",
            "678/678 [==============================] - 0s 547us/step - loss: 0.1894 - acc: 0.9263 - val_loss: 0.4129 - val_acc: 0.8413\n",
            "Epoch 556/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.2097 - acc: 0.9204 - val_loss: 0.3920 - val_acc: 0.8563\n",
            "Epoch 557/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.2080 - acc: 0.9204 - val_loss: 0.3851 - val_acc: 0.8473\n",
            "Epoch 558/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.1862 - acc: 0.9351 - val_loss: 0.4082 - val_acc: 0.8293\n",
            "Epoch 559/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1891 - acc: 0.9366 - val_loss: 0.4031 - val_acc: 0.8383\n",
            "Epoch 560/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.2114 - acc: 0.9336 - val_loss: 0.4860 - val_acc: 0.8114\n",
            "Epoch 561/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.2067 - acc: 0.9204 - val_loss: 0.4668 - val_acc: 0.8234\n",
            "Epoch 562/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.1814 - acc: 0.9440 - val_loss: 0.4439 - val_acc: 0.8293\n",
            "Epoch 563/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.2015 - acc: 0.9292 - val_loss: 0.3871 - val_acc: 0.8383\n",
            "Epoch 564/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.1807 - acc: 0.9381 - val_loss: 0.4058 - val_acc: 0.8353\n",
            "Epoch 565/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.2048 - acc: 0.9233 - val_loss: 0.4216 - val_acc: 0.8353\n",
            "Epoch 566/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.1892 - acc: 0.9440 - val_loss: 0.3919 - val_acc: 0.8443\n",
            "Epoch 567/1000\n",
            "678/678 [==============================] - 0s 550us/step - loss: 0.2164 - acc: 0.9071 - val_loss: 0.3798 - val_acc: 0.8473\n",
            "Epoch 568/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.2048 - acc: 0.9336 - val_loss: 0.4078 - val_acc: 0.8323\n",
            "Epoch 569/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.1859 - acc: 0.9395 - val_loss: 0.3841 - val_acc: 0.8413\n",
            "Epoch 570/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1917 - acc: 0.9366 - val_loss: 0.4272 - val_acc: 0.8293\n",
            "Epoch 571/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.1733 - acc: 0.9410 - val_loss: 0.3996 - val_acc: 0.8443\n",
            "Epoch 572/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1848 - acc: 0.9307 - val_loss: 0.3768 - val_acc: 0.8683\n",
            "Epoch 573/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.1969 - acc: 0.9145 - val_loss: 0.4010 - val_acc: 0.8443\n",
            "Epoch 574/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1942 - acc: 0.9307 - val_loss: 0.3932 - val_acc: 0.8413\n",
            "Epoch 575/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.1757 - acc: 0.9395 - val_loss: 0.4073 - val_acc: 0.8413\n",
            "Epoch 576/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.1854 - acc: 0.9381 - val_loss: 0.3966 - val_acc: 0.8563\n",
            "Epoch 577/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.1712 - acc: 0.9440 - val_loss: 0.3785 - val_acc: 0.8563\n",
            "Epoch 578/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.2063 - acc: 0.9204 - val_loss: 0.4581 - val_acc: 0.8174\n",
            "Epoch 579/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.2035 - acc: 0.9248 - val_loss: 0.4469 - val_acc: 0.8323\n",
            "Epoch 580/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.1931 - acc: 0.9322 - val_loss: 0.3860 - val_acc: 0.8443\n",
            "Epoch 581/1000\n",
            "678/678 [==============================] - 0s 549us/step - loss: 0.2031 - acc: 0.9233 - val_loss: 0.3849 - val_acc: 0.8563\n",
            "Epoch 582/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.1834 - acc: 0.9440 - val_loss: 0.4522 - val_acc: 0.8204\n",
            "Epoch 583/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.1963 - acc: 0.9204 - val_loss: 0.4219 - val_acc: 0.8443\n",
            "Epoch 584/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.1874 - acc: 0.9277 - val_loss: 0.4189 - val_acc: 0.8353\n",
            "Epoch 585/1000\n",
            "678/678 [==============================] - 0s 499us/step - loss: 0.1856 - acc: 0.9366 - val_loss: 0.4059 - val_acc: 0.8383\n",
            "Epoch 586/1000\n",
            "678/678 [==============================] - 0s 497us/step - loss: 0.1974 - acc: 0.9307 - val_loss: 0.3908 - val_acc: 0.8503\n",
            "Epoch 587/1000\n",
            "678/678 [==============================] - 0s 495us/step - loss: 0.1904 - acc: 0.9292 - val_loss: 0.3954 - val_acc: 0.8413\n",
            "Epoch 588/1000\n",
            "678/678 [==============================] - 0s 481us/step - loss: 0.2028 - acc: 0.9100 - val_loss: 0.4252 - val_acc: 0.8383\n",
            "Epoch 589/1000\n",
            "678/678 [==============================] - 0s 505us/step - loss: 0.1871 - acc: 0.9366 - val_loss: 0.4050 - val_acc: 0.8443\n",
            "Epoch 590/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.1781 - acc: 0.9292 - val_loss: 0.4446 - val_acc: 0.8234\n",
            "Epoch 591/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.1825 - acc: 0.9336 - val_loss: 0.3747 - val_acc: 0.8533\n",
            "Epoch 592/1000\n",
            "678/678 [==============================] - 0s 483us/step - loss: 0.1707 - acc: 0.9410 - val_loss: 0.4227 - val_acc: 0.8413\n",
            "Epoch 593/1000\n",
            "678/678 [==============================] - 0s 498us/step - loss: 0.1999 - acc: 0.9307 - val_loss: 0.4184 - val_acc: 0.8473\n",
            "Epoch 594/1000\n",
            "678/678 [==============================] - 0s 499us/step - loss: 0.1814 - acc: 0.9336 - val_loss: 0.4624 - val_acc: 0.8293\n",
            "Epoch 595/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1916 - acc: 0.9351 - val_loss: 0.4160 - val_acc: 0.8353\n",
            "Epoch 596/1000\n",
            "678/678 [==============================] - 0s 501us/step - loss: 0.1710 - acc: 0.9395 - val_loss: 0.3981 - val_acc: 0.8473\n",
            "Epoch 597/1000\n",
            "678/678 [==============================] - 0s 552us/step - loss: 0.1818 - acc: 0.9425 - val_loss: 0.4397 - val_acc: 0.8174\n",
            "Epoch 598/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.1745 - acc: 0.9366 - val_loss: 0.3821 - val_acc: 0.8383\n",
            "Epoch 599/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 0.1719 - acc: 0.9336 - val_loss: 0.4213 - val_acc: 0.8234\n",
            "Epoch 600/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.1903 - acc: 0.9336 - val_loss: 0.3784 - val_acc: 0.8593\n",
            "Epoch 601/1000\n",
            "678/678 [==============================] - 0s 550us/step - loss: 0.2017 - acc: 0.9263 - val_loss: 0.4039 - val_acc: 0.8563\n",
            "Epoch 602/1000\n",
            "678/678 [==============================] - 0s 559us/step - loss: 0.1908 - acc: 0.9292 - val_loss: 0.3971 - val_acc: 0.8443\n",
            "Epoch 603/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1801 - acc: 0.9410 - val_loss: 0.3758 - val_acc: 0.8473\n",
            "Epoch 604/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.1910 - acc: 0.9351 - val_loss: 0.3915 - val_acc: 0.8533\n",
            "Epoch 605/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1790 - acc: 0.9366 - val_loss: 0.3767 - val_acc: 0.8593\n",
            "Epoch 606/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1647 - acc: 0.9543 - val_loss: 0.4036 - val_acc: 0.8383\n",
            "Epoch 607/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.1881 - acc: 0.9351 - val_loss: 0.3809 - val_acc: 0.8653\n",
            "Epoch 608/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.1726 - acc: 0.9410 - val_loss: 0.4490 - val_acc: 0.8144\n",
            "Epoch 609/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.1721 - acc: 0.9454 - val_loss: 0.4178 - val_acc: 0.8383\n",
            "Epoch 610/1000\n",
            "678/678 [==============================] - 0s 562us/step - loss: 0.1750 - acc: 0.9440 - val_loss: 0.4606 - val_acc: 0.8263\n",
            "Epoch 611/1000\n",
            "678/678 [==============================] - 0s 539us/step - loss: 0.1865 - acc: 0.9351 - val_loss: 0.3835 - val_acc: 0.8503\n",
            "Epoch 612/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.1801 - acc: 0.9351 - val_loss: 0.3832 - val_acc: 0.8413\n",
            "Epoch 613/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.1627 - acc: 0.9469 - val_loss: 0.4242 - val_acc: 0.8383\n",
            "Epoch 614/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.1764 - acc: 0.9351 - val_loss: 0.3855 - val_acc: 0.8533\n",
            "Epoch 615/1000\n",
            "678/678 [==============================] - 0s 541us/step - loss: 0.1794 - acc: 0.9366 - val_loss: 0.3856 - val_acc: 0.8473\n",
            "Epoch 616/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.1608 - acc: 0.9499 - val_loss: 0.3795 - val_acc: 0.8443\n",
            "Epoch 617/1000\n",
            "678/678 [==============================] - 0s 556us/step - loss: 0.1785 - acc: 0.9277 - val_loss: 0.4055 - val_acc: 0.8353\n",
            "Epoch 618/1000\n",
            "678/678 [==============================] - 0s 541us/step - loss: 0.1711 - acc: 0.9410 - val_loss: 0.4217 - val_acc: 0.8323\n",
            "Epoch 619/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.1985 - acc: 0.9351 - val_loss: 0.4157 - val_acc: 0.8383\n",
            "Epoch 620/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.1596 - acc: 0.9440 - val_loss: 0.4134 - val_acc: 0.8323\n",
            "Epoch 621/1000\n",
            "678/678 [==============================] - 0s 571us/step - loss: 0.1821 - acc: 0.9366 - val_loss: 0.3889 - val_acc: 0.8353\n",
            "Epoch 622/1000\n",
            "678/678 [==============================] - 0s 539us/step - loss: 0.1865 - acc: 0.9292 - val_loss: 0.3647 - val_acc: 0.8593\n",
            "Epoch 623/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.1469 - acc: 0.9617 - val_loss: 0.4033 - val_acc: 0.8473\n",
            "Epoch 624/1000\n",
            "678/678 [==============================] - 0s 549us/step - loss: 0.1674 - acc: 0.9440 - val_loss: 0.4484 - val_acc: 0.8144\n",
            "Epoch 625/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.1705 - acc: 0.9425 - val_loss: 0.4455 - val_acc: 0.8323\n",
            "Epoch 626/1000\n",
            "678/678 [==============================] - 0s 555us/step - loss: 0.1663 - acc: 0.9469 - val_loss: 0.3912 - val_acc: 0.8473\n",
            "Epoch 627/1000\n",
            "678/678 [==============================] - 0s 551us/step - loss: 0.1689 - acc: 0.9425 - val_loss: 0.4149 - val_acc: 0.8353\n",
            "Epoch 628/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.1701 - acc: 0.9425 - val_loss: 0.3977 - val_acc: 0.8443\n",
            "Epoch 629/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.1710 - acc: 0.9425 - val_loss: 0.4185 - val_acc: 0.8383\n",
            "Epoch 630/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1726 - acc: 0.9381 - val_loss: 0.3890 - val_acc: 0.8593\n",
            "Epoch 631/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.1592 - acc: 0.9351 - val_loss: 0.3797 - val_acc: 0.8473\n",
            "Epoch 632/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1893 - acc: 0.9263 - val_loss: 0.4235 - val_acc: 0.8353\n",
            "Epoch 633/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.1563 - acc: 0.9410 - val_loss: 0.3975 - val_acc: 0.8533\n",
            "Epoch 634/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1710 - acc: 0.9410 - val_loss: 0.3933 - val_acc: 0.8443\n",
            "Epoch 635/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.1489 - acc: 0.9499 - val_loss: 0.4084 - val_acc: 0.8443\n",
            "Epoch 636/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.1562 - acc: 0.9454 - val_loss: 0.3922 - val_acc: 0.8443\n",
            "Epoch 637/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.1598 - acc: 0.9322 - val_loss: 0.3772 - val_acc: 0.8593\n",
            "Epoch 638/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.1646 - acc: 0.9454 - val_loss: 0.4169 - val_acc: 0.8473\n",
            "Epoch 639/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.1579 - acc: 0.9425 - val_loss: 0.4451 - val_acc: 0.8323\n",
            "Epoch 640/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.1913 - acc: 0.9336 - val_loss: 0.3750 - val_acc: 0.8503\n",
            "Epoch 641/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.1567 - acc: 0.9366 - val_loss: 0.4356 - val_acc: 0.8383\n",
            "Epoch 642/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.1570 - acc: 0.9528 - val_loss: 0.4118 - val_acc: 0.8383\n",
            "Epoch 643/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1634 - acc: 0.9366 - val_loss: 0.3656 - val_acc: 0.8533\n",
            "Epoch 644/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1629 - acc: 0.9440 - val_loss: 0.4149 - val_acc: 0.8443\n",
            "Epoch 645/1000\n",
            "678/678 [==============================] - 0s 549us/step - loss: 0.1472 - acc: 0.9513 - val_loss: 0.3744 - val_acc: 0.8593\n",
            "Epoch 646/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.1633 - acc: 0.9454 - val_loss: 0.4330 - val_acc: 0.8413\n",
            "Epoch 647/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 0.1460 - acc: 0.9484 - val_loss: 0.3747 - val_acc: 0.8413\n",
            "Epoch 648/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.1678 - acc: 0.9484 - val_loss: 0.3756 - val_acc: 0.8623\n",
            "Epoch 649/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1732 - acc: 0.9425 - val_loss: 0.4610 - val_acc: 0.8234\n",
            "Epoch 650/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.1582 - acc: 0.9528 - val_loss: 0.4158 - val_acc: 0.8443\n",
            "Epoch 651/1000\n",
            "678/678 [==============================] - 0s 541us/step - loss: 0.1609 - acc: 0.9395 - val_loss: 0.3954 - val_acc: 0.8413\n",
            "Epoch 652/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.1600 - acc: 0.9381 - val_loss: 0.4388 - val_acc: 0.8144\n",
            "Epoch 653/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.1638 - acc: 0.9454 - val_loss: 0.4267 - val_acc: 0.8323\n",
            "Epoch 654/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.1713 - acc: 0.9336 - val_loss: 0.4067 - val_acc: 0.8383\n",
            "Epoch 655/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1444 - acc: 0.9631 - val_loss: 0.3760 - val_acc: 0.8593\n",
            "Epoch 656/1000\n",
            "678/678 [==============================] - 0s 568us/step - loss: 0.1667 - acc: 0.9351 - val_loss: 0.4239 - val_acc: 0.8353\n",
            "Epoch 657/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.1468 - acc: 0.9558 - val_loss: 0.3784 - val_acc: 0.8443\n",
            "Epoch 658/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1349 - acc: 0.9617 - val_loss: 0.4091 - val_acc: 0.8413\n",
            "Epoch 659/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.1604 - acc: 0.9440 - val_loss: 0.3795 - val_acc: 0.8593\n",
            "Epoch 660/1000\n",
            "678/678 [==============================] - 0s 567us/step - loss: 0.1487 - acc: 0.9469 - val_loss: 0.3996 - val_acc: 0.8653\n",
            "Epoch 661/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.1682 - acc: 0.9395 - val_loss: 0.4144 - val_acc: 0.8383\n",
            "Epoch 662/1000\n",
            "678/678 [==============================] - 0s 547us/step - loss: 0.1539 - acc: 0.9484 - val_loss: 0.3837 - val_acc: 0.8593\n",
            "Epoch 663/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.1484 - acc: 0.9499 - val_loss: 0.4068 - val_acc: 0.8563\n",
            "Epoch 664/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1820 - acc: 0.9248 - val_loss: 0.3825 - val_acc: 0.8533\n",
            "Epoch 665/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.1474 - acc: 0.9572 - val_loss: 0.4315 - val_acc: 0.8293\n",
            "Epoch 666/1000\n",
            "678/678 [==============================] - 0s 553us/step - loss: 0.1626 - acc: 0.9513 - val_loss: 0.3577 - val_acc: 0.8563\n",
            "Epoch 667/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.1602 - acc: 0.9484 - val_loss: 0.3754 - val_acc: 0.8503\n",
            "Epoch 668/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1695 - acc: 0.9351 - val_loss: 0.3541 - val_acc: 0.8623\n",
            "Epoch 669/1000\n",
            "678/678 [==============================] - 0s 503us/step - loss: 0.1615 - acc: 0.9410 - val_loss: 0.3684 - val_acc: 0.8533\n",
            "Epoch 670/1000\n",
            "678/678 [==============================] - 0s 504us/step - loss: 0.1571 - acc: 0.9440 - val_loss: 0.3691 - val_acc: 0.8503\n",
            "Epoch 671/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 0.1563 - acc: 0.9469 - val_loss: 0.3555 - val_acc: 0.8653\n",
            "Epoch 672/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.1627 - acc: 0.9513 - val_loss: 0.3881 - val_acc: 0.8713\n",
            "Epoch 673/1000\n",
            "678/678 [==============================] - 0s 577us/step - loss: 0.1544 - acc: 0.9454 - val_loss: 0.4231 - val_acc: 0.8413\n",
            "Epoch 674/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.1501 - acc: 0.9484 - val_loss: 0.3866 - val_acc: 0.8653\n",
            "Epoch 675/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.1613 - acc: 0.9410 - val_loss: 0.3739 - val_acc: 0.8653\n",
            "Epoch 676/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.1514 - acc: 0.9543 - val_loss: 0.4051 - val_acc: 0.8533\n",
            "Epoch 677/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1433 - acc: 0.9484 - val_loss: 0.3949 - val_acc: 0.8563\n",
            "Epoch 678/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.1440 - acc: 0.9513 - val_loss: 0.4068 - val_acc: 0.8473\n",
            "Epoch 679/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1366 - acc: 0.9572 - val_loss: 0.3625 - val_acc: 0.8623\n",
            "Epoch 680/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1388 - acc: 0.9513 - val_loss: 0.3798 - val_acc: 0.8503\n",
            "Epoch 681/1000\n",
            "678/678 [==============================] - 0s 492us/step - loss: 0.1645 - acc: 0.9440 - val_loss: 0.3863 - val_acc: 0.8443\n",
            "Epoch 682/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.1563 - acc: 0.9499 - val_loss: 0.4311 - val_acc: 0.8323\n",
            "Epoch 683/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1546 - acc: 0.9469 - val_loss: 0.3784 - val_acc: 0.8473\n",
            "Epoch 684/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.1392 - acc: 0.9572 - val_loss: 0.4099 - val_acc: 0.8443\n",
            "Epoch 685/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.1413 - acc: 0.9484 - val_loss: 0.3903 - val_acc: 0.8443\n",
            "Epoch 686/1000\n",
            "678/678 [==============================] - 0s 576us/step - loss: 0.1565 - acc: 0.9410 - val_loss: 0.3846 - val_acc: 0.8443\n",
            "Epoch 687/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.1362 - acc: 0.9572 - val_loss: 0.3783 - val_acc: 0.8563\n",
            "Epoch 688/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1495 - acc: 0.9543 - val_loss: 0.4249 - val_acc: 0.8443\n",
            "Epoch 689/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1328 - acc: 0.9661 - val_loss: 0.4186 - val_acc: 0.8293\n",
            "Epoch 690/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.1311 - acc: 0.9646 - val_loss: 0.3645 - val_acc: 0.8533\n",
            "Epoch 691/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1627 - acc: 0.9410 - val_loss: 0.5177 - val_acc: 0.8024\n",
            "Epoch 692/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1473 - acc: 0.9440 - val_loss: 0.3647 - val_acc: 0.8563\n",
            "Epoch 693/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.1506 - acc: 0.9484 - val_loss: 0.3982 - val_acc: 0.8473\n",
            "Epoch 694/1000\n",
            "678/678 [==============================] - 0s 545us/step - loss: 0.1426 - acc: 0.9425 - val_loss: 0.4089 - val_acc: 0.8353\n",
            "Epoch 695/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.1426 - acc: 0.9425 - val_loss: 0.4017 - val_acc: 0.8503\n",
            "Epoch 696/1000\n",
            "678/678 [==============================] - 0s 502us/step - loss: 0.1474 - acc: 0.9499 - val_loss: 0.3705 - val_acc: 0.8593\n",
            "Epoch 697/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1434 - acc: 0.9484 - val_loss: 0.3937 - val_acc: 0.8353\n",
            "Epoch 698/1000\n",
            "678/678 [==============================] - 0s 482us/step - loss: 0.1597 - acc: 0.9454 - val_loss: 0.3572 - val_acc: 0.8623\n",
            "Epoch 699/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.1426 - acc: 0.9587 - val_loss: 0.3899 - val_acc: 0.8503\n",
            "Epoch 700/1000\n",
            "678/678 [==============================] - 0s 487us/step - loss: 0.1328 - acc: 0.9572 - val_loss: 0.3503 - val_acc: 0.8713\n",
            "Epoch 701/1000\n",
            "678/678 [==============================] - 0s 491us/step - loss: 0.1493 - acc: 0.9528 - val_loss: 0.4742 - val_acc: 0.8234\n",
            "Epoch 702/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.1300 - acc: 0.9646 - val_loss: 0.3563 - val_acc: 0.8503\n",
            "Epoch 703/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.1364 - acc: 0.9587 - val_loss: 0.3920 - val_acc: 0.8503\n",
            "Epoch 704/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.1283 - acc: 0.9617 - val_loss: 0.3809 - val_acc: 0.8503\n",
            "Epoch 705/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.1449 - acc: 0.9454 - val_loss: 0.3861 - val_acc: 0.8503\n",
            "Epoch 706/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.1316 - acc: 0.9587 - val_loss: 0.4281 - val_acc: 0.8353\n",
            "Epoch 707/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.1428 - acc: 0.9617 - val_loss: 0.3697 - val_acc: 0.8593\n",
            "Epoch 708/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1507 - acc: 0.9454 - val_loss: 0.3620 - val_acc: 0.8653\n",
            "Epoch 709/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.1444 - acc: 0.9454 - val_loss: 0.3867 - val_acc: 0.8473\n",
            "Epoch 710/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.1258 - acc: 0.9617 - val_loss: 0.3614 - val_acc: 0.8503\n",
            "Epoch 711/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.1494 - acc: 0.9469 - val_loss: 0.4499 - val_acc: 0.8323\n",
            "Epoch 712/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.1367 - acc: 0.9513 - val_loss: 0.3595 - val_acc: 0.8623\n",
            "Epoch 713/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1454 - acc: 0.9543 - val_loss: 0.4261 - val_acc: 0.8413\n",
            "Epoch 714/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1471 - acc: 0.9499 - val_loss: 0.3608 - val_acc: 0.8593\n",
            "Epoch 715/1000\n",
            "678/678 [==============================] - 0s 575us/step - loss: 0.1452 - acc: 0.9425 - val_loss: 0.3993 - val_acc: 0.8563\n",
            "Epoch 716/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.1371 - acc: 0.9543 - val_loss: 0.3801 - val_acc: 0.8473\n",
            "Epoch 717/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.1456 - acc: 0.9440 - val_loss: 0.4531 - val_acc: 0.8114\n",
            "Epoch 718/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.1514 - acc: 0.9440 - val_loss: 0.3917 - val_acc: 0.8473\n",
            "Epoch 719/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.1325 - acc: 0.9543 - val_loss: 0.4818 - val_acc: 0.8204\n",
            "Epoch 720/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1256 - acc: 0.9676 - val_loss: 0.4521 - val_acc: 0.8383\n",
            "Epoch 721/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1485 - acc: 0.9425 - val_loss: 0.4577 - val_acc: 0.8323\n",
            "Epoch 722/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.1472 - acc: 0.9425 - val_loss: 0.3962 - val_acc: 0.8443\n",
            "Epoch 723/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1402 - acc: 0.9513 - val_loss: 0.3620 - val_acc: 0.8683\n",
            "Epoch 724/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.1252 - acc: 0.9617 - val_loss: 0.3943 - val_acc: 0.8503\n",
            "Epoch 725/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.1486 - acc: 0.9440 - val_loss: 0.3870 - val_acc: 0.8593\n",
            "Epoch 726/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.1298 - acc: 0.9558 - val_loss: 0.4040 - val_acc: 0.8503\n",
            "Epoch 727/1000\n",
            "678/678 [==============================] - 0s 557us/step - loss: 0.1352 - acc: 0.9499 - val_loss: 0.3695 - val_acc: 0.8772\n",
            "Epoch 728/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.1386 - acc: 0.9469 - val_loss: 0.3862 - val_acc: 0.8503\n",
            "Epoch 729/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.1340 - acc: 0.9513 - val_loss: 0.3623 - val_acc: 0.8563\n",
            "Epoch 730/1000\n",
            "678/678 [==============================] - 0s 572us/step - loss: 0.1258 - acc: 0.9602 - val_loss: 0.4042 - val_acc: 0.8503\n",
            "Epoch 731/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1345 - acc: 0.9602 - val_loss: 0.4338 - val_acc: 0.8413\n",
            "Epoch 732/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1333 - acc: 0.9602 - val_loss: 0.3813 - val_acc: 0.8563\n",
            "Epoch 733/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1462 - acc: 0.9484 - val_loss: 0.3665 - val_acc: 0.8593\n",
            "Epoch 734/1000\n",
            "678/678 [==============================] - 0s 498us/step - loss: 0.1270 - acc: 0.9543 - val_loss: 0.3767 - val_acc: 0.8503\n",
            "Epoch 735/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.1196 - acc: 0.9690 - val_loss: 0.4276 - val_acc: 0.8503\n",
            "Epoch 736/1000\n",
            "678/678 [==============================] - 0s 504us/step - loss: 0.1452 - acc: 0.9543 - val_loss: 0.3857 - val_acc: 0.8563\n",
            "Epoch 737/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.1562 - acc: 0.9440 - val_loss: 0.4226 - val_acc: 0.8443\n",
            "Epoch 738/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1238 - acc: 0.9661 - val_loss: 0.3831 - val_acc: 0.8413\n",
            "Epoch 739/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.1249 - acc: 0.9617 - val_loss: 0.3640 - val_acc: 0.8653\n",
            "Epoch 740/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1382 - acc: 0.9572 - val_loss: 0.4082 - val_acc: 0.8503\n",
            "Epoch 741/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.1173 - acc: 0.9661 - val_loss: 0.3998 - val_acc: 0.8503\n",
            "Epoch 742/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.1433 - acc: 0.9410 - val_loss: 0.5099 - val_acc: 0.8174\n",
            "Epoch 743/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.1353 - acc: 0.9572 - val_loss: 0.3637 - val_acc: 0.8772\n",
            "Epoch 744/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.1398 - acc: 0.9528 - val_loss: 0.3883 - val_acc: 0.8533\n",
            "Epoch 745/1000\n",
            "678/678 [==============================] - 0s 575us/step - loss: 0.1207 - acc: 0.9646 - val_loss: 0.4855 - val_acc: 0.8234\n",
            "Epoch 746/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.1372 - acc: 0.9484 - val_loss: 0.4930 - val_acc: 0.8174\n",
            "Epoch 747/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 0.1236 - acc: 0.9631 - val_loss: 0.3741 - val_acc: 0.8593\n",
            "Epoch 748/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1214 - acc: 0.9661 - val_loss: 0.3703 - val_acc: 0.8563\n",
            "Epoch 749/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.1362 - acc: 0.9528 - val_loss: 0.3544 - val_acc: 0.8473\n",
            "Epoch 750/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1241 - acc: 0.9602 - val_loss: 0.4133 - val_acc: 0.8443\n",
            "Epoch 751/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.1332 - acc: 0.9572 - val_loss: 0.3817 - val_acc: 0.8473\n",
            "Epoch 752/1000\n",
            "678/678 [==============================] - 0s 503us/step - loss: 0.1293 - acc: 0.9543 - val_loss: 0.3484 - val_acc: 0.8653\n",
            "Epoch 753/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.1283 - acc: 0.9572 - val_loss: 0.3715 - val_acc: 0.8503\n",
            "Epoch 754/1000\n",
            "678/678 [==============================] - 0s 483us/step - loss: 0.1269 - acc: 0.9543 - val_loss: 0.3934 - val_acc: 0.8503\n",
            "Epoch 755/1000\n",
            "678/678 [==============================] - 0s 492us/step - loss: 0.1214 - acc: 0.9646 - val_loss: 0.4044 - val_acc: 0.8473\n",
            "Epoch 756/1000\n",
            "678/678 [==============================] - 0s 484us/step - loss: 0.1393 - acc: 0.9395 - val_loss: 0.4268 - val_acc: 0.8263\n",
            "Epoch 757/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.1178 - acc: 0.9543 - val_loss: 0.3503 - val_acc: 0.8653\n",
            "Epoch 758/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.1151 - acc: 0.9617 - val_loss: 0.3619 - val_acc: 0.8563\n",
            "Epoch 759/1000\n",
            "678/678 [==============================] - 0s 478us/step - loss: 0.1297 - acc: 0.9631 - val_loss: 0.4171 - val_acc: 0.8383\n",
            "Epoch 760/1000\n",
            "678/678 [==============================] - 0s 509us/step - loss: 0.1324 - acc: 0.9572 - val_loss: 0.3763 - val_acc: 0.8623\n",
            "Epoch 761/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.1275 - acc: 0.9587 - val_loss: 0.4113 - val_acc: 0.8443\n",
            "Epoch 762/1000\n",
            "678/678 [==============================] - 0s 488us/step - loss: 0.1208 - acc: 0.9646 - val_loss: 0.4261 - val_acc: 0.8533\n",
            "Epoch 763/1000\n",
            "678/678 [==============================] - 0s 495us/step - loss: 0.1400 - acc: 0.9513 - val_loss: 0.4481 - val_acc: 0.8323\n",
            "Epoch 764/1000\n",
            "678/678 [==============================] - 0s 486us/step - loss: 0.1297 - acc: 0.9454 - val_loss: 0.3677 - val_acc: 0.8683\n",
            "Epoch 765/1000\n",
            "678/678 [==============================] - 0s 486us/step - loss: 0.1193 - acc: 0.9617 - val_loss: 0.3644 - val_acc: 0.8533\n",
            "Epoch 766/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.1180 - acc: 0.9602 - val_loss: 0.4170 - val_acc: 0.8503\n",
            "Epoch 767/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.1225 - acc: 0.9572 - val_loss: 0.3486 - val_acc: 0.8653\n",
            "Epoch 768/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.1383 - acc: 0.9572 - val_loss: 0.3635 - val_acc: 0.8563\n",
            "Epoch 769/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.1261 - acc: 0.9558 - val_loss: 0.3631 - val_acc: 0.8653\n",
            "Epoch 770/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.1290 - acc: 0.9646 - val_loss: 0.3506 - val_acc: 0.8772\n",
            "Epoch 771/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.1213 - acc: 0.9661 - val_loss: 0.4119 - val_acc: 0.8473\n",
            "Epoch 772/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.1332 - acc: 0.9469 - val_loss: 0.3507 - val_acc: 0.8683\n",
            "Epoch 773/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.1266 - acc: 0.9484 - val_loss: 0.4109 - val_acc: 0.8533\n",
            "Epoch 774/1000\n",
            "678/678 [==============================] - 0s 502us/step - loss: 0.1131 - acc: 0.9705 - val_loss: 0.3735 - val_acc: 0.8533\n",
            "Epoch 775/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1358 - acc: 0.9558 - val_loss: 0.4051 - val_acc: 0.8473\n",
            "Epoch 776/1000\n",
            "678/678 [==============================] - 0s 564us/step - loss: 0.1152 - acc: 0.9587 - val_loss: 0.3734 - val_acc: 0.8623\n",
            "Epoch 777/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.1278 - acc: 0.9602 - val_loss: 0.3640 - val_acc: 0.8743\n",
            "Epoch 778/1000\n",
            "678/678 [==============================] - 0s 548us/step - loss: 0.1167 - acc: 0.9631 - val_loss: 0.3460 - val_acc: 0.8623\n",
            "Epoch 779/1000\n",
            "678/678 [==============================] - 0s 526us/step - loss: 0.1313 - acc: 0.9543 - val_loss: 0.4138 - val_acc: 0.8503\n",
            "Epoch 780/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1240 - acc: 0.9602 - val_loss: 0.4173 - val_acc: 0.8443\n",
            "Epoch 781/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1124 - acc: 0.9690 - val_loss: 0.3523 - val_acc: 0.8743\n",
            "Epoch 782/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.1193 - acc: 0.9558 - val_loss: 0.4320 - val_acc: 0.8383\n",
            "Epoch 783/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1179 - acc: 0.9572 - val_loss: 0.4282 - val_acc: 0.8473\n",
            "Epoch 784/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.1252 - acc: 0.9572 - val_loss: 0.3488 - val_acc: 0.8593\n",
            "Epoch 785/1000\n",
            "678/678 [==============================] - 0s 499us/step - loss: 0.1150 - acc: 0.9661 - val_loss: 0.3857 - val_acc: 0.8593\n",
            "Epoch 786/1000\n",
            "678/678 [==============================] - 0s 537us/step - loss: 0.1356 - acc: 0.9528 - val_loss: 0.3818 - val_acc: 0.8683\n",
            "Epoch 787/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.1141 - acc: 0.9617 - val_loss: 0.3909 - val_acc: 0.8503\n",
            "Epoch 788/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.1076 - acc: 0.9690 - val_loss: 0.3657 - val_acc: 0.8473\n",
            "Epoch 789/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.1131 - acc: 0.9661 - val_loss: 0.3817 - val_acc: 0.8473\n",
            "Epoch 790/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.1251 - acc: 0.9646 - val_loss: 0.3649 - val_acc: 0.8713\n",
            "Epoch 791/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.1180 - acc: 0.9631 - val_loss: 0.3604 - val_acc: 0.8623\n",
            "Epoch 792/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.1203 - acc: 0.9617 - val_loss: 0.3831 - val_acc: 0.8593\n",
            "Epoch 793/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.1144 - acc: 0.9661 - val_loss: 0.5282 - val_acc: 0.8084\n",
            "Epoch 794/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.1229 - acc: 0.9617 - val_loss: 0.3688 - val_acc: 0.8623\n",
            "Epoch 795/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.1182 - acc: 0.9587 - val_loss: 0.3972 - val_acc: 0.8443\n",
            "Epoch 796/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.1283 - acc: 0.9558 - val_loss: 0.3865 - val_acc: 0.8653\n",
            "Epoch 797/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1081 - acc: 0.9735 - val_loss: 0.3812 - val_acc: 0.8563\n",
            "Epoch 798/1000\n",
            "678/678 [==============================] - 0s 500us/step - loss: 0.1095 - acc: 0.9690 - val_loss: 0.4384 - val_acc: 0.8413\n",
            "Epoch 799/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.1084 - acc: 0.9661 - val_loss: 0.3615 - val_acc: 0.8683\n",
            "Epoch 800/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1189 - acc: 0.9587 - val_loss: 0.4000 - val_acc: 0.8413\n",
            "Epoch 801/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.1172 - acc: 0.9587 - val_loss: 0.4581 - val_acc: 0.8353\n",
            "Epoch 802/1000\n",
            "678/678 [==============================] - 0s 544us/step - loss: 0.1046 - acc: 0.9661 - val_loss: 0.3866 - val_acc: 0.8473\n",
            "Epoch 803/1000\n",
            "678/678 [==============================] - 0s 541us/step - loss: 0.1085 - acc: 0.9617 - val_loss: 0.3659 - val_acc: 0.8623\n",
            "Epoch 804/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.1209 - acc: 0.9543 - val_loss: 0.4628 - val_acc: 0.8383\n",
            "Epoch 805/1000\n",
            "678/678 [==============================] - 0s 575us/step - loss: 0.1093 - acc: 0.9617 - val_loss: 0.3807 - val_acc: 0.8623\n",
            "Epoch 806/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.1068 - acc: 0.9676 - val_loss: 0.3943 - val_acc: 0.8653\n",
            "Epoch 807/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.1190 - acc: 0.9513 - val_loss: 0.3675 - val_acc: 0.8503\n",
            "Epoch 808/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.1233 - acc: 0.9572 - val_loss: 0.3744 - val_acc: 0.8533\n",
            "Epoch 809/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.1231 - acc: 0.9617 - val_loss: 0.3521 - val_acc: 0.8653\n",
            "Epoch 810/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.1105 - acc: 0.9690 - val_loss: 0.4448 - val_acc: 0.8353\n",
            "Epoch 811/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.1136 - acc: 0.9617 - val_loss: 0.4209 - val_acc: 0.8383\n",
            "Epoch 812/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.1032 - acc: 0.9617 - val_loss: 0.3891 - val_acc: 0.8563\n",
            "Epoch 813/1000\n",
            "678/678 [==============================] - 0s 556us/step - loss: 0.1093 - acc: 0.9661 - val_loss: 0.3519 - val_acc: 0.8802\n",
            "Epoch 814/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.1132 - acc: 0.9705 - val_loss: 0.3882 - val_acc: 0.8593\n",
            "Epoch 815/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.1123 - acc: 0.9572 - val_loss: 0.3716 - val_acc: 0.8623\n",
            "Epoch 816/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.1166 - acc: 0.9572 - val_loss: 0.3691 - val_acc: 0.8623\n",
            "Epoch 817/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.0999 - acc: 0.9676 - val_loss: 0.4325 - val_acc: 0.8533\n",
            "Epoch 818/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1061 - acc: 0.9690 - val_loss: 0.3826 - val_acc: 0.8593\n",
            "Epoch 819/1000\n",
            "678/678 [==============================] - 0s 539us/step - loss: 0.1199 - acc: 0.9558 - val_loss: 0.4620 - val_acc: 0.8323\n",
            "Epoch 820/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.1104 - acc: 0.9617 - val_loss: 0.3829 - val_acc: 0.8593\n",
            "Epoch 821/1000\n",
            "678/678 [==============================] - 0s 533us/step - loss: 0.1036 - acc: 0.9735 - val_loss: 0.4113 - val_acc: 0.8563\n",
            "Epoch 822/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1208 - acc: 0.9572 - val_loss: 0.3532 - val_acc: 0.8623\n",
            "Epoch 823/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1124 - acc: 0.9646 - val_loss: 0.5021 - val_acc: 0.8293\n",
            "Epoch 824/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.1147 - acc: 0.9631 - val_loss: 0.4023 - val_acc: 0.8563\n",
            "Epoch 825/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.1035 - acc: 0.9602 - val_loss: 0.3909 - val_acc: 0.8503\n",
            "Epoch 826/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.1076 - acc: 0.9676 - val_loss: 0.3454 - val_acc: 0.8772\n",
            "Epoch 827/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.1249 - acc: 0.9587 - val_loss: 0.4796 - val_acc: 0.8174\n",
            "Epoch 828/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.0977 - acc: 0.9720 - val_loss: 0.3728 - val_acc: 0.8623\n",
            "Epoch 829/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.1170 - acc: 0.9558 - val_loss: 0.3562 - val_acc: 0.8593\n",
            "Epoch 830/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.0992 - acc: 0.9631 - val_loss: 0.3684 - val_acc: 0.8593\n",
            "Epoch 831/1000\n",
            "678/678 [==============================] - 0s 543us/step - loss: 0.1100 - acc: 0.9602 - val_loss: 0.4383 - val_acc: 0.8383\n",
            "Epoch 832/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.1114 - acc: 0.9617 - val_loss: 0.3721 - val_acc: 0.8593\n",
            "Epoch 833/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.1110 - acc: 0.9587 - val_loss: 0.3419 - val_acc: 0.8743\n",
            "Epoch 834/1000\n",
            "678/678 [==============================] - 0s 569us/step - loss: 0.0979 - acc: 0.9646 - val_loss: 0.4168 - val_acc: 0.8473\n",
            "Epoch 835/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.1035 - acc: 0.9631 - val_loss: 0.4270 - val_acc: 0.8443\n",
            "Epoch 836/1000\n",
            "678/678 [==============================] - 0s 542us/step - loss: 0.1131 - acc: 0.9631 - val_loss: 0.4185 - val_acc: 0.8443\n",
            "Epoch 837/1000\n",
            "678/678 [==============================] - 0s 494us/step - loss: 0.1169 - acc: 0.9587 - val_loss: 0.4260 - val_acc: 0.8473\n",
            "Epoch 838/1000\n",
            "678/678 [==============================] - 0s 490us/step - loss: 0.1036 - acc: 0.9558 - val_loss: 0.3873 - val_acc: 0.8443\n",
            "Epoch 839/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.1024 - acc: 0.9631 - val_loss: 0.3577 - val_acc: 0.8533\n",
            "Epoch 840/1000\n",
            "678/678 [==============================] - 0s 509us/step - loss: 0.0946 - acc: 0.9735 - val_loss: 0.3951 - val_acc: 0.8473\n",
            "Epoch 841/1000\n",
            "678/678 [==============================] - 0s 495us/step - loss: 0.1102 - acc: 0.9572 - val_loss: 0.4072 - val_acc: 0.8473\n",
            "Epoch 842/1000\n",
            "678/678 [==============================] - 0s 513us/step - loss: 0.1213 - acc: 0.9499 - val_loss: 0.3509 - val_acc: 0.8713\n",
            "Epoch 843/1000\n",
            "678/678 [==============================] - 0s 504us/step - loss: 0.0970 - acc: 0.9720 - val_loss: 0.3906 - val_acc: 0.8623\n",
            "Epoch 844/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.0970 - acc: 0.9617 - val_loss: 0.3958 - val_acc: 0.8653\n",
            "Epoch 845/1000\n",
            "678/678 [==============================] - 0s 501us/step - loss: 0.1110 - acc: 0.9676 - val_loss: 0.3829 - val_acc: 0.8623\n",
            "Epoch 846/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.1068 - acc: 0.9646 - val_loss: 0.4000 - val_acc: 0.8533\n",
            "Epoch 847/1000\n",
            "678/678 [==============================] - 0s 498us/step - loss: 0.1014 - acc: 0.9705 - val_loss: 0.3703 - val_acc: 0.8772\n",
            "Epoch 848/1000\n",
            "678/678 [==============================] - 0s 493us/step - loss: 0.1124 - acc: 0.9661 - val_loss: 0.4067 - val_acc: 0.8383\n",
            "Epoch 849/1000\n",
            "678/678 [==============================] - 0s 460us/step - loss: 0.1116 - acc: 0.9558 - val_loss: 0.3750 - val_acc: 0.8623\n",
            "Epoch 850/1000\n",
            "678/678 [==============================] - 0s 484us/step - loss: 0.1017 - acc: 0.9617 - val_loss: 0.3734 - val_acc: 0.8683\n",
            "Epoch 851/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.1177 - acc: 0.9617 - val_loss: 0.4058 - val_acc: 0.8323\n",
            "Epoch 852/1000\n",
            "678/678 [==============================] - 0s 528us/step - loss: 0.0952 - acc: 0.9690 - val_loss: 0.3956 - val_acc: 0.8503\n",
            "Epoch 853/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.1022 - acc: 0.9676 - val_loss: 0.3734 - val_acc: 0.8563\n",
            "Epoch 854/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.1007 - acc: 0.9705 - val_loss: 0.3843 - val_acc: 0.8563\n",
            "Epoch 855/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.1043 - acc: 0.9705 - val_loss: 0.4516 - val_acc: 0.8293\n",
            "Epoch 856/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1094 - acc: 0.9720 - val_loss: 0.4138 - val_acc: 0.8473\n",
            "Epoch 857/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.0964 - acc: 0.9631 - val_loss: 0.3681 - val_acc: 0.8563\n",
            "Epoch 858/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.1021 - acc: 0.9705 - val_loss: 0.4440 - val_acc: 0.8473\n",
            "Epoch 859/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.1180 - acc: 0.9646 - val_loss: 0.4383 - val_acc: 0.8473\n",
            "Epoch 860/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.1154 - acc: 0.9543 - val_loss: 0.3657 - val_acc: 0.8563\n",
            "Epoch 861/1000\n",
            "678/678 [==============================] - 0s 549us/step - loss: 0.1000 - acc: 0.9705 - val_loss: 0.3654 - val_acc: 0.8713\n",
            "Epoch 862/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.0966 - acc: 0.9602 - val_loss: 0.5055 - val_acc: 0.8293\n",
            "Epoch 863/1000\n",
            "678/678 [==============================] - 0s 536us/step - loss: 0.1004 - acc: 0.9602 - val_loss: 0.3764 - val_acc: 0.8533\n",
            "Epoch 864/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1211 - acc: 0.9543 - val_loss: 0.3661 - val_acc: 0.8563\n",
            "Epoch 865/1000\n",
            "678/678 [==============================] - 0s 551us/step - loss: 0.1097 - acc: 0.9705 - val_loss: 0.4126 - val_acc: 0.8383\n",
            "Epoch 866/1000\n",
            "678/678 [==============================] - 0s 504us/step - loss: 0.0867 - acc: 0.9690 - val_loss: 0.4005 - val_acc: 0.8383\n",
            "Epoch 867/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.0958 - acc: 0.9631 - val_loss: 0.4551 - val_acc: 0.8413\n",
            "Epoch 868/1000\n",
            "678/678 [==============================] - 0s 500us/step - loss: 0.0989 - acc: 0.9764 - val_loss: 0.3827 - val_acc: 0.8563\n",
            "Epoch 869/1000\n",
            "678/678 [==============================] - 0s 484us/step - loss: 0.0902 - acc: 0.9749 - val_loss: 0.3946 - val_acc: 0.8383\n",
            "Epoch 870/1000\n",
            "678/678 [==============================] - 0s 490us/step - loss: 0.0975 - acc: 0.9676 - val_loss: 0.4170 - val_acc: 0.8503\n",
            "Epoch 871/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.0954 - acc: 0.9617 - val_loss: 0.3578 - val_acc: 0.8563\n",
            "Epoch 872/1000\n",
            "678/678 [==============================] - 0s 508us/step - loss: 0.1086 - acc: 0.9617 - val_loss: 0.4473 - val_acc: 0.8353\n",
            "Epoch 873/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.0892 - acc: 0.9794 - val_loss: 0.3436 - val_acc: 0.8683\n",
            "Epoch 874/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.1238 - acc: 0.9572 - val_loss: 0.3760 - val_acc: 0.8623\n",
            "Epoch 875/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.1037 - acc: 0.9631 - val_loss: 0.4022 - val_acc: 0.8503\n",
            "Epoch 876/1000\n",
            "678/678 [==============================] - 0s 519us/step - loss: 0.0876 - acc: 0.9764 - val_loss: 0.3889 - val_acc: 0.8623\n",
            "Epoch 877/1000\n",
            "678/678 [==============================] - 0s 530us/step - loss: 0.1057 - acc: 0.9631 - val_loss: 0.3924 - val_acc: 0.8533\n",
            "Epoch 878/1000\n",
            "678/678 [==============================] - 0s 545us/step - loss: 0.1048 - acc: 0.9631 - val_loss: 0.3976 - val_acc: 0.8563\n",
            "Epoch 879/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.0992 - acc: 0.9631 - val_loss: 0.3962 - val_acc: 0.8623\n",
            "Epoch 880/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.0936 - acc: 0.9764 - val_loss: 0.3731 - val_acc: 0.8683\n",
            "Epoch 881/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.0791 - acc: 0.9838 - val_loss: 0.4252 - val_acc: 0.8533\n",
            "Epoch 882/1000\n",
            "678/678 [==============================] - 0s 521us/step - loss: 0.0854 - acc: 0.9720 - val_loss: 0.4371 - val_acc: 0.8383\n",
            "Epoch 883/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.1053 - acc: 0.9631 - val_loss: 0.4400 - val_acc: 0.8503\n",
            "Epoch 884/1000\n",
            "678/678 [==============================] - 0s 531us/step - loss: 0.1042 - acc: 0.9631 - val_loss: 0.4218 - val_acc: 0.8563\n",
            "Epoch 885/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.1120 - acc: 0.9528 - val_loss: 0.3918 - val_acc: 0.8563\n",
            "Epoch 886/1000\n",
            "678/678 [==============================] - 0s 504us/step - loss: 0.1156 - acc: 0.9602 - val_loss: 0.3589 - val_acc: 0.8623\n",
            "Epoch 887/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.1082 - acc: 0.9602 - val_loss: 0.3714 - val_acc: 0.8563\n",
            "Epoch 888/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.0860 - acc: 0.9735 - val_loss: 0.3993 - val_acc: 0.8563\n",
            "Epoch 889/1000\n",
            "678/678 [==============================] - 0s 506us/step - loss: 0.0858 - acc: 0.9720 - val_loss: 0.3697 - val_acc: 0.8653\n",
            "Epoch 890/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.1064 - acc: 0.9617 - val_loss: 0.3980 - val_acc: 0.8623\n",
            "Epoch 891/1000\n",
            "678/678 [==============================] - 0s 514us/step - loss: 0.0974 - acc: 0.9676 - val_loss: 0.4542 - val_acc: 0.8383\n",
            "Epoch 892/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.1024 - acc: 0.9646 - val_loss: 0.3658 - val_acc: 0.8623\n",
            "Epoch 893/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.1186 - acc: 0.9572 - val_loss: 0.4182 - val_acc: 0.8593\n",
            "Epoch 894/1000\n",
            "678/678 [==============================] - 0s 553us/step - loss: 0.0888 - acc: 0.9720 - val_loss: 0.5430 - val_acc: 0.8174\n",
            "Epoch 895/1000\n",
            "678/678 [==============================] - 0s 556us/step - loss: 0.0858 - acc: 0.9720 - val_loss: 0.5941 - val_acc: 0.8054\n",
            "Epoch 896/1000\n",
            "678/678 [==============================] - 0s 538us/step - loss: 0.0928 - acc: 0.9661 - val_loss: 0.3529 - val_acc: 0.8743\n",
            "Epoch 897/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.0938 - acc: 0.9720 - val_loss: 0.4716 - val_acc: 0.8174\n",
            "Epoch 898/1000\n",
            "678/678 [==============================] - 0s 507us/step - loss: 0.0958 - acc: 0.9705 - val_loss: 0.4567 - val_acc: 0.8413\n",
            "Epoch 899/1000\n",
            "678/678 [==============================] - 0s 529us/step - loss: 0.1085 - acc: 0.9558 - val_loss: 0.3952 - val_acc: 0.8593\n",
            "Epoch 900/1000\n",
            "678/678 [==============================] - 0s 512us/step - loss: 0.0852 - acc: 0.9720 - val_loss: 0.4168 - val_acc: 0.8593\n",
            "Epoch 901/1000\n",
            "678/678 [==============================] - 0s 495us/step - loss: 0.1098 - acc: 0.9646 - val_loss: 0.3904 - val_acc: 0.8533\n",
            "Epoch 902/1000\n",
            "678/678 [==============================] - 0s 503us/step - loss: 0.0954 - acc: 0.9735 - val_loss: 0.3814 - val_acc: 0.8563\n",
            "Epoch 903/1000\n",
            "678/678 [==============================] - 0s 496us/step - loss: 0.0918 - acc: 0.9764 - val_loss: 0.3640 - val_acc: 0.8623\n",
            "Epoch 904/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.0817 - acc: 0.9735 - val_loss: 0.4190 - val_acc: 0.8623\n",
            "Epoch 905/1000\n",
            "678/678 [==============================] - 0s 496us/step - loss: 0.0915 - acc: 0.9705 - val_loss: 0.4020 - val_acc: 0.8683\n",
            "Epoch 906/1000\n",
            "678/678 [==============================] - 0s 489us/step - loss: 0.0874 - acc: 0.9661 - val_loss: 0.3577 - val_acc: 0.8713\n",
            "Epoch 907/1000\n",
            "678/678 [==============================] - 0s 517us/step - loss: 0.0922 - acc: 0.9690 - val_loss: 0.4044 - val_acc: 0.8563\n",
            "Epoch 908/1000\n",
            "678/678 [==============================] - 0s 518us/step - loss: 0.1049 - acc: 0.9631 - val_loss: 0.3643 - val_acc: 0.8623\n",
            "Epoch 909/1000\n",
            "678/678 [==============================] - 0s 503us/step - loss: 0.0947 - acc: 0.9676 - val_loss: 0.4089 - val_acc: 0.8503\n",
            "Epoch 910/1000\n",
            "678/678 [==============================] - 0s 494us/step - loss: 0.0817 - acc: 0.9749 - val_loss: 0.4221 - val_acc: 0.8533\n",
            "Epoch 911/1000\n",
            "678/678 [==============================] - 0s 522us/step - loss: 0.0961 - acc: 0.9779 - val_loss: 0.5094 - val_acc: 0.8293\n",
            "Epoch 912/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.1085 - acc: 0.9602 - val_loss: 0.3743 - val_acc: 0.8683\n",
            "Epoch 913/1000\n",
            "678/678 [==============================] - 0s 543us/step - loss: 0.0997 - acc: 0.9631 - val_loss: 0.4210 - val_acc: 0.8563\n",
            "Epoch 914/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.0883 - acc: 0.9602 - val_loss: 0.3858 - val_acc: 0.8533\n",
            "Epoch 915/1000\n",
            "678/678 [==============================] - 0s 515us/step - loss: 0.0970 - acc: 0.9676 - val_loss: 0.4934 - val_acc: 0.8323\n",
            "Epoch 916/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.0902 - acc: 0.9720 - val_loss: 0.3899 - val_acc: 0.8563\n",
            "Epoch 917/1000\n",
            "678/678 [==============================] - 0s 505us/step - loss: 0.0909 - acc: 0.9646 - val_loss: 0.3708 - val_acc: 0.8533\n",
            "Epoch 918/1000\n",
            "678/678 [==============================] - 0s 520us/step - loss: 0.0931 - acc: 0.9779 - val_loss: 0.5017 - val_acc: 0.8323\n",
            "Epoch 919/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.0915 - acc: 0.9705 - val_loss: 0.4274 - val_acc: 0.8563\n",
            "Epoch 920/1000\n",
            "678/678 [==============================] - 0s 504us/step - loss: 0.0960 - acc: 0.9661 - val_loss: 0.3542 - val_acc: 0.8713\n",
            "Epoch 921/1000\n",
            "678/678 [==============================] - 0s 489us/step - loss: 0.0954 - acc: 0.9661 - val_loss: 0.3574 - val_acc: 0.8563\n",
            "Epoch 922/1000\n",
            "678/678 [==============================] - 0s 495us/step - loss: 0.0950 - acc: 0.9661 - val_loss: 0.3736 - val_acc: 0.8653\n",
            "Epoch 923/1000\n",
            "678/678 [==============================] - 0s 503us/step - loss: 0.0835 - acc: 0.9779 - val_loss: 0.4296 - val_acc: 0.8563\n",
            "Epoch 924/1000\n",
            "678/678 [==============================] - 0s 468us/step - loss: 0.0910 - acc: 0.9690 - val_loss: 0.3493 - val_acc: 0.8653\n",
            "Epoch 925/1000\n",
            "678/678 [==============================] - 0s 516us/step - loss: 0.0986 - acc: 0.9661 - val_loss: 0.4444 - val_acc: 0.8413\n",
            "Epoch 926/1000\n",
            "678/678 [==============================] - 0s 475us/step - loss: 0.0892 - acc: 0.9705 - val_loss: 0.3994 - val_acc: 0.8623\n",
            "Epoch 927/1000\n",
            "678/678 [==============================] - 0s 500us/step - loss: 0.0914 - acc: 0.9735 - val_loss: 0.3680 - val_acc: 0.8653\n",
            "Epoch 928/1000\n",
            "678/678 [==============================] - 0s 510us/step - loss: 0.0960 - acc: 0.9676 - val_loss: 0.4303 - val_acc: 0.8443\n",
            "Epoch 929/1000\n",
            "678/678 [==============================] - 0s 471us/step - loss: 0.0906 - acc: 0.9705 - val_loss: 0.3948 - val_acc: 0.8533\n",
            "Epoch 930/1000\n",
            "678/678 [==============================] - 0s 501us/step - loss: 0.0886 - acc: 0.9735 - val_loss: 0.3963 - val_acc: 0.8593\n",
            "Epoch 931/1000\n",
            "678/678 [==============================] - 0s 487us/step - loss: 0.0962 - acc: 0.9676 - val_loss: 0.4315 - val_acc: 0.8533\n",
            "Epoch 932/1000\n",
            "678/678 [==============================] - 0s 467us/step - loss: 0.0783 - acc: 0.9764 - val_loss: 0.3941 - val_acc: 0.8683\n",
            "Epoch 933/1000\n",
            "678/678 [==============================] - 0s 502us/step - loss: 0.0873 - acc: 0.9705 - val_loss: 0.4241 - val_acc: 0.8473\n",
            "Epoch 934/1000\n",
            "678/678 [==============================] - 0s 479us/step - loss: 0.0832 - acc: 0.9705 - val_loss: 0.3941 - val_acc: 0.8593\n",
            "Epoch 935/1000\n",
            "678/678 [==============================] - 0s 486us/step - loss: 0.0955 - acc: 0.9720 - val_loss: 0.3931 - val_acc: 0.8503\n",
            "Epoch 936/1000\n",
            "678/678 [==============================] - 0s 499us/step - loss: 0.0903 - acc: 0.9720 - val_loss: 0.3807 - val_acc: 0.8743\n",
            "Epoch 937/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.0809 - acc: 0.9779 - val_loss: 0.4042 - val_acc: 0.8593\n",
            "Epoch 938/1000\n",
            "678/678 [==============================] - 0s 534us/step - loss: 0.0951 - acc: 0.9661 - val_loss: 0.3776 - val_acc: 0.8683\n",
            "Epoch 939/1000\n",
            "678/678 [==============================] - 0s 555us/step - loss: 0.0785 - acc: 0.9794 - val_loss: 0.3574 - val_acc: 0.8533\n",
            "Epoch 940/1000\n",
            "678/678 [==============================] - 0s 579us/step - loss: 0.0874 - acc: 0.9676 - val_loss: 0.4048 - val_acc: 0.8533\n",
            "Epoch 941/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.0874 - acc: 0.9735 - val_loss: 0.4145 - val_acc: 0.8533\n",
            "Epoch 942/1000\n",
            "678/678 [==============================] - 0s 561us/step - loss: 0.0874 - acc: 0.9690 - val_loss: 0.4169 - val_acc: 0.8563\n",
            "Epoch 943/1000\n",
            "678/678 [==============================] - 0s 558us/step - loss: 0.1120 - acc: 0.9587 - val_loss: 0.3904 - val_acc: 0.8683\n",
            "Epoch 944/1000\n",
            "678/678 [==============================] - 0s 567us/step - loss: 0.0743 - acc: 0.9823 - val_loss: 0.3883 - val_acc: 0.8593\n",
            "Epoch 945/1000\n",
            "678/678 [==============================] - 0s 556us/step - loss: 0.0698 - acc: 0.9867 - val_loss: 0.5100 - val_acc: 0.8084\n",
            "Epoch 946/1000\n",
            "678/678 [==============================] - 0s 584us/step - loss: 0.0981 - acc: 0.9676 - val_loss: 0.4035 - val_acc: 0.8563\n",
            "Epoch 947/1000\n",
            "678/678 [==============================] - 0s 603us/step - loss: 0.0790 - acc: 0.9794 - val_loss: 0.4001 - val_acc: 0.8593\n",
            "Epoch 948/1000\n",
            "678/678 [==============================] - 0s 585us/step - loss: 0.0981 - acc: 0.9690 - val_loss: 0.3989 - val_acc: 0.8533\n",
            "Epoch 949/1000\n",
            "678/678 [==============================] - 0s 598us/step - loss: 0.0824 - acc: 0.9735 - val_loss: 0.4239 - val_acc: 0.8533\n",
            "Epoch 950/1000\n",
            "678/678 [==============================] - 0s 603us/step - loss: 0.0947 - acc: 0.9720 - val_loss: 0.4054 - val_acc: 0.8653\n",
            "Epoch 951/1000\n",
            "678/678 [==============================] - 0s 577us/step - loss: 0.0802 - acc: 0.9779 - val_loss: 0.3739 - val_acc: 0.8563\n",
            "Epoch 952/1000\n",
            "678/678 [==============================] - 0s 569us/step - loss: 0.0876 - acc: 0.9720 - val_loss: 0.4106 - val_acc: 0.8623\n",
            "Epoch 953/1000\n",
            "678/678 [==============================] - 0s 571us/step - loss: 0.1046 - acc: 0.9690 - val_loss: 0.4183 - val_acc: 0.8713\n",
            "Epoch 954/1000\n",
            "678/678 [==============================] - 0s 570us/step - loss: 0.0896 - acc: 0.9705 - val_loss: 0.3969 - val_acc: 0.8653\n",
            "Epoch 955/1000\n",
            "678/678 [==============================] - 0s 604us/step - loss: 0.0770 - acc: 0.9808 - val_loss: 0.4221 - val_acc: 0.8623\n",
            "Epoch 956/1000\n",
            "678/678 [==============================] - 0s 569us/step - loss: 0.0997 - acc: 0.9661 - val_loss: 0.3975 - val_acc: 0.8713\n",
            "Epoch 957/1000\n",
            "678/678 [==============================] - 0s 608us/step - loss: 0.0933 - acc: 0.9779 - val_loss: 0.4363 - val_acc: 0.8503\n",
            "Epoch 958/1000\n",
            "678/678 [==============================] - 0s 584us/step - loss: 0.0712 - acc: 0.9823 - val_loss: 0.4486 - val_acc: 0.8323\n",
            "Epoch 959/1000\n",
            "678/678 [==============================] - 0s 586us/step - loss: 0.0886 - acc: 0.9690 - val_loss: 0.3934 - val_acc: 0.8563\n",
            "Epoch 960/1000\n",
            "678/678 [==============================] - 0s 588us/step - loss: 0.0809 - acc: 0.9705 - val_loss: 0.4348 - val_acc: 0.8563\n",
            "Epoch 961/1000\n",
            "678/678 [==============================] - 0s 598us/step - loss: 0.0885 - acc: 0.9705 - val_loss: 0.3765 - val_acc: 0.8683\n",
            "Epoch 962/1000\n",
            "678/678 [==============================] - 0s 603us/step - loss: 0.0960 - acc: 0.9646 - val_loss: 0.3614 - val_acc: 0.8653\n",
            "Epoch 963/1000\n",
            "678/678 [==============================] - 0s 571us/step - loss: 0.0981 - acc: 0.9646 - val_loss: 0.3865 - val_acc: 0.8623\n",
            "Epoch 964/1000\n",
            "678/678 [==============================] - 0s 563us/step - loss: 0.0934 - acc: 0.9661 - val_loss: 0.3674 - val_acc: 0.8743\n",
            "Epoch 965/1000\n",
            "678/678 [==============================] - 0s 527us/step - loss: 0.0815 - acc: 0.9764 - val_loss: 0.3683 - val_acc: 0.8653\n",
            "Epoch 966/1000\n",
            "678/678 [==============================] - 0s 523us/step - loss: 0.0893 - acc: 0.9720 - val_loss: 0.3761 - val_acc: 0.8563\n",
            "Epoch 967/1000\n",
            "678/678 [==============================] - 0s 525us/step - loss: 0.0838 - acc: 0.9749 - val_loss: 0.3900 - val_acc: 0.8563\n",
            "Epoch 968/1000\n",
            "678/678 [==============================] - 0s 535us/step - loss: 0.0827 - acc: 0.9749 - val_loss: 0.4643 - val_acc: 0.8353\n",
            "Epoch 969/1000\n",
            "678/678 [==============================] - 0s 606us/step - loss: 0.0950 - acc: 0.9646 - val_loss: 0.4437 - val_acc: 0.8503\n",
            "Epoch 970/1000\n",
            "678/678 [==============================] - 0s 594us/step - loss: 0.0859 - acc: 0.9779 - val_loss: 0.4140 - val_acc: 0.8623\n",
            "Epoch 971/1000\n",
            "678/678 [==============================] - 0s 578us/step - loss: 0.0790 - acc: 0.9764 - val_loss: 0.3664 - val_acc: 0.8623\n",
            "Epoch 972/1000\n",
            "678/678 [==============================] - 0s 568us/step - loss: 0.0804 - acc: 0.9690 - val_loss: 0.4076 - val_acc: 0.8533\n",
            "Epoch 973/1000\n",
            "678/678 [==============================] - 0s 577us/step - loss: 0.0768 - acc: 0.9749 - val_loss: 0.3932 - val_acc: 0.8683\n",
            "Epoch 974/1000\n",
            "678/678 [==============================] - 0s 584us/step - loss: 0.0768 - acc: 0.9779 - val_loss: 0.3585 - val_acc: 0.8623\n",
            "Epoch 975/1000\n",
            "678/678 [==============================] - 0s 590us/step - loss: 0.0751 - acc: 0.9764 - val_loss: 0.3804 - val_acc: 0.8623\n",
            "Epoch 976/1000\n",
            "678/678 [==============================] - 0s 570us/step - loss: 0.0809 - acc: 0.9794 - val_loss: 0.3845 - val_acc: 0.8623\n",
            "Epoch 977/1000\n",
            "678/678 [==============================] - 0s 574us/step - loss: 0.0844 - acc: 0.9735 - val_loss: 0.4193 - val_acc: 0.8563\n",
            "Epoch 978/1000\n",
            "678/678 [==============================] - 0s 603us/step - loss: 0.0836 - acc: 0.9690 - val_loss: 0.3775 - val_acc: 0.8683\n",
            "Epoch 979/1000\n",
            "678/678 [==============================] - 0s 588us/step - loss: 0.0856 - acc: 0.9749 - val_loss: 0.3756 - val_acc: 0.8593\n",
            "Epoch 980/1000\n",
            "678/678 [==============================] - 0s 593us/step - loss: 0.0883 - acc: 0.9749 - val_loss: 0.3626 - val_acc: 0.8713\n",
            "Epoch 981/1000\n",
            "678/678 [==============================] - 0s 617us/step - loss: 0.0904 - acc: 0.9676 - val_loss: 0.3880 - val_acc: 0.8533\n",
            "Epoch 982/1000\n",
            "678/678 [==============================] - 0s 542us/step - loss: 0.0740 - acc: 0.9794 - val_loss: 0.4065 - val_acc: 0.8653\n",
            "Epoch 983/1000\n",
            "678/678 [==============================] - 0s 587us/step - loss: 0.0774 - acc: 0.9808 - val_loss: 0.3787 - val_acc: 0.8683\n",
            "Epoch 984/1000\n",
            "678/678 [==============================] - 0s 559us/step - loss: 0.0721 - acc: 0.9808 - val_loss: 0.4840 - val_acc: 0.8323\n",
            "Epoch 985/1000\n",
            "678/678 [==============================] - 0s 581us/step - loss: 0.0831 - acc: 0.9720 - val_loss: 0.3968 - val_acc: 0.8533\n",
            "Epoch 986/1000\n",
            "678/678 [==============================] - 0s 620us/step - loss: 0.0840 - acc: 0.9720 - val_loss: 0.4125 - val_acc: 0.8533\n",
            "Epoch 987/1000\n",
            "678/678 [==============================] - 0s 588us/step - loss: 0.0797 - acc: 0.9705 - val_loss: 0.3979 - val_acc: 0.8623\n",
            "Epoch 988/1000\n",
            "678/678 [==============================] - 0s 582us/step - loss: 0.0798 - acc: 0.9764 - val_loss: 0.4107 - val_acc: 0.8533\n",
            "Epoch 989/1000\n",
            "678/678 [==============================] - 0s 575us/step - loss: 0.0790 - acc: 0.9735 - val_loss: 0.3462 - val_acc: 0.8772\n",
            "Epoch 990/1000\n",
            "678/678 [==============================] - 0s 567us/step - loss: 0.0723 - acc: 0.9823 - val_loss: 0.3748 - val_acc: 0.8743\n",
            "Epoch 991/1000\n",
            "678/678 [==============================] - 0s 579us/step - loss: 0.0885 - acc: 0.9779 - val_loss: 0.3851 - val_acc: 0.8473\n",
            "Epoch 992/1000\n",
            "678/678 [==============================] - 0s 579us/step - loss: 0.0765 - acc: 0.9779 - val_loss: 0.3871 - val_acc: 0.8683\n",
            "Epoch 993/1000\n",
            "678/678 [==============================] - 0s 576us/step - loss: 0.1058 - acc: 0.9587 - val_loss: 0.3502 - val_acc: 0.8503\n",
            "Epoch 994/1000\n",
            "678/678 [==============================] - 0s 574us/step - loss: 0.0692 - acc: 0.9779 - val_loss: 0.3941 - val_acc: 0.8713\n",
            "Epoch 995/1000\n",
            "678/678 [==============================] - 0s 574us/step - loss: 0.0741 - acc: 0.9764 - val_loss: 0.4185 - val_acc: 0.8683\n",
            "Epoch 996/1000\n",
            "678/678 [==============================] - 0s 540us/step - loss: 0.0745 - acc: 0.9749 - val_loss: 0.4368 - val_acc: 0.8593\n",
            "Epoch 997/1000\n",
            "678/678 [==============================] - 0s 532us/step - loss: 0.0706 - acc: 0.9764 - val_loss: 0.3938 - val_acc: 0.8593\n",
            "Epoch 998/1000\n",
            "678/678 [==============================] - 0s 568us/step - loss: 0.0780 - acc: 0.9720 - val_loss: 0.4071 - val_acc: 0.8593\n",
            "Epoch 999/1000\n",
            "678/678 [==============================] - 0s 524us/step - loss: 0.0849 - acc: 0.9676 - val_loss: 0.4049 - val_acc: 0.8593\n",
            "Epoch 1000/1000\n",
            "678/678 [==============================] - 0s 511us/step - loss: 0.0840 - acc: 0.9705 - val_loss: 0.3847 - val_acc: 0.8593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFytY6LDzgJ0",
        "colab_type": "text"
      },
      "source": [
        "Let's plot the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFz4ClZov9gZ",
        "colab_type": "code",
        "outputId": "49395a87-b325-48e5-9010-98bcb582799f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf1W7LgP2DA5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "And now let's plot the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yyFBt7ASPUe",
        "colab_type": "code",
        "outputId": "deaf4a32-9da7-49e8-e4b6-ccbf44a3b126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(cnnhistory.history['acc'])\n",
        "plt.plot(cnnhistory.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVITTQKCUUhFOA4FSSkW4/we6BMXST6gWkwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaZONl1mD8XD",
        "colab_type": "text"
      },
      "source": [
        "Let's now create a classification report to review the f1-score of the model per class.\n",
        "To do so, we have to:\n",
        "- Create a variable predictions that will contain the model.predict_classes outcome\n",
        "- Convert our y_test (array of strings with our classes) to an array of int called new_Ytest, otherwise it will not be comparable to the predictions by the classification report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO25uIL-9vqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict_classes(x_testcnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i06grlBBSrn",
        "colab_type": "code",
        "outputId": "31ca6e6f-420a-47c1-db75-6ae904e68413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 2, 2, 2, 5, 0, 3, 4, 3, 5, 5, 0, 1, 3, 5, 5, 3, 2, 1, 4, 1,\n",
              "       4, 2, 5, 5, 5, 1, 5, 4, 3, 5, 4, 2, 2, 3, 0, 5, 4, 3, 1, 2, 4, 1,\n",
              "       2, 0, 4, 1, 5, 1, 0, 1, 1, 2, 2, 4, 0, 0, 5, 3, 5, 2, 5, 2, 3, 0,\n",
              "       1, 1, 3, 1, 1, 1, 1, 5, 1, 4, 1, 0, 1, 4, 3, 2, 2, 2, 2, 1, 1, 5,\n",
              "       1, 4, 3, 3, 3, 2, 1, 4, 2, 3, 3, 1, 3, 3, 3, 5, 5, 2, 1, 0, 5, 3,\n",
              "       5, 3, 5, 1, 0, 5, 1, 1, 5, 4, 2, 5, 1, 1, 3, 2, 2, 2, 1, 2, 2, 5,\n",
              "       5, 2, 2, 3, 1, 3, 2, 3, 3, 5, 5, 4, 3, 3, 2, 5, 2, 4, 4, 1, 3, 1,\n",
              "       3, 4, 1, 4, 3, 1, 3, 5, 3, 2, 5, 1, 5, 4, 2, 2, 4, 2, 5, 2, 2, 3,\n",
              "       1, 2, 5, 4, 2, 4, 4, 5, 5, 4, 2, 1, 3, 2, 5, 5, 3, 3, 2, 0, 4, 0,\n",
              "       5, 1, 2, 4, 2, 4, 1, 4, 4, 1, 5, 5, 2, 4, 1, 1, 5, 3, 2, 2, 2, 4,\n",
              "       3, 1, 0, 5, 2, 2, 5, 4, 0, 1, 1, 5, 3, 1, 2, 3, 2, 2, 3, 2, 4, 5,\n",
              "       3, 4, 0, 5, 1, 2, 4, 2, 5, 3, 4, 5, 5, 4, 1, 1, 3, 3, 2, 4, 5, 0,\n",
              "       5, 5, 0, 5, 4, 1, 4, 1, 4, 5, 4, 1, 3, 1, 5, 5, 5, 1, 4, 4, 2, 5,\n",
              "       5, 2, 1, 0, 2, 4, 1, 4, 0, 0, 1, 3, 1, 1, 5, 0, 5, 0, 4, 0, 0, 1,\n",
              "       1, 4, 5, 1, 1, 2, 1, 4, 3, 1, 1, 2, 5, 3, 0, 1, 5, 3, 4, 4, 4, 5,\n",
              "       3, 3, 5, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUHshx93CM_6",
        "colab_type": "code",
        "outputId": "22c6928a-1179-4a7f-cfe3-5d9d2bd30ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 2, 2, 2, 5, 0, 3, 4, 3, 5, 3, 0, 1, 3, 5, 3, 3, 1, 1, 4, 1,\n",
              "       4, 2, 5, 5, 5, 1, 5, 4, 3, 5, 4, 1, 2, 3, 0, 5, 4, 3, 1, 2, 4, 5,\n",
              "       2, 0, 5, 1, 5, 5, 0, 1, 1, 2, 2, 4, 0, 0, 3, 3, 5, 3, 5, 1, 3, 0,\n",
              "       1, 1, 3, 1, 1, 1, 1, 5, 1, 4, 1, 0, 1, 4, 3, 2, 2, 2, 2, 1, 1, 5,\n",
              "       1, 4, 3, 3, 5, 2, 1, 4, 2, 3, 3, 1, 3, 3, 3, 5, 5, 2, 1, 0, 5, 3,\n",
              "       5, 3, 5, 1, 0, 5, 1, 1, 5, 4, 2, 5, 1, 1, 3, 2, 1, 2, 1, 2, 2, 5,\n",
              "       5, 2, 2, 3, 1, 4, 2, 3, 3, 5, 5, 0, 3, 3, 1, 5, 2, 4, 4, 1, 3, 1,\n",
              "       3, 4, 2, 4, 3, 1, 3, 5, 3, 2, 3, 1, 5, 3, 1, 2, 4, 1, 3, 2, 2, 3,\n",
              "       1, 5, 5, 4, 2, 4, 3, 5, 3, 4, 2, 1, 5, 2, 5, 3, 3, 3, 1, 3, 4, 0,\n",
              "       5, 1, 2, 3, 2, 4, 2, 4, 4, 1, 5, 5, 2, 4, 2, 1, 3, 3, 2, 2, 2, 4,\n",
              "       3, 1, 0, 5, 1, 2, 5, 5, 0, 1, 1, 5, 3, 1, 2, 3, 2, 2, 3, 2, 4, 5,\n",
              "       3, 5, 0, 5, 1, 2, 5, 2, 5, 3, 5, 5, 5, 4, 1, 1, 3, 5, 1, 4, 5, 0,\n",
              "       5, 5, 0, 5, 4, 1, 4, 1, 4, 5, 4, 1, 3, 1, 3, 5, 5, 1, 4, 4, 2, 5,\n",
              "       3, 2, 1, 0, 2, 4, 1, 4, 0, 5, 1, 3, 2, 1, 5, 0, 5, 0, 4, 0, 0, 1,\n",
              "       1, 4, 4, 1, 1, 2, 1, 4, 3, 1, 1, 2, 5, 3, 0, 3, 5, 3, 4, 4, 4, 3,\n",
              "       3, 3, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMxojpvWCxOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W07EQaC8DE6i",
        "colab_type": "code",
        "outputId": "eb99aa5d-15a0-4dce-a658-da016b2602e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "new_Ytest"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 2, 2, 2, 5, 0, 3, 4, 3, 5, 3, 0, 1, 3, 5, 3, 3, 1, 1, 4, 1,\n",
              "       4, 2, 5, 5, 5, 1, 5, 4, 3, 5, 4, 1, 2, 3, 0, 5, 4, 3, 1, 2, 4, 5,\n",
              "       2, 0, 5, 1, 5, 5, 0, 1, 1, 2, 2, 4, 0, 0, 3, 3, 5, 3, 5, 1, 3, 0,\n",
              "       1, 1, 3, 1, 1, 1, 1, 5, 1, 4, 1, 0, 1, 4, 3, 2, 2, 2, 2, 1, 1, 5,\n",
              "       1, 4, 3, 3, 5, 2, 1, 4, 2, 3, 3, 1, 3, 3, 3, 5, 5, 2, 1, 0, 5, 3,\n",
              "       5, 3, 5, 1, 0, 5, 1, 1, 5, 4, 2, 5, 1, 1, 3, 2, 1, 2, 1, 2, 2, 5,\n",
              "       5, 2, 2, 3, 1, 4, 2, 3, 3, 5, 5, 0, 3, 3, 1, 5, 2, 4, 4, 1, 3, 1,\n",
              "       3, 4, 2, 4, 3, 1, 3, 5, 3, 2, 3, 1, 5, 3, 1, 2, 4, 1, 3, 2, 2, 3,\n",
              "       1, 5, 5, 4, 2, 4, 3, 5, 3, 4, 2, 1, 5, 2, 5, 3, 3, 3, 1, 3, 4, 0,\n",
              "       5, 1, 2, 3, 2, 4, 2, 4, 4, 1, 5, 5, 2, 4, 2, 1, 3, 3, 2, 2, 2, 4,\n",
              "       3, 1, 0, 5, 1, 2, 5, 5, 0, 1, 1, 5, 3, 1, 2, 3, 2, 2, 3, 2, 4, 5,\n",
              "       3, 5, 0, 5, 1, 2, 5, 2, 5, 3, 5, 5, 5, 4, 1, 1, 3, 5, 1, 4, 5, 0,\n",
              "       5, 5, 0, 5, 4, 1, 4, 1, 4, 5, 4, 1, 3, 1, 3, 5, 5, 1, 4, 4, 2, 5,\n",
              "       3, 2, 1, 0, 2, 4, 1, 4, 0, 5, 1, 3, 2, 1, 5, 0, 5, 0, 4, 0, 0, 1,\n",
              "       1, 4, 4, 1, 1, 2, 1, 4, 3, 1, 1, 2, 5, 3, 0, 3, 5, 3, 4, 4, 4, 3,\n",
              "       3, 3, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW2XHdTtEedk",
        "colab_type": "text"
      },
      "source": [
        "Okay, now we can display the classification report:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfVSRmMu96rC",
        "colab_type": "code",
        "outputId": "23ce65c3-d6b2-4774-bc72-c1fd9b53d866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(new_Ytest, predictions)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94        25\n",
            "           1       0.90      0.86      0.88        73\n",
            "           2       0.81      0.93      0.86        55\n",
            "           3       0.92      0.73      0.82        67\n",
            "           4       0.83      0.96      0.89        47\n",
            "           5       0.81      0.82      0.81        67\n",
            "\n",
            "    accuracy                           0.86       334\n",
            "   macro avg       0.87      0.88      0.87       334\n",
            "weighted avg       0.86      0.86      0.86       334\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu1S5IowfSDG",
        "colab_type": "text"
      },
      "source": [
        "And now, the confusion matrix: it will show us the misclassified samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdy09SCEd7Cl",
        "colab_type": "code",
        "outputId": "a32aae1a-9331-4a50-dcfa-beef865247b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(new_Ytest, predictions)\n",
        "print (matrix)\n",
        "\n",
        "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[24  0  0  0  1  0]\n",
            " [ 0 63 10  0  0  0]\n",
            " [ 0  4 51  0  0  0]\n",
            " [ 1  1  1 49  3 12]\n",
            " [ 0  0  0  1 45  1]\n",
            " [ 1  2  1  3  5 55]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_ySPOyHxkZ3",
        "colab_type": "text"
      },
      "source": [
        "# Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5kRmoD-sdHj",
        "colab_type": "code",
        "outputId": "838cd50b-f013-486d-f073-df97486e3d3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_name = 'SER.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess/Ravdess_model/SER.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN9hBX7Pn3ht",
        "colab_type": "text"
      },
      "source": [
        "### Loading saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4oAv6Kx8RBE",
        "colab_type": "code",
        "outputId": "e4395d04-b298-4d00-d5c4-1f11e8532046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess/Ravdess_model/SER.h5')\n",
        "loaded_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 5128      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUi-Zjuf8hDB",
        "colab_type": "code",
        "outputId": "07c2b031-d97f-444a-90ad-cadacdb386b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 0s 314us/step\n",
            "Restored model, accuracy: 85.93%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}